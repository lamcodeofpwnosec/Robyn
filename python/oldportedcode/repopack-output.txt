================================================================
Repopack Output File
================================================================

This file was generated by Repopack on: 2024-08-06T08:18:35.515Z

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This header section
2. Repository structure
3. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
1. This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
2. When processing this file, use the separators and "File:" markers to
  distinguish between different files in the repository.
3. Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.



For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
allocator.py
auxiliary.py
calibration.py
checks.py
cluster.py
convergence.py
data.py
exports.py
inputs.py
json.py
model.py
outputs.py
pareto.py
plots.py
refresh.py
response.py
transformation.py

================================================================
Repository Files
================================================================

================
File: allocator.py
================
# Copyright (c) Meta Platforms, Inc. and its affiliates.

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

####################################################################
import pandas as pd
import numpy as np
import itertools
import nlopt
import os
from functools import partial
from itertools import chain

from .checks import check_allocator, check_allocator_constrains, check_metric_dates, check_daterange
from .response import robyn_response, which_usecase
from .plots import allocation_plots

ROBYN_TEMP = None

def robyn_allocator(robyn_object=None,
                    select_build=0,
                    InputCollect=None,
                    OutputCollect=None,
                    select_model=None,
                    json_file=None,
                    scenario="max_response",
                    total_budget=None,
                    target_value=None,
                    date_range=None,
                    channel_constr_low=None,
                    channel_constr_up=None,
                    channel_constr_multiplier=3,
                    optim_algo="SLSQP_AUGLAG",
                    maxeval=100000,
                    constr_mode="eq",
                    plots=True,
                    plot_folder=None,
                    plot_folder_sub=None,
                    export=True,
                    quiet=False,
                    ui=False,
                    **kwargs):
    """
    Allocates budget for a given model using the Robyn framework.

    Parameters:
    - robyn_object (object): The Robyn object containing the model.
    - select_build (int): The build number of the model.
    - InputCollect (object): The input collection object.
    - OutputCollect (object): The output collection object.
    - select_model (object): The selected model.
    - json_file (str): The path to the JSON file containing the exported model.
    - scenario (str): The scenario for the budget allocation.
    - total_budget (float): The total budget for the allocation.
    - target_value (float): The target value for the allocation.
    - date_range (str): The date range for the allocation.
    - channel_constr_low (float): The lower constraint for the channels.
    - channel_constr_up (float): The upper constraint for the channels.
    - channel_constr_multiplier (int): The multiplier for the channel constraints.
    - optim_algo (str): The optimization algorithm to use.
    - maxeval (int): The maximum number of evaluations for the optimization.
    - constr_mode (str): The constraint mode for the optimization.
    - plots (bool): Whether to generate plots.
    - plot_folder (str): The folder to save the plots.
    - plot_folder_sub (str): The subfolder to save the plots.
    - export (bool): Whether to export the results.
    - quiet (bool): Whether to suppress the output.
    - ui (bool): Whether to use the user interface.
    - **kwargs: Additional keyword arguments.

    Returns:
    - None
    """
    # Use previously exported model using json_file
    if not json_file is None:
        if InputCollect is None:
            InputCollect = robyn_inputs(json_file=json_file, quiet=True, **kwargs)
        if OutputCollect is None:
            if plot_folder is None:
                json = robyn_read(json_file, step=2, quiet=True)
                plot_folder = dirname(json.ExportedModel.plot_folder)
                if not plot_folder_sub is None:
                    plot_folder_sub = None
            OutputCollect = robyn_run(json_file=json_file, export=export, plot_folder=plot_folder, plot_folder_sub=plot_folder_sub, **kwargs)
        if select_model is None:
            select_model = OutputCollect.allSolutions

    # Collect inputs
    # if not robyn_object is None and (InputCollect is None or OutputCollect is None or select_model is None):
    #     if "robyn_exported" in robyn_object.__class__.__name__:
    #         imported = robyn_object
    #         robyn_object = imported.robyn_object
    #     else:
    #         imported = robyn_load(robyn_object, select_build, quiet=True)
    #     InputCollect = imported.InputCollect
    #     OutputCollect = imported.OutputCollect
    #     select_model = imported.select_model
    # else:
    #     if select_model is None and len(OutputCollect.allSolutions) == 1:
    #         select_model = OutputCollect.allSolutions
    #     if any(InputCollect is None, OutputCollect is None, select_model is None):
    #         raise ValueError("When 'robyn_object' is not provided, then InputCollect, OutputCollect, select_model must be provided")

    if select_model is None and len(OutputCollect['allSolutions']) == 1:
        select_model = OutputCollect['allSolutions']

    # Check if any of InputCollect, OutputCollect, or select_model is None
    if InputCollect is None or OutputCollect is None or select_model is None:
        raise ValueError("When 'robyn_object' is not provided, then InputCollect, OutputCollect, and select_model must be provided")

    # Check inputs and parameters
    if len(InputCollect["robyn_inputs"]["paid_media_spends"]) <= 1:
        raise ValueError("Must have a valid model with at least two 'paid_media_spends'")

    if not quiet:
        print(f">>> Running budget allocator for model ID {select_model}...")

    # Set local data & params values
    paid_media_spends = InputCollect["robyn_inputs"]["paid_media_spends"]
    media_order = pd.Series(paid_media_spends).sort_values().index
    media_order_list = media_order.tolist()
    mediaSpendSorted = [paid_media_spends[i] for i in media_order_list]
    # mediaSpendSorted = paid_media_spends[media_order]
    dep_var_type = InputCollect["robyn_inputs"]["dep_var_type"]
    if channel_constr_low is None:
        channel_constr_low = 0.5 if scenario == "max_response" else 0.1
    if channel_constr_up is None:
        channel_constr_up = 2 if scenario == "max_response" else np.inf

    if isinstance(channel_constr_low, list) and len(channel_constr_low) == 1:
        channel_constr_low = pd.Series([channel_constr_low[0]] * len(paid_media_spends))
    elif not isinstance(channel_constr_low, list):
        channel_constr_low = pd.Series([channel_constr_low] * len(paid_media_spends))
    else:
        channel_constr_low = pd.Series(channel_constr_low)

    if isinstance(channel_constr_up, list) and len(channel_constr_up) == 1:
        channel_constr_up = pd.Series([channel_constr_up[0]] * len(paid_media_spends))
    elif not isinstance(channel_constr_up, list):
        channel_constr_up = pd.Series([channel_constr_up] * len(paid_media_spends))
    else:
        channel_constr_up = pd.Series(channel_constr_up)

    check_allocator_constrains(channel_constr_low, channel_constr_up)

    # channel_constr_low = pd.Series(channel_constr_low, index=paid_media_spends)
    channel_constr_low.index = paid_media_spends
    channel_constr_low = channel_constr_low.iloc[media_order]
    # channel_constr_up = pd.Series(channel_constr_up, index=paid_media_spends)
    channel_constr_up.index = paid_media_spends
    channel_constr_up = channel_constr_up.iloc[media_order]

    # channel_constr_low.index = paid_media_spends
    # channel_constr_up.index = paid_media_spends
    # channel_constr_low = channel_constr_low[media_order]
    # channel_constr_up = channel_constr_up[media_order]
    dt_hyppar = OutputCollect["resultHypParam"][OutputCollect["resultHypParam"]["solID"] == select_model]
    # dt_bestCoef = OutputCollect["xDecompAgg"][OutputCollect["xDecompAgg"]["solID"] == select_model][OutputCollect["xDecompAgg"].rn.isin(paid_media_spends)]
    dt_bestCoef = OutputCollect['xDecompAgg'][
        (OutputCollect['xDecompAgg']['solID'] == select_model) &
        (OutputCollect['xDecompAgg']['rn'].isin(paid_media_spends))
    ]

    # Check inputs and parameters
    scenario = check_allocator(OutputCollect, select_model, paid_media_spends, scenario, channel_constr_low, channel_constr_up, constr_mode)

    # Sort media
    dt_coef = dt_bestCoef[['rn', 'coefs']].copy()
    get_rn_order = np.argsort(dt_bestCoef['rn'].values)
    dt_coefSorted = dt_coef.iloc[get_rn_order].copy()
    dt_bestCoef.iloc[:] = dt_bestCoef.iloc[get_rn_order]
    coefSelectorSorted = (dt_coefSorted["coefs"] > 0)
    coefSelectorSorted.index = dt_coefSorted["rn"]

    from .checks import hyper_names

    # dt_hyppar = InputCollect.select(hyper_names(InputCollect["robyn_inputs"]["adstock"], mediaSpendSorted))
    # dt_hyppar = dt_hyppar.select(sort(dt_hyppar.columns))
    selected_columns = hyper_names(InputCollect["robyn_inputs"]["adstock"], all_media=mediaSpendSorted)

    # Clean up duplicated columns
    for col in dt_hyppar.columns:
        if col.endswith('_x'):
            base_name = col[:-2]  # remove the last two characters '_x'
            col_y = base_name + '_y'

            if col_y in dt_hyppar.columns:
                dt_hyppar[base_name] = dt_hyppar[col_y].combine_first(dt_hyppar[col])
                dt_hyppar.drop([col, col_y], axis=1, inplace=True)

    dt_hyppar = dt_hyppar[selected_columns]
    dt_hyppar = dt_hyppar[sorted(dt_hyppar.columns)]

    dt_bestCoef = dt_bestCoef.drop_duplicates(subset='rn', keep='first')
    dt_bestCoef = dt_bestCoef[dt_bestCoef['rn'].isin(mediaSpendSorted)]

    channelConstrLowSorted = channel_constr_low[mediaSpendSorted]
    channelConstrUpSorted = channel_constr_up[mediaSpendSorted]

    hills = get_hill_params(InputCollect, OutputCollect, dt_hyppar, dt_coef, mediaSpendSorted, select_model)
    alphas = hills["alphas"].reset_index()
    inflexions = hills["inflexions"].reset_index()
    coefs_sorted = hills["coefs_sorted"]

    start = InputCollect["robyn_inputs"]["rollingWindowStartWhich"]
    end = InputCollect["robyn_inputs"]["rollingWindowEndWhich"]
    window_loc = range(start, end + 1)

    dt_optimCost = InputCollect["robyn_inputs"]['dt_mod'].loc[window_loc]
    new_date_range = check_metric_dates(date_range, dt_optimCost.ds, InputCollect["robyn_inputs"]["dayInterval"], quiet=False, is_allocator=True)
    date_min = new_date_range["date_range_updated"][0]
    date_max = new_date_range["date_range_updated"][-1]
    # check_daterange(date_min, date_max, dt_optimCost["ds"])
    if pd.isna(date_min):
        date_min = dt_optimCost.ds.min()
    if pd.isna(date_max):
        date_max = dt_optimCost.ds.max()
    if date_min < dt_optimCost.ds.min():
        date_min = dt_optimCost.ds.min()
    if date_max > dt_optimCost.ds.max():
        date_max = dt_optimCost.ds.max()
    histFiltered = dt_optimCost.loc[dt_optimCost.ds.between(date_min, date_max), ]

    histSpendAll = dt_optimCost[mediaSpendSorted].sum()
    histSpendAllTotal = histSpendAll.sum()
    histSpendAllUnit = dt_optimCost[mediaSpendSorted].mean()
    histSpendAllUnitTotal = histSpendAllUnit.sum()
    histSpendAllShare = histSpendAllUnit / histSpendAllUnitTotal

    histSpendWindow = histFiltered[mediaSpendSorted].sum()
    histSpendWindowTotal = histSpendWindow.sum()
    initSpendUnit = histSpendWindowUnit = histFiltered[mediaSpendSorted].mean()
    histSpendWindowUnitTotal = initSpendUnit.sum()
    histSpendWindowShare = initSpendUnit / histSpendWindowUnitTotal

    simulation_period = initial_mean_period = [len(histFiltered[x]) for x in mediaSpendSorted]
    # nDates = {x: histFiltered.ds for x in mediaSpendSorted}
    nDates = {x: histFiltered.ds.tolist() for x in mediaSpendSorted}
    if not quiet:
        unique_mean_period = list(set(initial_mean_period))[0]
        print(f"Date Window: {date_min}:{date_max} ({unique_mean_period} {InputCollect['robyn_inputs']['intervalType']}s)")

    zero_spend_channel = [x for x in mediaSpendSorted if histSpendWindow[x] == 0]

    initSpendUnitTotal = initSpendUnit.sum()
    initSpendShare = initSpendUnit / initSpendUnitTotal
    unique_period = np.unique(simulation_period)[0]
    # total_budget_unit = total_budget / unique(simulation_period) if pd.isna(total_budget) else total_budget / unique(simulation_period)
    if pd.isna(total_budget):
        total_budget_unit = initSpendUnitTotal
    else:
        total_budget_unit = total_budget / unique_period
    total_budget_window = total_budget_unit * unique_period

    # Get use case based on inputs
    usecase = which_usecase(initSpendUnit[0], date_range)
    if usecase == "all_historical_vec":
        ndates_loc = np.where(InputCollect["robyn_inputs"]["dt_mod"].ds.isin(histFiltered.ds))[0]
    else:
        ndates_loc = np.arange(len(histFiltered.ds))
    usecase = f"{usecase}+ defined_budget" if not pd.isna(total_budget) else f"{usecase}+ historical_budget"

    # Response values based on date range -> mean spend
    initResponseUnit = []
    initResponseMargUnit = []
    hist_carryover = []
    qa_carryover = []
    for i in range(len(mediaSpendSorted)):
        resp = robyn_response(
            json_file=json_file,
            select_build=select_build,
            select_model=select_model,
            metric_name=mediaSpendSorted[i],
            dt_hyppar=OutputCollect["resultHypParam"],
            dt_coef=OutputCollect["xDecompAgg"],
            InputCollect=InputCollect,
            OutputCollect=OutputCollect,
            quiet=True
        )
        window_loc = range(window_loc.start - 1, window_loc.stop - 1)
        hist_carryover_temp = resp["input_carryover"][window_loc]
        qa_carryover.append(round(resp["input_total"][window_loc]))
        hist_carryover_temp.index = resp["date"][window_loc]
        hist_carryover.append(hist_carryover_temp)
        x_input = initSpendUnit[i]
        resp_simulate = fx_objective(
            x=x_input,
            coeff=coefs_sorted[mediaSpendSorted[i]],
            alpha=alphas[f"{mediaSpendSorted[i]}_alphas"][0],
            inflexion=inflexions[f"{mediaSpendSorted[i]}_gammas"][0],
            x_hist_carryover=np.mean(hist_carryover_temp),
            get_sum=False
        )
        resp_simulate_plus1 = fx_objective(
            x=x_input + 1,
            coeff=coefs_sorted[mediaSpendSorted[i]],
            alpha=alphas[f"{mediaSpendSorted[i]}_alphas"][0],
            inflexion=inflexions[f"{mediaSpendSorted[i]}_gammas"][0],
            x_hist_carryover=np.mean(hist_carryover_temp),
            get_sum=False
        )
        initResponseUnit = np.append(initResponseUnit, resp_simulate)
        initResponseMargUnit = np.append(initResponseMargUnit, resp_simulate_plus1 - resp_simulate)

    qa_carryover = pd.concat(qa_carryover, axis=1)
    qa_carryover = qa_carryover.fillna(0)
    # Assign names to the columns of qa_carryover
    qa_carryover.columns = mediaSpendSorted

    initResponseUnit = pd.DataFrame([initResponseUnit], columns=mediaSpendSorted)

    hist_carryover = pd.concat(hist_carryover, axis=1)
    hist_carryover = hist_carryover.fillna(0)
    # Assign names to the columns of hist_carryover
    hist_carryover.columns = mediaSpendSorted

    # QA adstock: simulated adstock should be identical to model adstock
    # qa_carryover_origin = OutputCollect$mediaVecCollect[
    #   .data$solID == select_model & .data$type == "adstockedMedia",
    #   "mediaSpendSorted"
    # ]
    # qa_carryover == qa_carryover_origin

    if len(zero_spend_channel) > 0 and not quiet:
        print("Media variables with 0 spending during date range:", zero_spend_channel)
        # hist_carryover[zero_spend_channel] = 0

    channelConstrLowSortedExt = np.where(
        1 - (1 - channelConstrLowSorted) * channel_constr_multiplier < 0,
        0, 1 - (1 - channelConstrLowSorted) * channel_constr_multiplier
    )
    channelConstrUpSortedExt = np.where(
        1 + (channelConstrUpSorted - 1) * channel_constr_multiplier < 0,
        channelConstrUpSorted * channel_constr_multiplier,
        1 + (channelConstrUpSorted - 1) * channel_constr_multiplier
    )

    target_value_ext = target_value
    if scenario == "target_efficiency":
        channelConstrLowSortedExt = channelConstrLowSorted
        channelConstrUpSortedExt = channelConstrUpSorted
        if dep_var_type == "conversion":
            if target_value is None:
                target_value = initResponseUnit.sum().sum() / initSpendUnit.sum().sum() * 1.2
            target_value_ext = target_value * 1.5
        else:
            if target_value is None:
                target_value = initResponseUnit.sum().sum() / initSpendUnit.sum().sum() * 0.8
            target_value_ext = 1

    temp_init = temp_init_all = initSpendUnit
    if len(zero_spend_channel) > 0:
        temp_init_all[zero_spend_channel] = histSpendAllUnit[zero_spend_channel]

    temp_ub = temp_ub_all = channelConstrUpSorted
    temp_lb = temp_lb_all = channelConstrLowSorted
    temp_ub_ext = temp_ub_ext_all = channelConstrUpSortedExt
    temp_lb_ext = temp_lb_ext_all = channelConstrLowSortedExt

    x0 = x0_all = lb = lb_all = temp_init_all * temp_lb_all
    ub = ub_all = temp_init_all * temp_ub_all
    x0_ext = x0_ext_all = lb_ext = lb_ext_all = temp_init_all * temp_lb_ext_all
    ub_ext = ub_ext_all = temp_init_all * temp_ub_ext_all

    skip_these = (channel_constr_low == 0) & (channel_constr_up == 0)
    zero_constraint_channel = [channel for channel in mediaSpendSorted if skip_these[channel]]
    if any(skip_these) and not quiet:
        print("Excluded variables (constrained to 0):", zero_constraint_channel)
    zero_coef_channel = []
    if not all(coefSelectorSorted):
        for index, value in coefSelectorSorted.items():
            if not value:  # Check if the value is False
                zero_coef_channel.append(index)
        if not quiet:
            print("Excluded variables (coefficients are 0):", zero_coef_channel)

    channel_to_drop_loc = [channel in (zero_coef_channel + zero_constraint_channel) for channel in mediaSpendSorted]
    channel_for_allocation = [mediaSpendSorted[i] for i in range(len(mediaSpendSorted)) if not channel_to_drop_loc[i]]
    if any(channel_to_drop_loc):

        temp_init = temp_init_all.loc[channel_for_allocation]
        temp_ub = temp_ub_all.loc[channel_for_allocation]
        temp_lb = temp_lb_all.loc[channel_for_allocation]
        x0 = x0_all.loc[channel_for_allocation]
        lb = lb_all.loc[channel_for_allocation]
        ub = ub_all.loc[channel_for_allocation]

        channel_indices = [temp_init_all.index.get_loc(c) for c in channel_for_allocation]
        temp_ub_ext = temp_ub_ext_all[channel_indices]
        temp_lb_ext = temp_lb_ext_all[channel_indices]
        x0_ext = x0_ext_all[channel_indices]
        lb_ext = lb_ext_all[channel_indices]
        ub_ext = ub_ext_all[channel_indices]

    x0 = lb = temp_init * temp_lb
    ub = temp_init * temp_ub
    x0_ext = lb_ext = temp_init * temp_lb_ext
    ub_ext = temp_init * temp_ub_ext

    coefs_eval = coefs_sorted[channel_for_allocation]
    alphas_keys = [f"{channel}_alphas" for channel in channel_for_allocation]
    alphas_eval = {key: alphas[key].iloc[0] for key in alphas_keys}
    gammas_keys = [f"{channel}_gammas" for channel in channel_for_allocation]
    inflexions_eval = {key: inflexions[key].iloc[0] for key in gammas_keys}

    hist_carryover_eval = hist_carryover[channel_for_allocation]

    column_means = hist_carryover_eval.mean(axis=0)
    x_hist_carryover = column_means.values
    # hist_carryover_eval = hist_carryover[channel_for_allocation].iloc[0].values

    eval_list = {
        "coefs_eval": coefs_eval,
        "alphas_eval": alphas_eval,
        "inflexions_eval": inflexions_eval,
        "total_budget": total_budget,
        "total_budget_unit": total_budget_unit,
        "hist_carryover_eval": hist_carryover_eval,
        "target_value": target_value,
        "target_value_ext": target_value_ext,
        "dep_var_type": dep_var_type
    }

    # So we can implicitly use these values within eval_f()
    global ROBYN_TEMP
    ROBYN_TEMP = eval_list

    # Set optim options
    if optim_algo == "MMA_AUGLAG":
        local_optimizer = nlopt.LD_MMA
    else:
        local_optimizer = nlopt.LD_SLSQP

    x0_list = []
    x0_ext_list = []
    lb_list = []
    ub_list = []
    lb_ext_list = []
    ub_ext_list = []

    channels_list = [item for item in mediaSpendSorted if item not in zero_spend_channel]
    for channel in channels_list:
        x0_list.append(x0.loc[channel])
        x0_ext_list.append(x0_ext.loc[channel])
        lb_list.append(lb.loc[channel])
        ub_list.append(ub.loc[channel])
        lb_ext_list.append(lb_ext.loc[channel])
        ub_ext_list.append(ub_ext.loc[channel])

    # Run optim
    if scenario == "max_response":


        ###
        ## nlsMod
        ###
        nlsMod_opt = nlopt.opt(nlopt.LD_AUGLAG, len(x0_list))  # Use the Augmented Lagrangian algorithm
        nlsMod_opt.set_lower_bounds(lb_list)
        nlsMod_opt.set_upper_bounds(ub_list)
        nlsMod_opt.set_min_objective(eval_f)
        nlsMod_opt.set_xtol_rel(1e-10)
        nlsMod_opt.set_maxeval(maxeval)

        if constr_mode == "eq":
            nlsMod_opt.add_equality_constraint(eval_g_eq, 1e-8)
        elif constr_mode == "ineq":
            nlsMod_opt.add_inequality_constraint(eval_g_ineq, 1e-8)

        # Local optimizer options (optional)
        nlsMod_local_opt = nlopt.opt(local_optimizer, len(x0_list))
        nlsMod_local_opt.set_xtol_rel(1e-10)
        nlsMod_opt.set_local_optimizer(nlsMod_local_opt)

        # Perform the optimization
        optmSpendUnit = nlsMod_opt.optimize(x0_list)
        nlsMod_min_f = nlsMod_opt.last_optimum_value()
        optmResponseUnit = calculate_channels(optmSpendUnit)

        ###
        ## nlsModUnbound
        ###
        nlsModUnbound_opt = nlopt.opt(nlopt.LD_AUGLAG, len(x0_ext_list))  # Use the Augmented Lagrangian algorithm
        nlsModUnbound_opt.set_lower_bounds(lb_ext_list)
        nlsModUnbound_opt.set_upper_bounds(ub_ext_list)
        nlsModUnbound_opt.set_min_objective(eval_f)
        nlsModUnbound_opt.set_xtol_rel(1e-10)
        nlsModUnbound_opt.set_maxeval(maxeval)

        if constr_mode == "eq":
            nlsModUnbound_opt.add_equality_constraint(eval_g_eq, 1e-8)
        elif constr_mode == "ineq":
            nlsModUnbound_opt.add_inequality_constraint(eval_g_ineq, 1e-8)

        # Local optimizer options (optional)
        nlsModUnbound_local_opt = nlopt.opt(local_optimizer, len(x0_ext_list))
        nlsModUnbound_local_opt.set_xtol_rel(1e-10)
        nlsModUnbound_opt.set_local_optimizer(nlsModUnbound_local_opt)

        # Perform the optimization
        optmSpendUnitUnbound = nlsModUnbound_opt.optimize(x0_ext_list)
        nlsModUnbound_min_f = nlsModUnbound_opt.last_optimum_value()
        optmResponseUnitUnbound = calculate_channels(optmSpendUnitUnbound)

    # TODO debug else statement following above if structure
    elif scenario == "target_efficiency":

        total_response = OutputCollect["xDecompAgg"]["xDecompAgg"].sum()

        ###
        ## nlsMod
        ###
        nlsMod_opt = nlopt.opt(nlopt.LD_AUGLAG, len(x0_list))
        nlsMod_opt.set_lower_bounds(lb_list)
        nlsMod_opt.set_upper_bounds([total_response] * len(x0_list))
        nlsMod_opt.set_min_objective(eval_f)
        nlsMod_opt.set_xtol_rel(1e-10)
        nlsMod_opt.set_maxeval(maxeval)

        wrapper_function = wrapper_eval_g_eq_effi(target_value)

        if constr_mode == "eq":
            nlsMod_opt.add_equality_constraint(wrapper_function, 1e-8)
        elif constr_mode == "ineq":
            nlsMod_opt.add_inequality_constraint(wrapper_function, 1e-8)

        nlsMod_local_opt = nlopt.opt(local_optimizer, len(x0_list))
        nlsMod_local_opt.set_xtol_rel(1e-10)
        nlsMod_opt.set_local_optimizer(nlsMod_local_opt)

        optmSpendUnit = nlsMod_opt.optimize(x0_list)
        nlsMod_min_f = nlsMod_opt.last_optimum_value()
        optmResponseUnit = calculate_channels(optmSpendUnit)
        # bounded optimisation
        # total_response = sum(OutputCollect.xDecompAgg.xDecompAgg)
        # nlsMod = nlopt.nlopt(
        #     x0=x0,
        #     f=eval_f,
        #     f_eq=eval_g_eq_effi if constr_mode == "eq" else None,
        #     f_ieq=eval_g_eq_effi if constr_mode == "ineq" else None,
        #     lb=lb,
        #     ub=total_response * [1] * len(ub),
        #     opts=[
        #         "algorithm", "NLOPT_LD_AUGLAG",
        #         "xtol_rel", 1.0e-10,
        #         "maxeval", maxeval,
        #         "local_opts", local_opts
        #     ],
        #     target_value=target_value
        # )
        # unbounded optimisation

        ###
        ## nlsModUnbound
        ###
        nlsModUnbound_opt = nlopt.opt(nlopt.LD_AUGLAG, len(x0_list))
        nlsModUnbound_opt.set_lower_bounds(lb_list)
        nlsModUnbound_opt.set_upper_bounds([total_response] * len(x0_list))
        nlsModUnbound_opt.set_min_objective(eval_f)
        nlsModUnbound_opt.set_xtol_rel(1e-10)
        nlsModUnbound_opt.set_maxeval(maxeval)

        wrapper_function = wrapper_eval_g_eq_effi(target_value_ext)

        if constr_mode == "eq":
            nlsModUnbound_opt.add_equality_constraint(wrapper_function, 1e-8)
        elif constr_mode == "ineq":
            nlsModUnbound_opt.add_inequality_constraint(wrapper_function, 1e-8)

        nlsMod_local_opt = nlopt.opt(local_optimizer, len(x0_list))
        nlsMod_local_opt.set_xtol_rel(1e-10)
        nlsModUnbound_opt.set_local_optimizer(nlsMod_local_opt)

        optmSpendUnitUnbound = nlsModUnbound_opt.optimize(x0_list)
        nlsModUnbound_min_f = nlsModUnbound_opt.last_optimum_value()
        optmResponseUnitUnbound = calculate_channels(optmSpendUnit)

        # nlsModUnbound = nlopt.nlopt(
        #     x0=x0,
        #     f=eval_f,
        #     f_eq=eval_g_eq_effi if constr_mode == "eq" else None,
        #     f_ieq=eval_g_eq_effi if constr_mode == "ineq" else None,
        #     lb=lb,
        #     ub=total_response * [1] * len(ub),
        #     opts=[
        #         "algorithm", "NLOPT_LD_AUGLAG",
        #         "xtol_rel", 1.0e-10,
        #         "maxeval", maxeval,
        #         "local_opts", local_opts
        #     ],
        #     target_value=target_value_ext
        # )


    optmResponseMargUnit = np.array(list(map(
        lambda x, coeff, alpha, inflexion, x_hist_carryover: fx_objective(x, coeff, alpha, inflexion, x_hist_carryover),
        optmSpendUnit + 1, coefs_eval, alphas_eval.values(), inflexions_eval.values(), x_hist_carryover
    ))) - optmResponseUnit

    optmResponseMargUnitUnbound = np.array(list(map(
        lambda x, coeff, alpha, inflexion, x_hist_carryover: fx_objective(x, coeff, alpha, inflexion, x_hist_carryover),
        optmSpendUnitUnbound + 1, coefs_eval, alphas_eval.values(), inflexions_eval.values(), x_hist_carryover
    ))) - optmResponseUnitUnbound

    # Collect the output
    names = [channel_for_allocation[i] for i in range(len(channel_for_allocation))]
    mediaSpendSorted = names

    optmSpendUnitOut = np.zeros(len(channel_for_allocation))
    optmResponseUnitOut = np.zeros(len(channel_for_allocation))
    optmResponseMargUnitOut = np.zeros(len(channel_for_allocation))
    optmSpendUnitUnboundOut = np.zeros(len(channel_for_allocation))
    optmResponseUnitUnboundOut = np.zeros(len(channel_for_allocation))
    optmResponseMargUnitUnboundOut = np.zeros(len(channel_for_allocation))

    arrays_to_zero = [optmSpendUnitOut, optmResponseUnitOut, optmResponseMargUnitOut,
                  optmSpendUnitUnboundOut, optmResponseUnitUnboundOut, optmResponseMargUnitUnboundOut]
    for array in arrays_to_zero:
        array[channel_to_drop_loc] = 0

    channel_to_drop_loc_temp = np.array(channel_to_drop_loc, dtype=bool)
    # Apply non-dropped channel values
    optmSpendUnitOut[~channel_to_drop_loc_temp] = optmSpendUnit

    optmResponseUnitOut[~channel_to_drop_loc_temp] = optmResponseUnit
    optmResponseMargUnitOut[~channel_to_drop_loc_temp] = optmResponseMargUnit
    optmSpendUnitUnboundOut[~channel_to_drop_loc_temp] = optmSpendUnitUnbound
    optmResponseUnitUnboundOut[~channel_to_drop_loc_temp] = optmResponseUnitUnbound
    optmResponseMargUnitUnboundOut[~channel_to_drop_loc_temp] = optmResponseMargUnitUnbound

    optmResponseUnitTotal = initResponseUnit.sum(axis=1).values[0]
    initResponseUnitTotal_array = np.full(initResponseUnit.shape[1], optmResponseUnitTotal)

    sum_initResponseUnit = np.sum(initResponseUnit)
    unique_simulation_period = np.unique(simulation_period)
    initResponseTotal = sum_initResponseUnit * unique_simulation_period
    initResponseUnitShare = initResponseUnit / sum_initResponseUnit

    sum_initResponseUnit = np.sum(initResponseUnit)
    periods_list = ["{} {}".format(period, InputCollect['robyn_inputs']['intervalType']) for period in initial_mean_period]

    dt_optimOut = {
        'solID':select_model,
        'dep_var_type':dep_var_type,
        'channels':mediaSpendSorted,
        'date_min':date_min,
        'date_max':date_max,
        'periods':periods_list,
        'constr_low':temp_lb_all,
        'constr_low_abs':lb_all,
        'constr_up':temp_ub_all,
        'constr_up_abs':ub_all,
        'unconstr_mult':channel_constr_multiplier,
        'constr_low_unb':temp_lb_ext_all,
        'constr_low_unb_abs':lb_ext_all,
        'constr_up_unb':temp_ub_ext_all,
        'constr_up_unb_abs':ub_ext_all,
        # Historical spends
        'histSpendAll':histSpendAll,
        'histSpendAllTotal':histSpendAllTotal,
        'histSpendAllUnit':histSpendAllUnit,
        'histSpendAllUnitTotal':histSpendAllUnitTotal,
        'histSpendAllShare':histSpendAllShare,
        'histSpendWindow':histSpendWindow,
        'histSpendWindowTotal':histSpendWindowTotal,
        'histSpendWindowUnit':histSpendWindowUnit,
        'histSpendWindowUnitTotal':histSpendWindowUnitTotal,
        'histSpendWindowShare':histSpendWindowShare,
        # Initial spends for allocation
        'initSpendUnit':initSpendUnit,
        'initSpendUnitTotal':initSpendUnitTotal,
        'initSpendShare':initSpendShare,
        'initSpendTotal':initSpendUnitTotal * np.unique(simulation_period),
        # initSpendUnitRaw=histSpendUnitRaw,
        # adstocked=adstocked,
        # adstocked_start_date=as.Date(ifelse(adstocked, head(resp$date, 1), NA), origin="1970-01-01"),
        # adstocked_end_date=as.Date(ifelse(adstocked, tail(resp$date, 1), NA), origin="1970-01-01"),
        # adstocked_periods=length(resp$date),
        'initResponseUnit':initResponseUnit,
        'initResponseUnitTotal':initResponseUnitTotal_array,
        'initResponseMargUnit':initResponseMargUnit,
        'initResponseTotal':initResponseTotal,
        'initResponseUnitShare':initResponseUnitShare,
        'initRoiUnit':initResponseUnit / initSpendUnit,
        'initCpaUnit':initSpendUnit / initResponseUnit,
        # Budget change
        'total_budget_unit':total_budget_unit,
        'total_budget_unit_delta':total_budget_unit / initSpendUnitTotal - 1,
        # Optimized
        'optmSpendUnit':optmSpendUnitOut,
        'optmSpendUnitDelta':(optmSpendUnitOut / initSpendUnit - 1),
        'optmSpendUnitTotal':sum(optmSpendUnitOut),
        'optmSpendUnitTotalDelta':sum(optmSpendUnitOut) / initSpendUnitTotal - 1,
        'optmSpendShareUnit':optmSpendUnitOut / sum(optmSpendUnitOut),
        'optmSpendTotal':sum(optmSpendUnitOut) * unique_simulation_period,
        'optmSpendUnitUnbound':optmSpendUnitUnboundOut,
        'optmSpendUnitDeltaUnbound':(optmSpendUnitUnboundOut / optmSpendUnit - 1),
        'optmSpendUnitTotalUnbound':sum(optmSpendUnitUnboundOut),
        'optmSpendUnitTotalDeltaUnbound':sum(optmSpendUnitUnboundOut) / initSpendUnitTotal - 1,
        'optmSpendShareUnitUnbound':optmSpendUnitUnboundOut / sum(optmSpendUnitUnboundOut),
        'optmSpendTotalUnbound':sum(optmSpendUnitUnboundOut) * unique_simulation_period,
        'optmResponseUnit':optmResponseUnitOut,
        'optmResponseMargUnit':optmResponseMargUnitOut,
        'optmResponseUnitTotal':sum(optmResponseUnitOut),
        'optmResponseTotal':sum(optmResponseUnitOut) * unique_simulation_period,
        'optmResponseUnitShare':optmResponseUnitOut / sum(optmResponseUnitOut),
        'optmRoiUnit':optmResponseUnitOut / optmSpendUnitOut,
        'optmCpaUnit':optmSpendUnitOut / optmResponseUnitOut,
        'optmResponseUnitLift':(optmResponseUnitOut / initResponseUnit) - 1,
        'optmResponseUnitUnbound':optmResponseUnitUnboundOut,
        'optmResponseMargUnitUnbound':optmResponseMargUnitUnboundOut,
        'optmResponseUnitTotalUnbound':sum(optmResponseUnitUnboundOut),
        'optmResponseTotalUnbound':sum(optmResponseUnitUnboundOut) * unique_simulation_period,
        'optmResponseUnitShareUnbound':optmResponseUnitUnboundOut / sum(optmResponseUnitUnboundOut),
        'optmRoiUnitUnbound':optmResponseUnitUnboundOut / optmSpendUnitUnboundOut,
        'optmCpaUnitUnbound':optmSpendUnitUnboundOut / optmResponseUnitUnboundOut,
        'optmResponseUnitLiftUnbound':(optmResponseUnitUnboundOut / initResponseUnit) - 1
    }

    dt_optimOut["optmResponseUnitTotalLift"] = (dt_optimOut["optmResponseUnitTotal"] / dt_optimOut["initResponseUnitTotal"]) - 1
    dt_optimOut["optmResponseUnitTotalLiftUnbound"] = (dt_optimOut["optmResponseUnitTotalUnbound"] / dt_optimOut["initResponseUnitTotal"]) - 1

    # Calculate curves and main points for each channel
    if scenario == "max_response":
        levs1 = ["Initial", "Bounded", f"Bounded x {channel_constr_multiplier}"]
    else:
        if dep_var_type == "revenue":
            levs1 = ["Initial", f"Hit ROAS {round(target_value, 2)}", f"Hit ROAS {target_value_ext}"]
        else:
            levs1 = ["Initial", f"Hit CPA {round(target_value, 2)}", f"Hit CPA {round(target_value_ext, 2)}"]

    eval_list["levs1"] = levs1

    # Create a list to store the dataframes
    df_list = []

    initResponseUnit_series = dt_optimOut['initResponseUnit'].iloc[0]
    temp_df = pd.DataFrame({
        'channels': dt_optimOut['channels'],
        'spend': dt_optimOut['initSpendUnit'],
        'response': initResponseUnit_series,
        'type': levs1[0]
    })
    df_list.append(temp_df)

    temp_df = pd.DataFrame({
        'channels': dt_optimOut['channels'],
        'spend': dt_optimOut['optmSpendUnit'],
        'response': dt_optimOut['optmResponseUnit'],
        'type': levs1[1]
    })
    df_list.append(temp_df)

    temp_df = pd.DataFrame({
        'channels': dt_optimOut['channels'],
        'spend': dt_optimOut['optmSpendUnitUnbound'],
        'response': dt_optimOut['optmResponseUnitUnbound'],
        'type': levs1[2]
    })
    df_list.append(temp_df)

    # Concatenate the dataframes
    dt_optimOutScurve = pd.concat(df_list)
    # Rename the columns
    dt_optimOutScurve.columns = ["channels", "spend", "response", "type"]
    # Append a new row

    dt_optimOutScurve = pd.concat([
        dt_optimOutScurve,
        pd.DataFrame({"channels": dt_optimOut["channels"], "spend": 0, "response": 0, "type": "Carryover"})
    ], ignore_index=True)

    # Convert the spend and response columns to numeric
    dt_optimOutScurve['spend'] = pd.to_numeric(dt_optimOutScurve['spend'])
    dt_optimOutScurve['response'] = pd.to_numeric(dt_optimOutScurve['response'])
    # Group by channels TODO: groupby below cause lost of "type"
    # dt_optimOutScurve = dt_optimOutScurve.groupby("channels").agg({"spend": "sum", "response": "sum"})

    plotDT_scurve = {}
    for i in channel_for_allocation:
        carryover_vec = eval_list['hist_carryover_eval'][i]
        dt_optimOutScurve = dt_optimOutScurve.assign(
            spend=np.where(
                (dt_optimOutScurve['channels'] == i) & (dt_optimOutScurve['type'].isin(levs1)),
                dt_optimOutScurve['spend'] + np.mean(carryover_vec),
                np.where(
                    (dt_optimOutScurve['channels'] == i) & (dt_optimOutScurve['type'] == 'Carryover'),
                    np.mean(carryover_vec),
                    dt_optimOutScurve['spend']
                )
            )
        )
        get_max_x = max(dt_optimOutScurve.loc[dt_optimOutScurve['channels'] == i, 'spend']) * 1.5
        simulate_spend = np.linspace(0, get_max_x, 100)
        simulate_response = fx_objective(
            x=simulate_spend,
            coeff=eval_list['coefs_eval'][i],
            alpha=eval_list['alphas_eval'][f'{i}_alphas'],
            inflexion=eval_list['inflexions_eval'][f'{i}_gammas'],
            x_hist_carryover=0,
            get_sum=False
        )
        simulate_response_carryover = fx_objective(
            x=np.mean(carryover_vec),
            coeff=eval_list['coefs_eval'][i],
            alpha=eval_list['alphas_eval'][f'{i}_alphas'],
            inflexion=eval_list['inflexions_eval'][f'{i}_gammas'],
            x_hist_carryover=0,
            get_sum=False
        )
        plotDT_scurve[i] = pd.DataFrame({
            'channel': i,
            'spend': simulate_spend,
            'mean_carryover': np.mean(carryover_vec),
            'carryover_response': simulate_response_carryover,
            'total_response': simulate_response
        })
        dt_optimOutScurve = dt_optimOutScurve.assign(
            response=np.where(
                (dt_optimOutScurve['channels'] == i) & (dt_optimOutScurve['type'] == 'Carryover'),
                simulate_response_carryover,
                dt_optimOutScurve['response']
            )
        )

    # Convert plotDT_scurve to a pandas DataFrame
    plotDT_scurve_df = pd.concat([
        pd.DataFrame(v).assign(channel=k) for k, v in plotDT_scurve.items()
    ], ignore_index=True)

    # Rename columns in mainPoints
    mainPoints = dt_optimOutScurve.rename(columns={"response": "response_point", "spend": "spend_point", "channels": "channel"})

    # Filter out Carryover rows from mainPoints
    temp_caov = mainPoints[mainPoints["type"] == "Carryover"]

    mainPoints["mean_spend"] = mainPoints["spend_point"] - mainPoints["channel"].map(temp_caov.set_index("channel")["spend_point"])
    mainPoints["mean_spend"] = np.where(mainPoints["type"] == "Carryover", mainPoints["spend_point"], mainPoints["mean_spend"])
    if levs1[1] == levs1[2]:
        levs1[2] = levs1[2] + "."
    mainPoints["type"] = pd.Categorical(mainPoints["type"], categories=["Carryover"] + levs1)
    mainPoints["roi_mean"] = mainPoints["response_point"] / mainPoints["mean_spend"]

    mresp_caov = mainPoints[mainPoints["type"] == "Carryover"]["response_point"].values
    mresp_init = mainPoints[mainPoints["type"] == levs1[0]]["response_point"].values - mresp_caov
    mresp_b = mainPoints[mainPoints["type"] == levs1[1]]["response_point"].values - mresp_caov
    mresp_unb = mainPoints[mainPoints["type"] == levs1[2]]["response_point"].values - mresp_caov

    mainPoints["marginal_response"] = np.concatenate((mresp_init, mresp_b, mresp_unb, np.zeros(len(mresp_init))))
    mainPoints["roi_marginal"] = mainPoints["marginal_response"] / mainPoints["mean_spend"]
    mainPoints["cpa_marginal"] = mainPoints["mean_spend"] / mainPoints["marginal_response"]

    eval_list["mainPoints"] = mainPoints

    # # Calculate mean spend and ROI for each channel
    # mainPoints["mean_spend"] = mainPoints["spend_point"] - temp_caov["spend_point"]
    # mainPoints.loc[mainPoints["type"] == "Carryover", "mean_spend"] = mainPoints.loc[mainPoints["type"] == "Carryover", "spend_point"]
    # mainPoints["roi_mean"] = mainPoints["response_point"] / mainPoints["mean_spend"]

    # # Calculate marginal response, ROI, and CPA for each channel
    # mresp_caov = mainPoints[mainPoints["type"] == "Carryover"]["response_point"]
    # mresp_init = mainPoints[mainPoints["type"] == levels(mainPoints["type"])[2]]["response_point"] - mresp_caov
    # mresp_b = mainPoints[mainPoints["type"] == levels(mainPoints["type"])[3]]["response_point"] - mresp_caov
    # mresp_unb = mainPoints[mainPoints["type"] == levels(mainPoints["type"])[4]]["response_point"] - mresp_caov
    # mainPoints["marginal_response"] = [mresp_init, mresp_b, mresp_unb] + [0] * len(mresp_init)
    # mainPoints["roi_marginal"] = mainPoints["marginal_response"] / mainPoints["mean_spend"]
    # mainPoints["cpa_marginal"] = mainPoints["mean_spend"] / mainPoints["marginal_response"]

    # Exporting directory
    if export:
        if json_file is None and plot_folder is not None:
            if plot_folder_sub is None:
                plot_folder_sub = os.path.basename(OutputCollect['plot_folder'])
            plot_folder = os.path.join(plot_folder, plot_folder_sub)
        else:
            plot_folder = os.path.join(OutputCollect['plot_folder'])
        if not os.path.exists(plot_folder):
            print(f"Creating directory for allocator: {plot_folder}")
            os.makedirs(plot_folder)
        # Export results into CSV
        export_dt_optimOut = dt_optimOut
        if dep_var_type == "conversion":
            export_dt_optimOut.columns = [col.replace("Roi", "CPA") for col in export_dt_optimOut.columns]

        # Convert all values to lists to ensure consistent length
        max_len = max(len(v) if isinstance(v, (list, np.ndarray, pd.Series)) else 1 for v in export_dt_optimOut.values())
        # Convert constant values to lists
        export_dt_optimOut_c = export_dt_optimOut.copy()
        for k, v in export_dt_optimOut.items():
            if not isinstance(v, (list, np.ndarray, pd.Series, pd.DataFrame)):
                export_dt_optimOut[k] = [v] * max_len
        # Pad ndarrays
        for k, v in export_dt_optimOut.items():
            if isinstance(v, np.ndarray):
                export_dt_optimOut[k] = np.pad(v, (0, max_len - len(v)))
        # Convert Series to lists
        for k, v in export_dt_optimOut.items():
            if isinstance(v, pd.Series):
                export_dt_optimOut[k] = v.tolist()
        # Convert DataFrames to lists
        for k, v in export_dt_optimOut.items():
            if isinstance(v, pd.DataFrame):
                export_dt_optimOut[k] = list(chain.from_iterable(v.values.tolist()))
        # Create the DataFrame
        export_dt_optimOut = pd.DataFrame.from_dict(export_dt_optimOut, orient='columns')

        export_dt_optimOut.to_csv(os.path.join(plot_folder, f"{select_model}_{scenario}_reallocated.csv"), index=False)
    # Plot allocator results
    if plots:
        # TODO: Enable plots at some point and uncomment this
        plots = None
        # plots = allocation_plots(
        #     InputCollect, OutputCollect,
        #     dt_optimOut,
        #     select_model, scenario, eval_list,
        #     export, plot_folder, quiet
        # )
    else:
        plots = None
    output = {
        'dt_optimOut': dt_optimOut,
        'mainPoints': mainPoints,
        'nlsMod': nlsMod_opt,
        'plots': plots,
        'scenario': scenario,
        'usecase': usecase,
        'total_budget': total_budget_window if total_budget is None else total_budget,
        'skipped_coef0': zero_coef_channel,
        'skipped_constr': zero_constraint_channel,
        'no_spend': zero_spend_channel,
        'ui': plots if ui else None
    }
    # output = pd.DataFrame(output)
    return output


# Define the objective function
def fx_objective(x, coeff, alpha, inflexion, x_hist_carryover, get_sum=False, SIMPLIFY=True):
    """
    Calculate the objective function value for a given set of parameters.

    Parameters:
    x (float): The input value.
    coeff (float): Coefficient value.
    alpha (float): Exponent value.
    inflexion (float): Inflexion value.
    x_hist_carryover (float): Carryover value.
    get_sum (bool, optional): If True, returns the sum of the objective function values. Defaults to False.
    SIMPLIFY (bool, optional): If True, simplifies the objective function value. Defaults to True.

    Returns:
    float: The objective function value or the sum of objective function values if get_sum is True.
    """
    if get_sum:
        return np.sum(coeff * x**alpha * np.exp(-inflexion * x))
    else:
        return coeff * x**alpha * np.exp(-inflexion * x)

# Define the optimization problem
def optimize(x0, coeff, alpha, inflexion, x_hist_carryover, total_budget, channel_constr_low, channel_constr_up, channel_constr_multiplier, optim_algo, maxeval, constr_mode):
    """
    Optimize the allocation of resources based on the given parameters.

    Args:
        x0 (array_like): Initial guess for the allocation.
        coeff (array_like): Coefficients for the allocation function.
        alpha (float): Exponent for the allocation function.
        inflexion (float): Inflexion parameter for the allocation function.
        x_hist_carryover (array_like): Historical allocation values.
        total_budget (float): Total budget for the allocation.
        channel_constr_low (array_like): Lower bounds for channel constraints.
        channel_constr_up (array_like): Upper bounds for channel constraints.
        channel_constr_multiplier (float): Multiplier for channel constraints.
        optim_algo (str): Optimization algorithm to use.
        maxeval (int): Maximum number of function evaluations.
        constr_mode (str): Constraint mode ('eq' for equality, 'ineq' for inequality).

    Returns:
        array_like: Optimized allocation.

    """
    import scipy.optimize as opt

    def f(x):
        return -np.sum(coeff * x**alpha * np.exp(-inflexion * x))

    def cons(x):
        return x - total_budget

    if constr_mode == "eq":
        cons = (x - total_budget) * channel_constr_multiplier

    if channel_constr_low is not None and channel_constr_up is not None:
        cons = np.concatenate((cons,
                                x - channel_constr_low,
                                channel_constr_up - x))

    res = opt.minimize(f, x0, method=optim_algo, constraints=cons, options={"maxiter": maxeval})

    return res.x


def print_robyn_allocator(x):
    """
    Prints the allocator details for Robyn.

    Args:
        x: The input object containing allocator details.

    Returns:
        None
    """
    temp = x.dt_optimOut[~x.dt_optimOut.optmRoiUnit.isna(), ]
    coef0 = (len(x.skipped_coef0) > 0) * f"Coefficient 0: {v2t(x.skipped_coef0, quotes=False)}"
    constr = (len(x.skipped_constr) > 0) * f"Constrained @0: {v2t(x.skipped_constr, quotes=False)}"
    nospend = (len(x.no_spend) > 0) * f"Spend = 0: {v2t(x.no_spend, quotes=False)}"
    media_skipped = " | ".join([coef0, constr, nospend])
    media_skipped = media_skipped if media_skipped else "None"

    print(f"Model ID: {x.dt_optimOut.solID[0]}\n"
          f"Scenario: {x.scenario}\n"
          f"Use case: {x.usecase}\n"
          f"Window: {x.dt_optimOut.date_min[0]}:{x.dt_optimOut.date_max[0]} ({x.dt_optimOut.periods[0]})\n"
          f"Dep. Variable Type: {temp.dep_var_type[0]}\n"
          f"Media Skipped: {media_skipped}\n"
          f"Relative Spend Increase: {num_abbr(100 * x.dt_optimOut.optmSpendUnitTotalDelta[0], 3)}% ({formatNum(sum(x.dt_optimOut.optmSpendUnitTotal) - sum(x.dt_optimOut.initSpendUnitTotal), abbr=True, sign=True)})"
          f"Total Response Increase (Optimized): {signif(100 * x.dt_optimOut.optmResponseUnitTotalLift[0], 3)}%\n"
          f"Allocation Summary:\n"
         )


def plot_robyn_allocator(x, *args, **kwargs):
    """
    Plot the Robyn allocator.

    Parameters:
    - x: The input data.
    - *args: Additional positional arguments to be passed to the plot function.
    - **kwargs: Additional keyword arguments to be passed to the plot function.
    """
    plots = x.plots
    plots = plots.plots
    plot(plots, *args, **kwargs)

def calculate_channels(X):
    eval_list = ROBYN_TEMP  # This should be defined globally or passed as an argument
    return np.array([
        fx_objective_channel(x, coeff, alpha, inflexion, x_hist)
        for x, coeff, alpha, inflexion, x_hist in zip(
            X,
            eval_list['coefs_eval'],
            eval_list['alphas_eval'].values(),
            eval_list['inflexions_eval'].values(),
            eval_list['hist_carryover_eval'].mean(axis=0)
        )
    ])

def eval_f(X, grad):
    eval_list = ROBYN_TEMP
    results = np.array([
        fx_objective(x, coeff, alpha, inflexion, x_hist)
        for x, coeff, alpha, inflexion, x_hist in zip(X, eval_list['coefs_eval'], eval_list['alphas_eval'].values(), eval_list['inflexions_eval'].values(), eval_list['hist_carryover_eval'].mean(axis=0))
    ])
    objective = -np.sum(results)

    if grad.size > 0:
        grad[:] = np.array([
            fx_gradient(x, coeff, alpha, inflexion, x_hist)
            for x, coeff, alpha, inflexion, x_hist in zip(X, eval_list['coefs_eval'], eval_list['alphas_eval'].values(), eval_list['inflexions_eval'].values(), eval_list['hist_carryover_eval'].mean(axis=0))
        ])

    return objective

def fx_objective(x, coeff, alpha, inflexion, x_hist_carryover, get_sum=True):
    """
    Calculate the objective function value for the given parameters.

    Parameters:
    x (array-like): Input values.
    coeff (float): Coefficient value.
    alpha (float): Alpha value.
    inflexion (float): Inflexion value.
    x_hist_carryover (array-like): Historical carryover values.
    get_sum (bool, optional): Flag to determine if the sum of the objective function should be returned. Defaults to True.

    Returns:
    float: Objective function value.
    """
    # Apply Michaelis Menten model to scale spend to exposure
    xScaled = x
    # Adstock scales
    xAdstocked = x + np.mean(x_hist_carryover)
    # Hill transformation
    if get_sum:
        xOut = coeff * np.sum((1 + inflexion**alpha / xAdstocked**alpha)**-1)
    else:
        xOut = coeff * ((1 + inflexion**alpha / xAdstocked**alpha)**-1)
    return xOut

def fx_gradient(x, coeff, alpha, inflexion, x_hist_carryover):
    """
    Calculate the gradient of the function fx.

    Parameters:
    x (float): The input value.
    coeff (float): Coefficient value.
    alpha (float): Alpha value.
    inflexion (float): Inflexion value.
    x_hist_carryover (float): Carryover value.

    Returns:
    float: The gradient of the function fx.
    """
    # Apply Michaelis Menten model to scale spend to exposure
    xScaled = x
    # Adstock scales
    xAdstocked = x + np.mean(x_hist_carryover)
    xOut = -coeff * np.sum((alpha * (inflexion**alpha) * (xAdstocked**(alpha - 1))) / (xAdstocked**alpha + inflexion**alpha)**2)
    return xOut

def fx_objective_channel(x, coeff, alpha, inflexion, x_hist_carryover):
    # Adstock calculation
    xAdstocked = x + np.mean(x_hist_carryover)
    xOut = -coeff * np.sum((1 + inflexion**alpha / xAdstocked**alpha)**-1)
    return xOut

# def fx_objective_channel(x, coeff, alpha, inflexion, x_hist_carryover):
#     """
#     Calculate the objective value for a channel allocation.

#     Parameters:
#     x (array-like): The channel allocation.
#     coeff (float): Coefficient used in the objective calculation.
#     alpha (float): Alpha parameter used in the objective calculation.
#     inflexion (float): Inflexion parameter used in the objective calculation.
#     x_hist_carryover (array-like): Historical carryover values.

#     Returns:
#     float: The objective value for the given channel allocation.
#     """
#     # Apply Michaelis Menten model to scale spend to exposure
#     xScaled = x
#     # Adstock scales
#     xAdstocked = x + np.mean(x_hist_carryover)
#     xOut = -coeff * np.sum((1 + inflexion**alpha / xAdstocked**alpha)**-1)
#     return xOut

def eval_g_eq(X, grad):

    eval_list = ROBYN_TEMP
    # Assuming 'total_budget_unit' is a scalar value representing the total budget
    constraint_value = np.sum(X) - eval_list["total_budget_unit"]
    if grad.size > 0:
        grad[:] = np.ones(len(X))  # Set gradient of the constraint
    return constraint_value  # Return the scalar constraint value

def eval_g_ineq(X, grad):

    eval_list = ROBYN_TEMP
    constraint_value = np.sum(X) - eval_list["total_budget_unit"]
    if grad.size > 0:
        grad[:] = np.ones(len(X))  # Set gradient of the constraint
    return constraint_value  # This must be non-negative for "ineq" constraints to be satisfied


# def eval_g_eq(X, target_value):
#     """
#     Evaluate the equality constraint function for optimization.

#     Parameters:
#     X (array-like): The decision variables.
#     target_value (float): The target value for the constraint.

#     Returns:
#     dict: A dictionary containing the constraint value and its gradient.
#         The constraint value is the sum of the decision variables minus the total budget unit.
#         The gradient is an array of ones with the same length as X.
#     """
#     global ROBYN_TEMP
#     eval_list = ROBYN_TEMP
#     constr = np.sum(X) - eval_list["total_budget_unit"]
#     grad = np.ones(len(X))
#     return {"constraints": constr, "jacobian": grad}

# def eval_g_ineq(X, target_value):
#     """
#     Evaluate the inequality constraints for the optimization problem.

#     Parameters:
#     - X: A numpy array representing the decision variables.
#     - target_value: The target value for the optimization problem.

#     Returns:
#     - A dictionary containing the constraints and their gradients.
#     """
#     global ROBYN_TEMP
    # eval_list = ROBYN_TEMP
    # constr = np.sum(X) - eval_list["total_budget_unit"]
    # grad = np.ones(len(X))
    # return {"constraints": constr, "jacobian": grad}

def wrapper_eval_g_eq_effi(target_value):
    def eval_g_eq_effi(X, grad):
        """
        Evaluate the equality constraints and their Jacobian for the given input vector X and target value.

        Parameters:
        X (array-like): Input vector.
        target_value (float): Target value for the constraints.

        Returns:
        dict: A dictionary containing the constraints and their Jacobian.
            The constraints are stored under the key 'constraints',
            and the Jacobian is stored under the key 'jacobian'.
        """
        global ROBYN_TEMP
        eval_list = ROBYN_TEMP
        # sum_response = np.sum(np.vectorize(fx_objective)(x=X, coeff=eval_list["coefs_eval"], alpha=eval_list["alphas_eval"].values(), inflexion=eval_list["inflexions_eval"].values(), x_hist_carryover=eval_list["hist_carryover_eval"].mean(axis=0)))
        sum_response = np.sum([
            fx_objective(x, coeff, alpha, inflexion, x_hist)
            for x, coeff, alpha, inflexion, x_hist in zip(X, eval_list['coefs_eval'], eval_list['alphas_eval'].values(), eval_list['inflexions_eval'].values(), eval_list['hist_carryover_eval'].mean(axis=0))
        ])
        if target_value is None or (isinstance(target_value, (list, np.ndarray)) and len(target_value) == 0):
            if eval_list["dep_var_type"] == "conversion":
                constr = np.sum(X) - sum_response * eval_list["target_value"]
            else:
                constr = np.sum(X) - sum_response / eval_list["target_value"]
        else:
            if eval_list["dep_var_type"] == "conversion":
                constr = np.sum(X) - sum_response * target_value
            else:
                constr = np.sum(X) - sum_response / target_value
        # grad = np.ones(len(X)) - np.vectorize(fx_gradient)(x=X, coeff=eval_list["coefs_eval"], alpha=eval_list["alphas_eval"].values(), inflexion=eval_list["inflexions_eval"].values(), x_hist_carryover=eval_list["hist_carryover_eval"].mean(axis=0))
        grads = [
            fx_gradient(x, coeff, alpha, inflexion, x_hist)
            for x, coeff, alpha, inflexion, x_hist in zip(
                X,
                eval_list["coefs_eval"],
                eval_list["alphas_eval"].values(),
                eval_list["inflexions_eval"].values(),
                eval_list["hist_carryover_eval"].mean(axis=0)
            )
        ]
        grad = np.ones(len(X)) - np.array(grads)
        return constr
    return eval_g_eq_effi

def eval_g_eq_effi(X, target_value):
    """
    Evaluate the equality constraints for the efficiency function.

    Parameters:
    X (array-like): The decision variables.
    target_value (float): The target value for the efficiency function.

    Returns:
    dict: A dictionary containing the constraints and the jacobian.
    """
    global ROBYN_TEMP
    eval_list = ROBYN_TEMP
    # sum_response = np.sum(np.vectorize(fx_objective)(X, eval_list["coefs_eval"], eval_list["alphas_eval"].values(), eval_list["inflexions_eval"].values(), eval_list["hist_carryover_eval"].mean(axis=0)))
    sum_response = np.sum([
        fx_objective(x, coeff, alpha, inflexion, x_hist)
        for x, coeff, alpha, inflexion, x_hist in zip(X, eval_list['coefs_eval'], eval_list['alphas_eval'].values(), eval_list['inflexions_eval'].values(), eval_list['hist_carryover_eval'].mean(axis=0))
    ])
    if target_value is None or (isinstance(target_value, (list, np.ndarray)) and len(target_value) == 0):

        if eval_list["dep_var_type"] == "conversion":
            constr = np.sum(X) - sum_response * eval_list["target_value"]
        else:
            constr = np.sum(X) - sum_response / eval_list["target_value"]
    else:
        if eval_list["dep_var_type"] == "conversion":
            constr = np.sum(X) - sum_response * target_value
        else:
            constr = np.sum(X) - sum_response / target_value
    # grad = np.ones(len(X)) - np.vectorize(fx_gradient)(X, eval_list["coefs_eval"], eval_list["alphas_eval"].values(), eval_list["inflexions_eval"].values(), eval_list["hist_carryover_eval"].mean(axis=0))
    grads = [
        fx_gradient(x, coeff, alpha, inflexion, x_hist)
        for x, coeff, alpha, inflexion, x_hist in zip(
            X,
            eval_list["coefs_eval"],
            eval_list["alphas_eval"].values(),
            eval_list["inflexions_eval"].values(),
            eval_list["hist_carryover_eval"].mean(axis=0)
        )
    ]
    grad = np.ones(len(X)) - np.array(grads)
    return constr

def get_adstock_params(InputCollect, dt_hyppar):
    """
    Retrieves the adstock parameters based on the adstock type specified in InputCollect.

    Parameters:
    InputCollect (object): The input collection object.
    dt_hyppar (DataFrame): The DataFrame containing the adstock hyperparameters.

    Returns:
    DataFrame: The adstock hyperparameters based on the adstock type.

    """
    if InputCollect.adstock == "geometric":
        getAdstockHypPar = dt_hyppar.loc[dt_hyppar.columns.str.extract(".*_thetas", expand=False)]
    else:
        getAdstockHypPar = dt_hyppar.loc[dt_hyppar.columns.str.extract(".*_shapes|.*_scales", expand=False)]
    return getAdstockHypPar


def get_hill_params(InputCollect, OutputCollect, dt_hyppar, dt_coef, mediaSpendSorted, select_model, chnAdstocked=None):
    """
    Calculate the hill parameters for the given inputs.

    Args:
        InputCollect (type): Description of InputCollect.
        OutputCollect (type): Description of OutputCollect.
        dt_hyppar (type): Description of dt_hyppar.
        dt_coef (type): Description of dt_coef.
        mediaSpendSorted (type): Description of mediaSpendSorted.
        select_model (type): Description of select_model.
        chnAdstocked (type, optional): Description of chnAdstocked. Defaults to None.

    Returns:
        dict: A dictionary containing the calculated hill parameters:
            - "alphas": The alphas values.
            - "inflexions": The inflexions values.
            - "coefs_sorted": The sorted coefficients.
    """
    hillHypParVec = dt_hyppar.loc[:, dt_hyppar.columns.str.contains("_alphas|_gammas")]
    if isinstance(mediaSpendSorted, list):
        columns_to_select_alpha = [s + "_alphas" for s in mediaSpendSorted]
    else:
        columns_to_select_alpha = mediaSpendSorted + "_alphas"
    alphas = hillHypParVec.loc[:, columns_to_select_alpha]
    # alphas = hillHypParVec.loc[:, mediaSpendSorted + "_alphas"]
    if isinstance(mediaSpendSorted, list):
        columns_to_select_gammas = [s + "_gammas" for s in mediaSpendSorted]
    else:
        columns_to_select_gammas = mediaSpendSorted + "_gammas"
    gammas = hillHypParVec.loc[:, columns_to_select_gammas]

    if chnAdstocked is None:
        mask = (OutputCollect['mediaVecCollect']['type'] == 'adstockedMedia') & (OutputCollect['mediaVecCollect']['solID'] == select_model)
        filtered_df = OutputCollect['mediaVecCollect'].loc[mask]

        selected_df = filtered_df.loc[:, mediaSpendSorted]
        chnAdstocked = selected_df.iloc[InputCollect['robyn_inputs']['rollingWindowStartWhich']:InputCollect['robyn_inputs']['rollingWindowEndWhich'] + 1]  # +1 if end index should be inclusive

    if isinstance(gammas, pd.DataFrame):
        gammas_cycled = itertools.cycle(gammas.values.flatten())
        # Initialize inflexions as the first row of chnAdstocked
        inflexions = chnAdstocked.iloc[0:1].copy()
        # Apply a function to each column of chnAdstocked
        for i in range(chnAdstocked.shape[1]):
            max_val = np.max(chnAdstocked.iloc[:, i])
            min_val = np.min(chnAdstocked.iloc[:, i])
            # Ensure gamma_value is a scalar by using float() or accessing the first element if it's a single-element array
            gamma_value = float(next(gammas_cycled))  # This assumes gammas is now a flat array or list
            inflexion = (max_val - min_val) * (1 - gamma_value + gamma_value)
            inflexions.iloc[0, i] = inflexion
        # Set the column names of inflexions to match gammas
        if isinstance(gammas, pd.DataFrame):
            inflexions.columns = gammas.columns
    else:
        gammas = np.array([gammas])
        # Create a list to store the inflexions
        inflexions = []
        for i in range(chnAdstocked.shape[1]):
            max_val = np.max(chnAdstocked.iloc[:, i])
            min_val = np.min(chnAdstocked.iloc[:, i])
            # Ensure gamma_value is a scalar by using float() or accessing the first element if it's a single-element array
            gamma_value = float(gammas[i % len(gammas)])  # This assumes gammas is already a flat array or list
            inflexion = (max_val - min_val) * (1 - gamma_value + gamma_value)
            inflexions.append(inflexion)

    coefs = dt_coef.set_index('rn').loc[mediaSpendSorted]['coefs']
    if not isinstance(coefs, float):
        coefs = coefs.drop_duplicates()

    if isinstance(dt_coef, pd.DataFrame):
        coefs = dt_coef['coefs']
    else:
        coefs = pd.Series(dt_coef)
    # Assign names to the elements of coefs
    coefs.index = dt_coef['rn']
    # Sort coefs according to mediaSpendSorted
    coefs_sorted = coefs[mediaSpendSorted]

    # Removing old logic as it might not be needed
    # if isinstance(coefs, pd.DataFrame):
    #     coefs_sorted = pd.Series(coefs.iloc[:, 0].values, index=coefs.index, name='DataFrame Column')
    # elif isinstance(coefs, pd.Series):
    #     coefs_sorted = pd.Series(coefs.values, index=coefs.index, name=coefs.name)
    # elif isinstance(coefs, np.ndarray):
    #     coefs_sorted = pd.Series(coefs, index=range(len(coefs)), name='Array')
    # elif isinstance(coefs, list):
    #     coefs_sorted = pd.Series(coefs, index=range(len(coefs)), name='List')
    # else:
    #     coefs_sorted = pd.Series(coefs, index=dt_coef.index, name=dt_coef['rn'].values[0])

    return {'alphas': alphas, 'inflexions': inflexions, 'coefs_sorted': coefs_sorted}

================
File: auxiliary.py
================
# Copyright (c) Meta Platforms, Inc. and its affiliates.

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

####################################################################
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def get_rsq(true, predicted, p=None, df_int=None, n_train=None):
    """
    Calculate R-squared.

    Parameters
    ----------
    true : array-like
        True values.
    predicted : array-like
        Predicted values.
    p : int, optional
        Number of parameters.
    df_int : int, optional
        Degrees of freedom for intercept.
    n_train : int, optional
        Number of training examples.

    Returns
    -------
    rsq : float
        R-squared value.
    """
    sse = np.sum((predicted - true)**2)
    sst = np.sum((true - np.mean(true))**2)
    rsq = 1 - sse / sst

    if p is not None and df_int is not None:
        n = n_train if n_train is not None else len(true)
        rdf = n - p - 1
        rsq_adj = 1 - (1 - rsq) * (n - df_int) / rdf
        rsq = rsq_adj

    return rsq

def robyn_palette():
    """
    Robyn colors.

    Returns
    -------
    pal : list
        Color palette.
    """
    pal = [
        "#21130d", "#351904", "#543005", "#8C510A", "#BF812D", "#DFC27D", "#F6E8C3",
        "#F5F5F5", "#C7EAE5", "#80CDC1", "#35978F", "#01665E", "#043F43", "#04272D"
    ]
    repeated = 4
    return {
        "fill": rep(pal, repeated),
        "colour": rep(c(rep("#FFFFFF", 4), rep("#000000", 7), rep("#FFFFFF", 3)), repeated)
    }

def flatten_hyps(x):
    """
    Flatten hypothesis.

    Parameters
    ----------
    x : array-like
        Hypothesis.

    Returns
    -------
    str
        Flattened hypothesis.
    """
    if x is None:
        return x
    temp = np.array(lapply(x, lambda x: f"{x:.6f}".format(x)))
    return np.array(temp).reshape(-1, 1)

def robyn_update(dev=True, *args):
    """
    Update Robyn version.

    Parameters
    ----------
    dev : bool, optional
        Dev version? If not, CRAN version.
    *args : tuple
        Parameters to pass to install_github or install.packages.

    Returns
    -------
    None
    """
    if dev:
        try:
            import remotes
            remotes.install_github("facebookexperimental/Robyn/R", *args)
        except:
            pass
    else:
        import utils
        utils.install.packages("Robyn", *args)

================
File: calibration.py
================
# Copyright (c) Meta Platforms, Inc. and its affiliates.

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

####################################################################
import numpy as np
import pandas as pd
import warnings

from .transformation import saturation_hill, transform_adstock


def robyn_calibrate(
    calibration_input,
    df_raw,
    dayInterval,
    xDecompVec,
    coefs,
    hypParamSam,
    wind_start,
    wind_end,
    adstock,
):
    """
    Calibrates the given input data using the ROBYN algorithm.

    Args:
        calibration_input (DataFrame): The input data for calibration.
        df_raw (DataFrame): The raw data.
        dayInterval (int): The interval in days.
        xDecompVec (DataFrame): The decomposition vector.
        coefs (DataFrame): The coefficients.
        hypParamSam (DataFrame): The hyperparameters.
        wind_start (int): The start index of the window.
        wind_end (int): The end index of the window.
        adstock (str): The adstock type.

    Returns:
        DataFrame: The calibrated data.
    """
    # Convert the R code's dataframe to a Pandas dataframe
    df_raw = pd.DataFrame(df_raw)
    # Extract the necessary columns from the dataframe
    ds_wind = df_raw["ds"][(wind_start-1):(wind_end-1)]
    min_ds = calibration_input["liftStartDate"] >= np.min(ds_wind)
    max_ds = calibration_input["liftEndDate"] <= (np.max(ds_wind) - np.timedelta64(dayInterval - 1,'D'))
    include_study = np.any(min_ds & max_ds)

    if not include_study:
        warnings.warn(
            "All calibration_input in outside modelling window. Running without calibration"
        )  ## Manually added
        return None
    elif include_study and calibration_input is not None:

        calibration_input["pred"] = pd.NA
        calibration_input["pred_total"] = pd.NA
        calibration_input["decompStart"] = pd.NA
        calibration_input["decompEnd"] = pd.NA

        # Split the channels into a list
        temp_channels = calibration_input["channel"].values
        split_channels = list()
        for val in temp_channels:
            str_list = str(val).split("+")
            split_channels.append(str_list)
            # for channel_str in str_list:

        ##split_channels = list(calibration_input["channel"].values)

        # Loop through each channel
        for l_study in range(len(split_channels)):
            # Get the current channel and its corresponding scope
            get_channels = split_channels[l_study]
            scope = calibration_input["calibration_scope"].values[l_study]
            study_start = calibration_input["liftStartDate"].values[l_study]
            study_end = calibration_input["liftEndDate"].values[l_study]
            study_pos = df_raw.index[(df_raw['ds'] >= study_start) & (df_raw['ds'] <= study_end)].values

            if study_start in df_raw['ds'].values:
                calib_pos = study_pos
            else:
                calib_pos = [min(study_pos) - 1]
                calib_pos.extend(study_pos)

            calibrate_dates = df_raw.loc[calib_pos, "ds"].values
            calibrate_dates = pd.to_datetime(calibrate_dates).date

            xDecompVec['ds'] = pd.to_datetime(xDecompVec['ds']).dt.date
            mask = xDecompVec['ds'].isin(calibrate_dates)
            calib_pos_rw = xDecompVec.index[mask]
            xDecompVec_filtered = xDecompVec.loc[calib_pos_rw]

            # Initialize the list of calibrated channels for this scope
            l_chn_collect_scope = []
            l_chn_total_collect_scope = []

            # Initialize the list of calibrated channels
            l_chn_collect = {}
            l_chn_total_collect = {}

            # Loop through each position in the channel
            for l_chn in range(len(get_channels)):
                # Get the current channel and its corresponding position
                if scope == 'immediate':
                    channel = get_channels[l_chn]
                    m = df_raw[get_channels[l_chn]].values ## [0]
                    ##pos = np.where(df_raw["ds"] == channel)[0]
                    ## ps = df_raw[df_raw["ds"] == channel]
                    theta = shape = scale = None
                    if adstock == 'geometric':
                        theta = hypParamSam["{}{}".format(get_channels[l_chn], "_thetas")] ##.values[0][0]

                    if adstock.startswith('weibull'):
                        shape = hypParamSam["{}{}".format(get_channels[l_chn], "_shapes")]##.values[0][0]
                        scale = hypParamSam["{}{}".format(get_channels[l_chn], "_scales")]##.values[0][0]

                    x_list = transform_adstock(m, adstock, theta=theta, shape=shape, scale=scale)

                    if adstock == "weibull_pdf":
                        m_imme = x_list['x_imme']
                    else:
                        m_imme = m

                    m_total = x_list['x_decayed']
                    m_coav = m_total - m_imme

                    m_caov_calib = m_coav[calib_pos]
                    m_total_rw = m_total[wind_start:wind_end]

                    alpha = hypParamSam["{}{}".format(get_channels[l_chn], "_alphas")]##.values[0][0]
                    gamma = hypParamSam["{}{}".format(get_channels[l_chn], "_gammas")]##.values[0][0]

                    m_calib_caov_sat = saturation_hill(m_total_rw, alpha = alpha, gamma = gamma, x_marginal = m_caov_calib)
                    coeff_value = coefs.loc[get_channels[l_chn], 's0']
                    m_calib_caov_decomp = m_calib_caov_sat * coeff_value

                    m_calib_total_decomp = xDecompVec.loc[calib_pos_rw, get_channels[l_chn]]
                    m_calib_decomp= m_calib_total_decomp.reset_index(drop=True) - m_calib_caov_decomp.reset_index(drop=True)

                    if scope == 'total':
                        m_calib_decomp = m_calib_total_decomp = xDecompVec[calib_pos_rw, get_channels[l_chn]]

                    l_chn_collect[get_channels[l_chn]] = m_calib_decomp
                    l_chn_total_collect[get_channels[l_chn]] = m_calib_total_decomp

            l_chn_collect = pd.DataFrame(l_chn_collect)
            l_chn_total_collect = pd.DataFrame(l_chn_total_collect)

            if len(get_channels) > 1:
                # Sum across rows if there are multiple channels
                l_chn_collect = l_chn_collect.sum(axis=1)
                l_chn_total_collect = l_chn_total_collect.sum(axis=1)
            else:
                # If there's only one channel, it's already effectively a single "flattened" Series
                l_chn_collect = l_chn_collect.squeeze()
                l_chn_total_collect = l_chn_total_collect.squeeze()

            calibration_input.at[l_study,"pred"] = l_chn_collect.sum()
            calibration_input.at[l_study,"pred_total"] = l_chn_total_collect.sum()
            calibration_input.at[l_study,"decompStart"] = calibrate_dates[0]
            calibration_input.at[l_study,"decompEnd"] = calibrate_dates[1]

        liftCollect = pd.DataFrame(calibration_input)
        liftCollect[['pred', 'pred_total']] = liftCollect[['pred', 'pred_total']] #.astype(float)
        liftCollect[['decompStart', 'decompEnd']] = liftCollect[['decompStart', 'decompEnd']] #.astype(pd.Timestamp)

        liftCollect['liftDays'] = (liftCollect['liftEndDate'] - liftCollect['liftStartDate']) / np.timedelta64(1, 'D')
        liftCollect['decompDays'] = (liftCollect['decompStart'] - liftCollect['decompEnd']) / np.timedelta64(1, 'D')

        liftCollect['decompAbsScaled'] = liftCollect['pred'] / liftCollect['decompDays'] * liftCollect['liftDays']
        liftCollect['decompAbsTotalScaled'] = liftCollect['pred_total'] / liftCollect['decompDays'] * liftCollect['liftDays']

        liftCollect['liftMedia'] = liftCollect['channel']
        liftCollect['liftStart'] = liftCollect['liftStartDate']
        liftCollect['liftEnd'] = liftCollect['liftEndDate']

        liftCollect['calibrated_pct'] = abs((liftCollect['decompAbsScaled'] - liftCollect['liftAbs']) / liftCollect['liftAbs'])
        liftCollect['mape_lift'] = liftCollect['decompAbsScaled'] / liftCollect['decompAbsTotalScaled']

        return liftCollect[['liftMedia',
            'liftStart',
            'liftEnd',
            'liftAbs',
            'decompStart',
            'decompEnd',
            'decompAbsScaled',
            'decompAbsTotalScaled',
            'calibrated_pct',
            'mape_lift']]

================
File: checks.py
================
# Copyright (c) Meta Platforms, Inc. and its affiliates.

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

####################################################################
import pandas as pd
import numpy as np
import re
import datetime
import warnings

from itertools import chain
import os

from datetime import datetime, timedelta
import platform

OPTS_PDN = ["positive", "negative", "default"]

HYPS_NAMES = ["thetas", "shapes", "scales", "alphas", "gammas", "penalty"]

HYPS_OTHERS = ["lambda", "train_size"]

LEGACY_PARAMS = ["cores", "iterations", "trials", "intercept_sign", "nevergrad_algo"]


def check_nas(df: pd.DataFrame):
    """
    Check for missing (NA) values and infinite (Inf) values in a DataFrame.

    Args:
        df (pd.DataFrame): The DataFrame to be checked.

    Raises:
        ValueError: If the DataFrame contains missing or infinite values.

    Returns:
        None
    """
    name = df.__repr__()
    ## Manually added df.isna().sum() instead of `np.count_nan(df)`
    if df.any().isna().sum() > 0 or df.any().isnull().sum() > 0: ## added any()
        ## naVals = lares.missingness(df) ## Lares Missingness determines the naVal Columns
        null_columns = df.columns[df.isnull().any()] ## lares.missingness(df) checks for missing values, this line checks for nulls
        na_columns = df.columns[df.isna().any()] ## lares.missingness(df) checks for missing values, this line checks for na
        cols = null_columns.append(na_columns)
        raise ValueError("Dataset {} contains missing (NA) values.".format(name) + "\nThese values must be removed or fixed "
              "for Robyn to properly work.\nMissing values: {}, ".format(', '.join([cols[i] for i in range])) ## Manually added ValueError
     )
    ## have_inf = [sum(np.isposfinite(x)) for x in df]
    ## Manually changed to, may need to change?
    inf_count = np.isinf(df.all()).values.sum()
    if inf_count: ## removed any to > 0
        col_names = df.columns.to_series()[np.isinf(df).all()]  ## manually added.
        # col_names= ','. join([ names[i] for i , _ in   enumerategenerator])
        raise ValueError("Dataset " + name + "contains Inf values".format(col_names[:-2]) + "having an infinite number of nans\nCheck:"+ ", ".join([col_names[_]] for _, val in zip(enumerate(0)))) ## Manually added ValueError


def check_novar(dt_input, InputCollect=None):
    """
    Check for columns with no variance in the input dataframe.

    Parameters:
    dt_input (pandas.DataFrame): The input dataframe to check.
    InputCollect (dict, optional): Additional input parameters. Default is None.

    Raises:
    ValueError: If there are columns with no variance.

    Returns:
    None
    """

    ## Manual: lares package required?
    ## zerovar method checks for each column:
    ## novar = dt_input.apply(lambda x: x.var() == 0, axis=1)
    novar_columns = list(dt_input.loc[:,dt_input.nunique()==1].columns)
    if len(novar_columns) > 0:
        msg = f"There are {len(novar_columns)} column(s) with no-variance: {novar_columns}"
        if InputCollect is not None:
            msg += f"Please, remove the variable(s) to proceed...\n>>> Note: there's no variance on these variables because of the modeling window filter ({InputCollect['window_start']}:{InputCollect['window_end']})"

        ## Manual : No else case present in the original code.
        ## else:
        ##    msg += "Please, remove the variable(s) to proceed..."
        raise ValueError(msg)

def check_varnames(dt_input, dt_holidays, dep_var, date_var, context_vars, paid_media_spends, organic_vars):
    """
    Check variable names for duplicates and invalid characters.

    Args:
        dt_input (pandas.DataFrame): Input data frame.
        dt_holidays (pandas.DataFrame): Holidays data frame.
        dep_var (str): Name of the dependent variable.
        date_var (str): Name of the date variable.
        context_vars (list): List of context variables.
        paid_media_spends (list): List of paid media spend variables.
        organic_vars (list): List of organic variables.

    Raises:
        ValueError: If there are duplicated variable names or invalid variable names.

    """
    ## dt_input.name = 'dt_input' ## Manual: Added name
    ## dt_holidays.name = 'dt_holidays' ## Manual: Added name

    ## Manually changed to dict, to get its name correctly, name of the variable is lost after converting it to list
    dfs = {"dt_input" : dt_input, "dt_holidays" : dt_holidays}

    for table_name, table in dfs.items():
        ## table_name = key , commented out since, in R code it can read the variable name after it is converted to list

        if table_name == "dt_input":
            var_lists = [dep_var, date_var, context_vars, paid_media_spends, organic_vars, "auto"]
        ## elif table_name == "dt_holidays":, commented out since no else case defined in R code.
        if table_name == "dt_holidays":
            var_lists = ["ds", "country"]
        ## else:, commented out since no else case defined in R code.
        ##    continue

        df = dfs[table_name] ## Manually added access by key instead of index since dfs is a dict.
        ## table_var_list = [var for var in var_lists if var != "auto"], commented out since var lists is a lists of lists therefore additional column name checks can't be performed, flattened list is required.

        table_var_list = list()
        for var in var_lists:
            if isinstance(var, list):
                for v in var:
                    table_var_list.append(v)
            elif var != "auto":
                table_var_list.append(var)

        # Duplicate names
        ## unique_names = set(var_lists) ## Manual: Changed from list to set
        unique_names = [*table_var_list]
        if len(table_var_list) != len(unique_names):
            these = [var for var in table_var_list if df[var].value_counts() > 1]
            raise ValueError(f"You have duplicated variable names for {table_name} in different parameters. Check: {', '.join(these)}")

        # Names with spaces
        with_space = [var for var in table_var_list if re.search("\s+", var)]
        if any(with_space):
            raise ValueError(f"You have invalid variable names on {table_name} with spaces. Please fix columns: {', '.join(with_space)}")


def check_datevar(dt_input, date_var="auto"):
    """
    Checks if the date variable is correct and returns a dictionary with the date variable name,
    interval type, and a tibble object of the input data.

    Parameters:
    - dt_input: The input data as a pandas DataFrame.
    - date_var: The name of the date variable to be checked. If set to "auto", the function will automatically detect the date variable.

    Returns:
    - output: A dictionary containing the following keys:
        - "date_var": The name of the date variable.
        - "dayInterval": The interval between the first two dates.
        - "intervalType": The type of interval (day, week, or month).
        - "dt_input": The input data as a pandas DataFrame.
    """

    """
    Checks if the date variable is correct and returns a dictionary with the date variable name,
    interval type, and a tibble object of the input data.
    """
    # Convert dt_input to a pandas DataFrame
    ## Manually commented out: dt_input = pd.DataFrame(dt_input)

    # Check if date_var is auto
    if date_var == "auto":
        # Find the first column that contains dates
        is_date = np.where(dt_input.apply(lambda x: isinstance(x, datetime.date)).any())[0]

        # If only one date column is found, set date_var to its name
        if len(is_date) == 1:
            date_var = dt_input.columns[is_date[0]]
            print(f"Automatically detected 'date_var': {date_var}")
        else:
            # If multiple date columns are found, raise an error
            raise ValueError("Can't automatically find a single date variable to set 'date_var'")

    # Check if date_var is valid
    ## Manually added instance and length check since string has len > 1 for "DATE", always fails
    if date_var is None or (True if isinstance(date_var, list) and len(date_var) > 1 else False) or date_var not in dt_input.columns:
        raise ValueError("You must provide only 1 correct date variable name for 'date_var'")

    # Arrange the data by the date variable
    # dt_input = dt_input.reindex(columns=[date_var])
    # print("Debug input checks 117: \n", dt_input)

    # Convert the date variable to a Date object
    # dt_input[date_var] = pd.to_datetime(dt_input[date_var], origin="1970-01-01")
    ## Manually added
    dt_input[date_var] = dt_input[date_var].apply(pd.to_datetime)

    # Check if the date variable contains duplicates or invalid dates
    date_var_dates = [dt_input[date_var].iloc[0], dt_input[date_var].iloc[1]]
    ## Added manually
    date_var_dates_set = set(date_var_dates)

    # if any(table(date_var_dates) > 1): manually commented out , need to count date_var_dates
    if len(date_var_dates) > len(date_var_dates_set): ## Added manually
        raise ValueError("Date variable shouldn't have duplicated dates (panel data)")

    if pd.isnull(np.array(date_var_dates)).any() or any(anydate == 'Inf' for anydate in date_var_dates): ## anydate_var_dates).any(): ## Manually changed to isNaN
        raise ValueError("Dates in 'date_var' must have format '2020-12-31' and can't contain NA nor Inf values")

    # Calculate the interval between the first two dates
    dayInterval = (date_var_dates[1] - date_var_dates[0]).days

    # Determine the interval type (day, week, or month)
    intervalType = None
    if dayInterval == 1:
        intervalType = "day"
    elif dayInterval == 7:
        intervalType = "week"
    elif dayInterval in range(28, 31):
        intervalType = "month"
    else:
        raise ValueError(f"{date_var} data has to be daily, weekly or monthly")

    # Return a dictionary with the results
    output = {
        "date_var": date_var,
        "dayInterval": dayInterval,
        "intervalType": intervalType,
        "dt_input": dt_input
    }

    return output


def check_depvar(dt_input, dep_var, dep_var_type):
    if dep_var is None:
        raise ValueError("Must provide a valid dependent variable name for 'dep_var'")
    if dep_var not in dt_input.columns:
        raise ValueError("Must provide a valid dependent name for 'dep_var'")
    if isinstance(dep_var, list) and len(dep_var) > 1: ## Manually changed
        raise ValueError("Must provide only 1 dependent variable name for 'dep_var'")
    ## if not (dt_input[dep_var].dtypes.isnumeric() or dt_input.iloc[:, dep_var].dtypes.isinteger()):
    ## Added manually
    if not (issubclass(dt_input[dep_var].dtypes.type, np.float64) or issubclass(dt_input[dep_var].dtypes.type, np.integer)):
        raise ValueError("'dep_var' must be a numeric or integer variable")
    if dep_var_type is None:
        raise ValueError("Must provide a dependent variable type for 'dep_var_type'")
    if dep_var_type not in ["conversion", "revenue"]:
        raise ValueError("'dep_var_type' must be 'conversion' or 'revenue'")
    ## if len(dep_var_type) != 1, R checks for length for string but no need in Python.


def check_prophet(dt_holidays, prophet_country, prophet_vars, prophet_signs, day_interval):
    # Check if prophet_vars is a valid vector
    ## Manual: Commented out this and called check_vector
    ## if not isinstance(prophet_vars, list) or not all(isinstance(x, str) for x in prophet_vars):
    ##    raise ValueError("prophet_vars must be a list of strings")

    check_vector(prophet_vars)

    if prophet_signs is not None:
        check_vector(prophet_signs)

    ## Wrong translation
    # Check if prophet_signs is a valid
    ## if not isinstance(prophet_signs, list) or not all(isinstance(x, str) for x in prophet_signs):
    ##    raise ValueError("prophet_signs must be a list of strings")
    ## Manually added
    if dt_holidays is None or prophet_vars is None:
        return None

    ## Wrong Translation
    # Check if prophet_country is a valid string
    ## if prophet_country is None or not isinstance(prophet_country, str):
        ## raise ValueError("prophet_country must be a string")

    prophet_vars = [(v.lower()) for v in prophet_vars] ## Manually added

    opts = ["trend", "season", "monthly", "weekday", "holiday"] ## Manually added

    # Check if prophet_vars contains holiday and prophet_country is not None
    if "holiday" not in prophet_vars:
        if prophet_country is not None: ## Manually added nested ifs
            ## warnings.warn("Ignoring prophet_vars = 'holiday' input given your data granularity")
            warnings.warn(f"Input 'prophet_country' is defined as {prophet_country} but 'holiday' is not setup within 'prophet_vars' parameter")
        prophet_country = None ## Manually added

    if not all(pv in opts for pv in prophet_vars): ## Manually added
        raise ValueError(f"Allowed values for `prophet_vars` are:  {opts}") ## Manually added

    # Check if prophet_vars contains weekday and day_interval > 7
    if "weekday" in prophet_vars and day_interval > 7:
        warnings.warn("Ignoring prophet_vars = 'weekday' input given your data granularity")

    # Check if prophet_country is not in dt_holidays$country
    ## Manually added: Changed a lot
    if "holiday" in prophet_vars and (prophet_country is None or prophet_country not in dt_holidays["country"].values):
        ## raise ValueError(f"prophet_country must be one of the countries in dt_holidays$country, got {prophet_country}")
        unique_countries = set(dt_holidays["country"].values)
        country_count = len(unique_countries)
        raise ValueError(f"You must provide 1 country code in 'prophet_country' input. {country_count} countries are included: {unique_countries} If your country is not available, manually include data to 'dt_holidays' or remove 'holidays' from 'prophet_vars' input.")

    ## Manually added
    if prophet_signs is None:
        prophet_signs = ["default"] * len(prophet_vars)

    # Check if prophet_signs is a valid vector of strings
    if not all(x in OPTS_PDN for x in prophet_signs):
        raise ValueError(f"Allowed values for 'prophet_signs' are: {', '.join(OPTS_PDN)}")

    # Check if prophet_signs has the same length as prophet_vars
    if len(prophet_signs) != len(prophet_vars):
        raise ValueError("'prophet_signs' must have the same length as 'prophet_vars'")

    return prophet_signs

## Converted using Code Lama 34B
def check_context(dt_input, context_vars, context_signs):
    if context_vars is not None:
        if context_signs is None:
            context_signs = ["default"] * len(context_vars) ## Manually corrected
        if not all(sign in OPTS_PDN for sign in context_signs): ## Manually corrected
            raise ValueError("Allowed values for 'context_signs' are: " + ", ".join(OPTS_PDN))
        if len(context_signs) != len(context_vars):
            raise ValueError("Input 'context_signs' must have same length as 'context_vars'")
        temp = [var in dt_input.columns for var in context_vars]
        if not all(temp):
            raise ValueError("Input 'context_vars' not included in data. Check: " + str(context_vars[~temp]))
        return context_signs


def check_vector(x):
    ## In R, lists could be like dicts, therefore in this one it checks if it is an array without names (keys)
    ## if isinstance(x, pd.DataFrame) or isinstance(x, list):, manually changed to below
    if not isinstance(x, list): ##  or not isinstance(type(x, np.array.):
        raise ValueError(f"Input '{x}' must be a valid vector")


def check_paidmedia(dt_input, paid_media_vars, paid_media_signs, paid_media_spends):
    # Check if paid_media_spends is provided
    if paid_media_spends is None:
        raise ValueError("Must provide 'paid_media_spends'")

    # Check if paid_media_vars is a vector,
    ## Manually added, check_vector, commented out the check
    ## if not isinstance(paid_media_vars, list):
    ##    raise ValueError("'paid_media_vars' must be a vector")
    check_vector(paid_media_vars)

    # Check if paid_media_signs is a vector
    paid_media_signs = list() ## Manually added, check_vector, commented out the check, check_vector in R also checks if empty
    check_vector(paid_media_signs)
    ## if not isinstance(paid_media_signs, list):
    ##     raise ValueError("'paid_media_signs' must be a vector")

    # Check if paid_media_spends is a vector
    ## Manually added, check_vector, commented out the check
    ## if not isinstance(paid_media_spends, list):
    ##    raise ValueError("'paid_media_spends' must be a vector")
    ## Manully commented out check_adstock(paid_media_spends)
    check_vector(paid_media_spends)

    # Check length of paid_media_vars, paid_media_signs, and paid_media_spends
    media_var_count = len(paid_media_vars)
    spend_var_count = len(paid_media_spends)

    ## Manual, wrong interpretation
    # Check if paid_media_signs is a scalar or a vector of the same length as paid_media_vars
    ## if not (isinstance(paid_media_signs, int) or (isinstance(paid_media_signs, list) and len(paid_media_signs) == media_var_count)):
    ##    raise ValueError("'paid_media_signs' must be a scalar or a vector of the same length as 'paid_media_vars'")

    # Check if paid_media_vars are in dt_input
    temp = [var in dt_input.columns for var in paid_media_vars]
    if not all(temp): ## Manually corrected
        raise ValueError("Input 'paid_media_vars' not included in data. Check: " + str(paid_media_vars))

    temp = [var in dt_input.columns for var in paid_media_spends]
    if not all(temp): ## Manually corrected
        raise ValueError("Input 'paid_media_spends' not included in data. Check: " + str(paid_media_spends[~temp]))

    ## Missed code part, Manually added
    if len(paid_media_signs) == 0:
        paid_media_signs = ["positive"] * media_var_count

    # Check if paid_media_signs are in OPTS_PDN
    ## if not all(paid_media_signs in OPTS_PDN):
    ## Manually added below.
    if not all(x in OPTS_PDN for x in paid_media_signs):
        raise ValueError("Allowed values for 'paid_media_signs' are: " + str(OPTS_PDN))

    # Check if paid_media_signs is a vector of the same length as paid_media_vars
    if len(paid_media_signs) == 1: ## Manually added
        paid_media_signs = paid_media_signs[0] * media_var_count ## Manually added
        ## raise ValueError("Input 'paid_media_signs' must have the same length as 'paid_media_vars'"), manually commented out

    if len(paid_media_signs) != media_var_count:
        raise ValueError("Input 'paid_media_signs' must have the same length as 'paid_media_vars'")

    ## Manually added
    if spend_var_count != media_var_count:
        raise ValueError("Input 'paid_media_spends' must have the same length as 'paid_media_vars'")

    # Check if dt_input[paid_media_vars] are numeric
    ## Manually to be corrected: This part should check all columns of dt_input with names given with paid_media_vars to confirm all numeric
    ## is_num = dt_input[paid_media_vars].apply(lambda x: x.isnumeric())
    ## if not all(is_num):
    if not all(dt_input[paid_media_vars].apply(lambda s: pd.to_numeric(s, errors='coerce').notnull().all()) == True):
        raise ValueError("All your 'paid_media_vars' must be numeric. Check: " + str(paid_media_vars))

    # Check if dt_input[paid_media_vars] are non-negative
    # Manually commented out get_cols = dt_input[paid_media_vars].apply(lambda x: any(x < 0)), need to get unique names first
    vars_list = [paid_media_vars, paid_media_spends]
    unique_cols = list(set().union(*vars_list)) ## Added manually
    ## if pd.to_numeric(dt_input[paid_media_vars], errors='coerce').notnull().all():
    if any(dt_input[unique_cols].lt(0).any()):
        ## check_media_names = dt_input[paid_media_vars].columns[get_cols] , no need.
        ## df_check = dt_input[check_media_names], no need
        negative_list = dt_input[unique_cols].lt(0).any(axis=0).index.to_list() ## Added Manually, check false ones only.
        ## check_media_val = df_check.apply(lambda x: any(x < 0))
        raise ValueError("Contains negative values. Media must be >=0: " + str(negative_list))

    ## Manually corrected
    return {
        "paid_media_signs" : paid_media_signs,
        "mediaVarCount" : media_var_count,
        "paid_media_vars" : paid_media_vars
    }


def check_organicvars(dt_input, organic_vars, organic_signs):
    """
    Checks that the input variables are present in the data and that the signs are valid.
    """
    if organic_vars is None:
        return None

    ## Manually added
    if organic_signs is None:
        organic_signs = list()

    ## Manually corrected: uncomment next section and add check_vector
    # Check that organic_vars is a vector
    ## if not isinstance(organic_vars, list) and not isinstance(organic_vars, np.ndarray):
    ##    raise ValueError("organic_vars must be a vector")
    check_vector(organic_vars)

    ## Manually corrected: uncomment next section and add check_vector
    # Check that organic_signs is a vector
    ## if not isinstance(organic_signs, list) and not isinstance(organic_signs, np.ndarray):
    ##    raise ValueError("organic_signs must be a vector")
    check_vector(organic_signs)

    # Check that organic_vars are present in the data
    temp = [var in dt_input.columns for var in organic_vars]
    if not all(temp):
        raise ValueError("Input 'organic_vars' not included in data. Check:")

    # Check that organic_signs are valid
    ## Manually Corrected to because logical var checks are different in R and Python when it is a Null value
    ## if organic_signs is None:
    if len(organic_signs) == 0 and len(organic_vars) > 0:
        organic_signs = ["positive"] * len(organic_vars)

    ## Wrong translation, Manually commented out
    ## elif not all(sign in ["positive", "negative"] for sign in organic_signs):
    ##    raise ValueError("Allowed values for 'organic_signs' are: positive, negative")
    ## Manually corrected to
    if all(var in OPTS_PDN for var in organic_signs):
       ValueError("Allowed values for 'organic_signs' are: ", OPTS_PDN)

    # Check that organic_signs has the same length as organic_vars
    if len(organic_signs) != len(organic_vars):
        raise ValueError("Input 'organic_signs' must have same length as 'organic_vars'")

    ## MAnually corrected to
    ## return [("organic_signs", organic_signs)]
    return {"organic_signs": organic_signs}


def check_factorvars(dt_input, factor_vars=None, context_vars=None, organic_vars=None):
    """
    Checks if the input variables are numeric and sets factor variables accordingly.
    """

    # Check if factor_vars is a vector
    ## Manually added converting factor_vars to list, otherwise null
    if factor_vars is None:
        factor_vars = []

    ## Below is not necessary for Python, because in R sometimes single vars are passed as a string and then converted to list
    ## if not isinstance(factor_vars, list):
    ##    factor_vars = [factor_vars]

    # Check if context_vars and organic_vars are vectors
    ## Manual: this is unnecessary for python
    ## if not isinstance(context_vars, list):
    ##    context_vars = [context_vars]
    ## if not isinstance(organic_vars, list):
    ##    organic_vars = [organic_vars]
    check_vector(factor_vars)
    check_vector(context_vars)
    check_vector(organic_vars)

    # Select columns from dt_input
    ## Manually corrected to
    # temp = dt_input.select(context_vars, organic_vars)
    temp = dt_input[context_vars + organic_vars]

    # Check if columns are numeric
    ## Manually corrected to get non_numeric columns
    ## are_not_numeric = temp.apply(lambda x: not x.is_numeric).any()
    are_not_numeric = temp.apply(lambda s: not pd.to_numeric(s, errors='coerce').notnull().all()) ## pd.series

    # Find columns that are not numeric and are not in factor_vars
    ## Manually corrected to
    ## these = are_not_numeric[not are_not_numeric.index.isin(factor_vars)]
    if any(are_not_numeric):
        not_numeric = list()
        for val in are_not_numeric.index.values:
            if are_not_numeric[val] == True:
                not_numeric.append(val)

        these = list(np.setdiff1d(not_numeric, factor_vars))

        # Convert these to a list
        ## Redundant for Python, commented out these already a list
        ##these = list(these.index[these])

        # Add these variables to factor_vars
        ## Manually corrected if these is empty or not
        if len(these) > 0:
            print(f"Automatically set these variables as 'factor_vars': {these}")
            factor_vars = factor_vars + these

    # Check if factor_vars are in context_vars and organic_vars
    ## Manually corrected to check all vars, not single line, maybe there are methods for that
    ## if not all(factor_vars in context_vars + organic_vars):
    if factor_vars is not None:
        combined_vars = context_vars + organic_vars
        if not all(var in combined_vars for var in factor_vars):
            raise ValueError("Input 'factor_vars' must be any from 'context_vars' or 'organic_vars' inputs")

    return factor_vars


def check_allvars(all_ind_vars):
    if len(all_ind_vars) != len(set(all_ind_vars)):
        raise ValueError("All input variables must have unique names")


def check_datadim(dt_input, all_ind_vars, rel=10):
    num_obs = dt_input.shape[0]
    if num_obs < len(all_ind_vars) * rel:
        warnings.warn(f"There are {len(all_ind_vars)} independent variables and {num_obs} data points. We recommend row:column ratio of {rel} to 1")
    if dt_input.shape[1] <= 2:
        raise ValueError("Provide a valid 'dt_input' input with at least 3 columns or more")


def check_windows(dt_input, date_var, all_media, window_start=None, window_end=None):
    # Convert date variable to datetime object
    ## Manually corrected: dates_vec is a series need to be np.array or list
    ## dates_vec = pd.to_datetime(dt_input[date_var], origin='1970-01-01')
    dates_vec = pd.to_datetime(dt_input[date_var], format='%Y-%m-%d', origin='unix').values

    # Check and set window_start
    if window_start is None:
        window_start = dates_vec.min()
    else:
        ## Manually corrected, removed
        ## window_start = pd.to_datetime(window_start, format='%Y-%m-%d', origin='1970-01-01')
        window_start = np.datetime64(window_start)
        ## Manually corrected is None
        if window_start is None:
            raise ValueError("Input 'window_start' must have date format, i.e. '{}'".format(datetime.today().strftime('%Y-%m-%d')))
        elif window_start < dates_vec.min():
            window_start = dates_vec.min()
            print("Input 'window_start' is smaller than the earliest date in input data. It's automatically set to the earliest date: {}".format(window_start))

    # Find the index of the closest date to window_start
    ## Manually corrected, removed idxmin to np.argmin to get min index
    rollingWindowStartWhich = np.argmin(abs(dates_vec - window_start)) + 1 ##, R shows 7 but Python has 0 based index.

    if window_start not in dates_vec:
        ## Manually corrected, removed [0] from end of the statement
        window_start = dt_input.loc[rollingWindowStartWhich - 1, date_var]
        print("Input 'window_start' is adapted to the closest date contained in input data: {}".format(window_start))

    refreshAddedStart = window_start

    # Check and set window_end
    if window_end is None:
        window_end = dates_vec.max()
    else:
        ## Manually corrected, removed
        ## window_end = pd.to_datetime(window_end, format='%Y-%m-%d', origin='1970-01-01')
        window_end = np.datetime64(window_end)
        ## Manually corrected is None
        if window_end is None:
            raise ValueError("Input 'window_end' must have date format, i.e. '{}'".format(datetime.today().strftime('%Y-%m-%d')))
        elif window_end > dates_vec.max():
            window_end = dates_vec.max()
            print("Input 'window_end' is larger than the latest date in input data. It's automatically set to the latest date: {}".format(window_end))
        elif window_end < window_start:
            window_end = dates_vec.max()
            print("Input 'window_end' must be >= 'window_start'. It's automatically set to the latest date: {}".format(window_end))

    # Find the index of the closest date to window_end
    ## Manually corrected, removed idxmin to np.argmin to get min index
    rollingWindowEndWhich = np.argmin(abs(dates_vec - window_end)) + 1

    if window_end not in dates_vec:
        ## Manually corrected, removed [0] from end of the statement
        window_end = dt_input.loc[rollingWindowEndWhich - 1, date_var]
        print("Input 'window_end' is adapted to the closest date contained in input data: {}".format(window_end))

    rollingWindowLength = rollingWindowEndWhich - rollingWindowStartWhich + 1

    # Select media channels and check for zeros
    dt_init = dt_input.loc[(rollingWindowStartWhich - 1):(rollingWindowEndWhich - 1), all_media]
    init_all0 = dt_init.select_dtypes(include='number').sum(axis=0) == 0

    if any(init_all0):
        raise ValueError("These media channels contain only 0 within training period {} to {}: {}".format(dt_input.loc[rollingWindowStartWhich - 1, date_var][0], dt_input.loc[rollingWindowEndWhich - 1, date_var][0], ', '.join(dt_init[init_all0.index.values].columns.values)))

    output = {
        'dt_input': dt_input,
        'window_start': window_start,
        'rollingWindowStartWhich': rollingWindowStartWhich,
        'refreshAddedStart': refreshAddedStart,
        'window_end': window_end,
        'rollingWindowEndWhich': rollingWindowEndWhich,
        'rollingWindowLength': rollingWindowLength
    }
    return output


def check_adstock(adstock):
    if adstock is None:
        raise ValueError("Input 'adstock' can't be NULL. Set any of: 'geometric', 'weibull_cdf' or 'weibull_pdf'")

    if adstock == "weibull":
        adstock = "weibull_cdf"

    if adstock not in ["geometric", "weibull_cdf", "weibull_pdf"]:
        raise ValueError("Input 'adstock' must be 'geometric', 'weibull_cdf' or 'weibull_pdf'")

    return adstock


def check_hyperparameters(hyperparameters=None, adstock=None, paid_media_spends=None, organic_vars=None, exposure_vars=None):
    """
    Checks the hyperparameters for the model.
    """
    if hyperparameters is None:
        warnings.warn("Input 'hyperparameters' not provided yet. To include them, run robyn_inputs(InputCollect = InputCollect, hyperparameters = ...)")
        return None
    ## Manually corrected, check columns of hyperparameters DF not dict
    if "train_size" not in hyperparameters.columns:
        hyperparameters["train_size"] = [0.5, 0.8]
        warnings.warn("Automatically added missing hyperparameter range: 'train_size' = c(0.5, 0.8)")

    # Non-adstock hyperparameters check
    check_train_size(hyperparameters)

    # Adstock hyperparameters check
    ## hyperparameters_ordered = hyperparameters.copy()
    hyperparameters_ordered = hyperparameters.copy(deep=True)
    hyperparameters_ordered = hyperparameters_ordered.reindex(sorted(hyperparameters_ordered.columns), axis=1)

    ##get_hyp_names = list(hyperparameters_ordered.keys())
    get_hyp_names = hyperparameters_ordered.columns.values
    ## original_order = [get_hyp_names.index(x) for x in get_hyp_names]
    original_order = hyperparameters.columns.values

    ref_hyp_name_spend = hyper_names(adstock, all_media=paid_media_spends)
    ref_hyp_name_expo = hyper_names(adstock, all_media=exposure_vars)
    ref_hyp_name_org = hyper_names(adstock, all_media=organic_vars)
    ## ref_hyp_name_other = get_hyp_names[get_hyp_names not in HYPS_OTHERS]
    ref_hyp_name_other = [var for var in get_hyp_names if var in HYPS_OTHERS]

    ref_all_media = sorted(ref_hyp_name_spend + ref_hyp_name_org + HYPS_OTHERS)
    ## Added missing lists
    all_ref_names = ref_hyp_name_spend + ref_hyp_name_expo + ref_hyp_name_org + HYPS_OTHERS
    ## all_ref_names = all_ref_names[all_ref_names.index(get_hyp_names)]
    all_ref_names.sort()

    ##if not all(get_hyp_names == all_ref_names):
    if not all([var in all_ref_names for var in get_hyp_names]):
        wrong_hyp_names = [x for x in get_hyp_names if x not in all_ref_names]
        raise ValueError(f"Input 'hyperparameters' contains following wrong names: {', '.join(wrong_hyp_names)}")

    total = len(get_hyp_names)
    total_in = len(ref_hyp_name_spend + ref_hyp_name_org + ref_hyp_name_other)
    if total != total_in:
        raise ValueError(f"{total} hyperparameter values are required, and {total_in} were provided.")

    # Old workflow: replace exposure with spend hyperparameters
    ## if any(get_hyp_names == ref_hyp_name_expo):
    if any([var in ref_hyp_name_expo for var in get_hyp_names]):
        get_expo_pos = [i for i, x in enumerate(get_hyp_names) if x in ref_hyp_name_expo]
        get_hyp_names[get_expo_pos] = ref_all_media[get_expo_pos]
        hyperparameters_ordered = {x: y for x, y in zip(get_hyp_names, hyperparameters_ordered)}

    check_hyper_limits(hyperparameters_ordered, "thetas")
    check_hyper_limits(hyperparameters_ordered, "alphas")
    check_hyper_limits(hyperparameters_ordered, "gammas")
    check_hyper_limits(hyperparameters_ordered, "shapes")
    check_hyper_limits(hyperparameters_ordered, "scales")

    return hyperparameters_ordered


def check_train_size(hyps):
    if "train_size" in hyps:
        if not 1 <= len(hyps["train_size"]) <= 2:
            raise ValueError("Hyperparameter 'train_size' must be length 1 (fixed) or 2 (range)")
        if any(hyps["train_size"] <= 0.1) or any(hyps["train_size"] > 1):
            raise ValueError("Hyperparameter 'train_size' values must be defined between 0.1 and 1")


def check_hyper_limits(hyperparameters, hyper):
    #hyper_which = [i for i, v in enumerate(hyperparameters) if v.endswith(hyper)]
    hyper_which = [v for v in hyperparameters if v.endswith(hyper)]
    ## if not hyper_which:
    if len(hyper_which) == 0:
        return
    limits = hyper_limits()[hyper]
    for i in hyper_which:
        values = hyperparameters[i]
        # Lower limit
        ## ineq = f"{values[0]} <= {limits[0]}"
        ineq = f"{values[0]}{limits[0]}"
        ##lower_pass = eval(parse(text=ineq))
        lower_pass = eval(ineq)
        if not lower_pass:
            raise ValueError(f"{hyperparameters.name[i]}'s hyperparameter must have lower bound {limits[0]}")
        # Upper limit
        ## ineq = f"{values[1]} <= {limits[1]}"
        ineq = f"{values[1]}{limits[1]}"
        ## upper_pass = eval(parse(text=ineq)) or len(values) == 1
        upper_pass = eval(ineq) or len(values) == 1
        if not upper_pass:
            raise ValueError(f"{hyperparameters.keys()[i]}'s hyperparameter must have upper bound {limits[1]}")
        # Order of limits
        order_pass = True if values[0] <= values[1] else False
        if not order_pass:
            raise ValueError(f"{hyperparameters.name[i]}'s hyperparameter must have lower bound first and upper bound second")


def check_calibration(dt_input, date_var, calibration_input, dayInterval, dep_var,
                      window_start, window_end, paid_media_spends, organic_vars):
    ## To be debugged when calibration_input provided
    if calibration_input is not None:
        ## calibration_input = pd.DataFrame(calibration_input)
        these = ["channel", "liftStartDate", "liftEndDate", "liftAbs", "spend", "confidence", "metric", "calibration_scope"]
        if not all([True if this in calibration_input.columns.values else False for this in these]): ## Added values
            raise ValueError("Input 'calibration_input' must contain columns: " + str(these) + ". Check the demo script for instruction.")
        ## Convert int64 values into float
        calibration_input["liftAbs"] = calibration_input["liftAbs"].astype(float)
        if not all(calibration_input["liftAbs"].apply(lambda x: isinstance(x, float) and not np.isnan(x))):
            raise ValueError("Check 'calibration_input$liftAbs': all lift values must be valid numerical numbers")
        all_media = paid_media_spends + organic_vars
        cal_media = calibration_input["channel"].apply(lambda x: re.split(r'[^\w_]+', x)).tolist()
        cal_media_flat = list(chain.from_iterable(cal_media))
        if not all(item in all_media for item in cal_media_flat):
            these = [item for item in cal_media_flat if item not in all_media]
            raise ValueError("All channels from 'calibration_input' must be any of: " + str(all_media) + ". Check: " + str(these))
        for i in range(len(calibration_input)):
            temp = calibration_input.iloc[i]
            if temp["liftStartDate"] < window_start or temp["liftEndDate"] > window_end:
                raise ValueError("Your calibration's date range for " + temp["channel"] + " between " + temp["liftStartDate"] + " and " + temp["liftEndDate"] + " is not within modeling window (" + window_start + " to " + window_end + "). Please, remove this experiment from 'calibration_input'.")
            if temp["liftStartDate"] > temp["liftEndDate"]:
                raise ValueError("Your calibration's date range for " + temp["channel"] + " between " + temp["liftStartDate"] + " and " + temp["liftEndDate"] + " should respect liftStartDate <= liftEndDate. Please, correct this experiment from 'calibration_input'.")
        if "spend" in calibration_input.columns.values:
            for i in range(len(calibration_input["channel"])): ## added channel
                temp = calibration_input.iloc[i]
                temp2 = cal_media[i]
                if all([item in organic_vars for item in temp2]):
                    continue
                dt_input_spend = dt_input.loc[(dt_input[date_var] >= temp["liftStartDate"]) & (dt_input[date_var] <= temp["liftEndDate"]), temp2].sum().round(0)
                if (dt_input_spend > temp["spend"] * 1.1).any() or (dt_input_spend < temp["spend"] * 0.9).any():
                    warnings.warn("Your calibration's spend (" + str(temp["spend"]) + ") for " + temp["channel"] + " between " + temp["liftStartDate"].strftime('%Y-%m-%d') + " and " + temp["liftEndDate"].strftime('%Y-%m-%d') + " does not match your dt_input spend (~" + str(dt_input_spend) + "). Please, check again your dates or split your media inputs into separate media channels.")
        if "confidence" in calibration_input.columns:
            for i in range(len(calibration_input)):
                temp = calibration_input.iloc[i]
                if temp["confidence"] < 0.8:
                    warnings.warn("Your calibration's confidence for " + temp["channel"] + " between " + temp["liftStartDate"] + " and " + temp["liftEndDate"] + " is lower than 80%%, thus low-confidence. Consider getting rid of this experiment and running it again.")
        if "metric" in calibration_input.columns:
            for i in range(len(calibration_input)):
                temp = calibration_input.iloc[i]
                if temp["metric"] != dep_var:
                    raise ValueError("Your calibration's metric for " + temp["channel"] + " between " + temp["liftStartDate"] + " and " + temp["liftEndDate"] + " is not '" + dep_var + "'. Please, remove this experiment from 'calibration_input'.")
        if "scope" in calibration_input.columns: ## removed calibration_ before scope
            these = ["immediate", "total"]
            if not all([item in these for item in calibration_input["calibration_scope"]]):
                raise ValueError("Inputs in 'calibration_input$calibration_scope' must be any of: " + str(these))
    return calibration_input


def check_obj_weight(calibration_input, objective_weights, refresh):
    obj_len = 2 if isinstance(calibration_input, type(None)) else 3
    if not isinstance(objective_weights, type(None)):
        if len(objective_weights) != obj_len:
            raise ValueError(f"objective_weights must have length of {obj_len}")
        if any([weight < 0 or weight > 10 for weight in objective_weights]):
            raise ValueError("objective_weights must be >= 0 & <= 10")
    if isinstance(objective_weights, type(None)) and refresh:
        if obj_len == 2:
            objective_weights = [1, 10]
        else:
            objective_weights = [1, 10, 10]
    return objective_weights


def check_iteration(calibration_input, iterations, trials, hyps_fixed, refresh):
    if not refresh:
        if not hyps_fixed:
            if calibration_input is None and (iterations < 2000 or trials < 5):
                warnings.warn("We recommend to run at least 2000 iterations per trial and 5 trials to build initial model")
            else:
                if iterations < 2000 or trials < 10:
                    warnings.warn(f"You are calibrating MMM. We recommend to run at least 2000 iterations per trial and {10} trials to build initial model")
    return


## def check_InputCollect(list):
def check_input_collect(input_collect):
    names_list = ["dt_input", "paid_media_vars", "paid_media_spends", "context_vars", "organic_vars", "all_ind_vars", "date_var", "dep_var",
                  "rollingWindowStartWhich", "rollingWindowEndWhich", "mediaVarCount", "factor_vars", "prophet_vars", "prophet_signs", "prophet_country",
                  "intervalType", "dt_holidays"]
    if not all([var in input_collect.keys() for var in names_list]):
        not_present = [name for name in names_list if name not in input_collect.keys()]
        raise ValueError(f"Some elements where not provided in your inputs list: {', '.join(not_present)}")

    if len(input_collect['dt_input']) <= 1:
        raise ValueError('Check your \'dt_input\' object')

    return


def check_robyn_name(robyn_object, quiet=False):
    if not isinstance(robyn_object, type(None)):
        if not os.path.exists(robyn_object):
            file_end = os.path.basename(robyn_object)[-4:]
            if file_end != '.RDS':
                raise ValueError("Input 'robyn_object' must has format .RDS")
        else:
            if not quiet:
                print(f"Skipping export into RDS file")
    return


def check_dir(plot_folder):
    file_end = os.path.basename(plot_folder)[-3:]
    if file_end == '.RDS':
        plot_folder = os.path.dirname(plot_folder)
        print(f"Using robyn object location: {plot_folder}")
    else:
        plot_folder = os.path.join(os.path.dirname(plot_folder), os.path.basename(plot_folder))
    if not os.path.exists(plot_folder):
        plot_folder = os.getcwd()
        print(f"WARNING: Provided 'plot_folder' doesn't exist. Using current working directory: {plot_folder}")
    return plot_folder


def check_calibconstr(calibration_constraint, iterations, trials, calibration_input, refresh):
    if calibration_input.empty and not refresh:
        total_iters = iterations * trials
        if calibration_constraint < 0.01 or calibration_constraint > 0.1:
            warnings.warn(f"Input 'calibration_constraint' must be >= 0.01 and <= 0.1. Changed to default: 0.1")
            calibration_constraint = 0.1
        models_lower = 500
        if total_iters * calibration_constraint < models_lower:
            warnings.warn(f"Input 'calibration_constraint' set for top {calibration_constraint * 100}% calibrated models. {round(total_iters * calibration_constraint, 0)} models left for pareto-optimal selection. Minimum suggested: {models_lower}")
    return calibration_constraint


def check_hyper_fixed(input_collect, dt_hyper_fixed, add_penalty_factor):
    hyper_fixed = False if dt_hyper_fixed is None else True ## manually fixed
    hyp_param_sam_name = hyper_names(adstock=input_collect['adstock'], all_media=input_collect['all_media'])
    hyp_param_sam_name = np.concatenate((hyp_param_sam_name, HYPS_OTHERS))
    if add_penalty_factor:
        for_penalty = np.array(input_collect.dt_mod.columns)[np.logical_not(np.isin(input_collect.dt_mod.columns, ['ds', 'dep_var']))]
        hyp_param_sam_name = np.concatenate((hyp_param_sam_name, [f'{p}_penalty' for p in for_penalty]))
    if hyper_fixed:
        dt_hyper_fixed = pd.DataFrame(dt_hyper_fixed)
        if len(dt_hyper_fixed) != 1:
            raise ValueError("Provide only 1 model / 1 row from OutputCollect$resultHypParam or pareto_hyperparameters.csv from previous runs")
        if not all(hyp_param_sam_name in dt_hyper_fixed.columns):
            raise ValueError("Input 'dt_hyper_fixed' is invalid. Please provide 'OutputCollect$resultHypParam' result from previous runs or 'pareto_hyperparameters.csv' data with desired model ID. Missing values for:", hyp_param_sam_name)
    return {'hyper_fixed': hyper_fixed, 'hyp_param_sam_name': hyp_param_sam_name}


def check_parallel():
    ##return 'unix' in platform().OS.type
    return platform.system() == 'Linux' or platform.system() == 'Darwin'


def check_parallel_plot():
    ##return 'Darwin' not in Sys.info()['sysname']
    return platform.system() != 'Darwin'


def check_init_msg(input_collect, cores):
    ## opt = sum(lapply(input_collect['hyper_updated'], len) == 2)
    ## fix = sum(lapply(input_collect.hyper_updated, len) == 1)
    opt = 0
    fix = 0
    for key, value in input_collect['hyper_updated'].items():
        if len(value) == 2:
            opt += 1
        elif len(value) == 1:
            fix += 1

    det = f"({opt} to iterate + {fix} fixed)"
    base = f"Using {input_collect['adstock']} adstocking with {len(input_collect['hyper_updated'])} hyperparameters {det}"
    if cores == 1:
        print(base + " with no parallel computation")
    else:
        if check_parallel():
            print(base + " on " + str(cores) + " cores")
        else:
            print(base + " on 1 core (Windows fallback)")


def check_class(x: list, object):
    for c in x: ## Manually fixed
        if not isinstance(c, object):
            raise ValueError(f"Input object must be class {x}")
    ## if not x in class(object):


def check_allocator_constrains(low, upr):
    if (low < 0).any():
        raise ValueError("Inputs 'channel_constr_low' must be >= 0")
    if len(upr) != len(low):
        raise ValueError("Inputs 'channel_constr_up' and 'channel_constr_low' must have the same length or length 1")
    if (upr < low).any():
        raise ValueError("Inputs 'channel_constr_up' must be >= 'channel_constr_low'")



# def check_allocator(OutputCollect, select_model, paid_media_spends, scenario, channel_constr_low, channel_constr_up, constr_mode):
#     check_allocator_constrains(channel_constr_low, channel_constr_up)
#     if select_model not in OutputCollect["allSolutions"]:
#         raise ValueError(f"Provided 'select_model' is not within the best results.")
#     if scenario not in ("max_response", "target_efficiency"):
#         raise ValueError(f"Input 'scenario' must be one of: {', '.join(('max_response', 'target_efficiency'))}")
#     if scenario == "target_efficiency" and not (channel_constr_low is None or channel_constr_up is None):
#         raise ValueError("Input 'channel_constr_low' and 'channel_constr_up' must be None for scenario 'target_efficiency'")
#     if len(channel_constr_low) != 1 and len(channel_constr_low) != len(paid_media_spends):
#         raise ValueError(f"Input 'channel_constr_low' have to contain either only 1 value or have same length as 'paid_media_spends': {len(paid_media_spends)}")
#     if len(channel_constr_up) != 1 and len(channel_constr_up) != len(paid_media_spends):
#         raise ValueError(f"Input 'channel_constr_up' have to contain either only 1 value or have same length as 'paid_media_spends': {len(paid_media_spends)}")
#     if constr_mode not in ("eq", "ineq"):
#         raise ValueError(f"Input 'constr_mode' must be one of: {', '.join(('eq', 'ineq'))}")
#     return scenario


def check_allocator(OutputCollect, select_model, paid_media_spends, scenario, channel_constr_low, channel_constr_up, constr_mode):
    check_allocator_constrains(channel_constr_low, channel_constr_up)
    if select_model not in OutputCollect['allSolutions']:
        raise ValueError(
            "Provided 'select_model' is not within the best results. Try any of: " +
            ', '.join(OutputCollect['allSolutions'])
        )
    if "max_historical_response" in scenario: 
        scenario = "max_response"
    opts = ["max_response", "target_efficiency"] # Deprecated: max_response_expected_spend
    if scenario not in opts:
        raise ValueError("Input 'scenario' must be one of: " + ', '.join(opts))
    if not (scenario == "target_efficiency" and channel_constr_low is None and channel_constr_up is None):
        if len(channel_constr_low) != 1 and len(channel_constr_low) != len(paid_media_spends):
            raise ValueError(
                "Input 'channel_constr_low' have to contain either only 1" +
                "value or have same length as 'InputCollect$paid_media_spends':" + str(len(paid_media_spends))
            )
        if len(channel_constr_up) != 1 and len(channel_constr_up) != len(paid_media_spends):
            raise ValueError(
                "Input 'channel_constr_up' have to contain either only 1" +
                "value or have same length as 'InputCollect$paid_media_spends':" + str(len(paid_media_spends))
            )
    opts = ["eq", "ineq"]
    if constr_mode not in opts:
        raise ValueError("Input 'constr_mode' must be one of: " + ', '.join(opts))
    return scenario


def check_metric_type(metric_name, paid_media_spends, paid_media_vars, exposure_vars, organic_vars):

    if paid_media_spends.count(metric_name) == 1:
        metric_type = "spend"
    elif exposure_vars.count(metric_name) == 1:
        metric_type = "exposure"
    elif organic_vars.count(metric_name) == 1:
        metric_type = "organic"
    else:
        raise ValueError(f"Invalid 'metric_name' input: {metric_name}")
    return metric_type

def format_date(date):
    if isinstance(date, np.datetime64):
        date = str(np.datetime_as_string(date, unit='D'))
        date = datetime.strptime(date, '%Y-%m-%d')
        return date.strftime("%Y-%m-%d")
    elif isinstance(date, datetime):
        return date.strftime("%Y-%m-%d")
    else:
        raise TypeError("Unsupported date type")

def check_metric_dates(date_range=None, all_dates=None, day_interval=None, quiet=False, is_allocator=False): ## Manually fixed, moved all_dates to first argument.
    """
    Checks the date range and returns the updated date range and location.
    """
    if date_range is None:
        if day_interval is None:
            raise ValueError("Input 'date_range' or 'dayInterval' must be defined")

        date_range = "all"
        if not quiet:
            print(f"Automatically picked date_range = '{date_range}'")

    if "last" in date_range or "all" in date_range:
        # Using last_n as date_range range
        if "all" in date_range:
            date_range = ["last_" + str(len(all_dates))]
        get_n = int(date_range[0].replace("last_", "")) if "last_" in date_range[0] else 1
        date_range = all_dates[-get_n:]
        # date_range_updated = [date for date in all_dates if date in date_range]
        date_range_updated = np.intersect1d(all_dates, date_range)

        date_range_loc = range(len(date_range_updated))

        min_date = min(date_range_updated)
        max_date = max(date_range_updated)

        min_date_formatted = format_date(min_date)
        max_date_formatted = format_date(max_date)

        rg = ":".join([min_date_formatted, max_date_formatted])

        # TODO: Need to test the else statement
    else:
        # Using dates as date_range range
        try:
            date_range = pd.to_datetime(date_range, origin='unix', unit='s')
            if len(date_range) == 1:
                # Using only 1 date
                if date_range in all_dates.values:
                    date_range_updated = date_range
                    date_range_loc = all_dates[all_dates == date_range].index
                    if not quiet:
                        print(f"Using ds '{date_range_updated[0]}' as the response period")
                else:
                    date_range_loc = (all_dates - date_range).abs().argmin()
                    date_range_updated = all_dates.iloc[date_range_loc]
                    if not quiet:
                        print(f"Input 'date_range' ({date_range}) has no match. Picking closest date: {date_range_updated}")
            elif len(date_range) == 2:
                # Using two dates as "from-to" date range
                date_range_loc = [all_dates.sub(date).abs().idxmin() for date in date_range]
                date_range_loc = range(date_range_loc[0], date_range_loc[1] + 1)
                date_range_updated = all_dates.iloc[date_range_loc]
                if not quiet and not set(date_range).issubset(date_range_updated.values):
                    print(f"At least one date in 'date_range' input do not match any date. Picking closest dates for range: {date_range_updated.min()}:{date_range_updated.max()}")
                rg = ":".join(map(str, date_range_updated.agg(['min', 'max'])))
                get_n = len(date_range_loc)
            else:
                # Manually inputting each date
                date_range_updated = date_range
                if set(date_range).issubset(all_dates.values):
                    date_range_loc = all_dates.index[all_dates.isin(date_range_updated)]
                else:
                    date_range_loc = [all_dates.sub(date).abs().idxmin() for date in date_range]
                    rg = ":".join(map(str, pd.to_datetime(date_range).agg(['min', 'max'])))
                if pd.Series(date_range_loc).diff().dropna().eq(1).all():
                    date_range_updated = all_dates.iloc[date_range_loc]
                    if not quiet:
                        print(f"At least one date in 'date_range' do not match ds. Picking closest date: {date_range_updated}")
                else:
                    raise ValueError("Input 'date_range' needs to have sequential dates")
        except ValueError:
            raise ValueError("Input 'date_range' must have date format '2023-01-01' or use 'last_n'")

    return {
        'date_range_updated': date_range_updated,
        'metric_loc': date_range_loc
    }


def check_metric_value(metric_value, metric_name, all_values, metric_loc):
    """
    Checks the metric value and returns the updated metric value and location.
    """
    # if np.any(np.isnan(metric_value)): TODO: Check if this is necessary
    #     metric_value = None

    if not metric_value is None:
        if not np.isreal(metric_value):
            raise ValueError(f"Input 'metric_value' for {metric_name} must be a numerical value")

        if np.any(metric_value < 0):
            raise ValueError(f"Input 'metric_value' for {metric_name} must be positive")

        if len(metric_loc) > 1 and len(metric_value) == 1:
            metric_value_updated = np.array(metric_value / len(metric_loc))
            # print(f"'metric_value' {metric_value} splitting into {len(metric_loc)} periods evenly")
        else:
            if len(metric_value) != len(metric_loc):
                raise ValueError("robyn_response metric_value & date_range must have same length")
            metric_value_updated = metric_value

    else:
        metric_value_updated = [all_values[i] for i in metric_loc]

    all_values_updated = all_values.copy()
    for i, value in zip(metric_loc, metric_value_updated):
        all_values_updated[i] = value


    return {"metric_value_updated" : metric_value_updated, "all_values_updated" : all_values_updated}


def check_legacy_input(InputCollect, cores=None, iterations=None, trials=None, intercept_sign=None, nevergrad_algo=None):
    """
    Checks for legacy input parameters and warns the user if they are used.
    """
    if not any([var in InputCollect.keys() for var in LEGACY_PARAMS]):
        return InputCollect

    # Warn the user these InputCollect params will be (are) deprecated
    legacy_values = InputCollect[LEGACY_PARAMS]
    legacy_values = [x for x in legacy_values if x is not None] ## Manually fixed
    if len(legacy_values) > 0:
        warnings.warn(f"Using legacy InputCollect values. Please set {', '.join(LEGACY_PARAMS)} within robyn_run() instead")

    # Overwrite InputCollect with robyn_run() inputs
    if cores is not None:
        InputCollect['cores'] = cores
    if iterations is not None:
        InputCollect['iterations'] = iterations
    if trials is not None:
        InputCollect['trials'] = trials
    if intercept_sign is not None:
        InputCollect['intercept_sign'] = intercept_sign
    if nevergrad_algo is not None:
        InputCollect['nevergrad_algo'] = nevergrad_algo

    InputCollect['deprecated_params'] = True
    return InputCollect


def check_run_inputs(cores, iterations, trials, intercept_sign, nevergrad_algo):
    """
    Checks that the inputs for robyn_run() are valid.
    """
    if iterations is None:
        raise ValueError("Must provide 'iterations' in robyn_run()")
    if trials is None:
        raise ValueError("Must provide 'trials' in robyn_run()")
    if nevergrad_algo is None:
        raise ValueError("Must provide 'nevergrad_algo' in robyn_run()")
    opts = ['non_negative', 'unconstrained']
    if intercept_sign not in opts:
        raise ValueError(f"Input 'intercept_sign' must be any of: {', '.join(opts)}")


def check_daterange(date_min, date_max, dates):
    """
    Checks that the date range for the data is valid.
    """
    if date_min is not None:
        if len(date_min) > 1:
            raise ValueError("Set a single date for 'date_min' parameter")
        if date_min < min(dates):
            warnings.warn(f"Parameter 'date_min' not in your data's date range. Changed to '{min(dates)}'")

    if date_max is not None:
        if len(date_max) > 1:
            raise ValueError("Set a single date for 'date_max' parameter")
        if date_max > max(dates):
            warnings.warn(f"Parameter 'date_max' not in your data's date range. Changed to '{max(dates)}'")


def check_refresh_data(Robyn, dt_input):
    """
    Checks that the refresh data is valid.
    """
    original_periods = len(Robyn['listInit']['InputCollect']['dt_modRollWind'])
    new_periods = len(filter(dt_input, get(Robyn['listInit']['InputCollect']['date_var']) > Robyn['listInit']['InputCollect']['window_end']))
    it = Robyn['listInit']['InputCollect']['intervalType']
    if new_periods > 0.5 * (original_periods + new_periods):
        warnings.warn(f"We recommend re-building a model rather than refreshing this one. More than 50%% of your refresh data ({original_periods + new_periods} {it}) is new data ({new_periods} {it})")


def expand_grid_helper(grep_list, names_list, expand_list):
    """
     Following R method is not available in python.
     local_name <- sort(apply(expand.grid(all_media, HYPS_NAMES[
        grepl("thetas|alphas|gammas", HYPS_NAMES)
        ]), 1, paste, collapse = "_"))
     Therefore, created this helper method
    """
    expanded_list = list()
    if expand_list is None:
        expand_list = list()

    for i in range(len(expand_list)):
        for j in range(len(names_list)):
            if names_list[j] in grep_list:
                new_elem = expand_list[i] + "_" + names_list[j]
                expanded_list.append(new_elem)

    return expanded_list

## Manually added, in R it is in inputs but not possible in Python since it creates transitive dependency
def hyper_names(adstock, all_media):
    adstock = check_adstock(adstock)
    if adstock == "geometric":
        local_name = sorted(expand_grid_helper(["thetas", "alphas", "gammas"], HYPS_NAMES, all_media))
    elif adstock in ["weibull_cdf", "weibull_pdf"]:
        local_name = sorted(expand_grid_helper(["shapes", "scales", "alphas", "gammas"], HYPS_NAMES, all_media))
    return local_name


## Manually added, in R it is in inputs but not possible in Python since it creates transitive dependency
def hyper_limits():
    return pd.DataFrame(
        {
            "thetas": [">=0", "<1"],
            "alphas": [">0", "<10"],
            "gammas": [">0", "<=1"],
            "shapes": [">=0", "<20"],
            "scales": [">=0", "<=1"],
        }
    )

================
File: cluster.py
================
# Copyright (c) Meta Platforms, Inc. and its affiliates.

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

####################################################################
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re

import seaborn as sns
import statsmodels.api as sm
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
##from sklearn.stats import bootstrap
##from sklearn.bootstrapping import Bootstrap
## from sklearn.plot_utils import plot_layout
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import f1_score
from matplotlib.ticker import FormatStrFormatter
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
## from sklearn.preprocessing import Dropna
from scipy.stats import norm
import scipy.stats as stats


## Manual imports

def determine_optimal_k(df, max_clusters, random_state=42):
    wss = []
    K_range = range(1, max_clusters + 1)
    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=random_state).fit(df)
        wss.append(kmeans.inertia_)

    # Calculate the second derivative of the WSS
    # The second derivative is a simple way to find the inflection point where the rate of decrease
    # of WSS changes significantly, corresponding to the elbow
    second_derivative = np.diff(wss, n=2)

    # The optimal k is where the second derivative is maximized
    optimal_k = np.argmax(second_derivative) + 2  # +2 because np.diff reduces the original array by 1 for each differentiation and we start counting from 1

    return optimal_k

def clusterKmeans_auto(df, min_clusters=3, limit_clusters=10, seed=None):
    features = df.select_dtypes(include=[np.number])  # Assuming numerical columns for clustering
    features.columns = features.columns.astype(str)
    features.columns = [str(col) for col in features.columns]
    # Determine the range of k values to try
    k_range = range(1, limit_clusters + 1)

    # Calculate WSS for each k
    wss = []
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=seed)
        kmeans.fit(features)
        wss.append(kmeans.inertia_)

    # Plot WSS to identify the elbow (optional visualization step)
    plt.figure(figsize=(8, 4))
    plt.plot(k_range, wss, 'bo-', markersize=8, lw=2)
    plt.title('Elbow Method For Optimal k')
    plt.xlabel('Number of Clusters k')
    plt.ylabel('Within-Cluster Sum of Squares (WSS)')
    plt.grid(True)
    plt.show()

    # Optionally, automatically determine the optimal k based on the elbow method or other criteria
    # This part is simplified; more sophisticated methods could be applied for determining 'k'
    optimal_k = determine_optimal_k(features, 20)
    optimal_k = max(optimal_k, min_clusters)
    if optimal_k >= 4:
        optimal_k = 3

    # Perform final clustering with determined optimal k
    limit_clusters = min(len(df) - 1, 30)
    final_kmeans = KMeans(n_clusters=optimal_k, max_iter=limit_clusters, random_state=seed, tol=0.05)
    final_kmeans.fit(features)

    # Perform PCA for dimensionality reduction
    pca = PCA(n_components=optimal_k)
    df_pca = pca.fit_transform(features)
    # Perform t-SNE for dimensionality reduction
    tsne = TSNE(n_components=optimal_k, random_state=seed)
    df_tsne = tsne.fit_transform(features)

    # Adding cluster labels to the original DataFrame
    df['cluster'] = final_kmeans.labels_

    return df, optimal_k, wss, final_kmeans, df_pca, df_tsne

def plot_wss_and_save(wss, path, dpi=500, width=5, height=4):
    """
    Creates and saves a WSS plot.

    Args:
    - wss: Array of WSS values.
    - path: File path for the saved plot.
    - dpi: Dots per inch (resolution) of the saved plot.
    - width: Width of the figure in inches.
    - height: Height of the figure in inches.
    """
    # Create the plot
    plt.figure(figsize=(width, height))
    k_values = range(1, len(wss) + 1)
    plt.plot(k_values, wss, marker='o', linestyle='-', color='blue')
    plt.title('WSS vs. Number of Clusters')
    plt.xlabel('Number of Clusters')
    plt.ylabel('WSS')
    plt.grid(True, which='both', linestyle='--', linewidth=0.5, color='gray')
    plt.tight_layout()

    # Set background to white
    plt.gca().set_facecolor('white')
    plt.gcf().set_facecolor('white')

    # Save the plot
    plt.savefig(path, dpi=dpi, bbox_inches='tight')
    plt.close()

def robyn_clusters(input, dep_var_type, cluster_by='hyperparameters', all_media=None, k='auto', limit=1, weights=None, dim_red='PCA', quiet=False, export=False, seed=123):
    """
    Clusters the data based on specified parameters and returns a dictionary containing various outputs.

    Parameters:
    - input: The input data, either a robyn_outputs object or a dataframe.
    - dep_var_type: The type of dependent variable ('continuous' or 'categorical').
    - cluster_by: The variable to cluster by, either 'hyperparameters' or 'performance'. Default is 'hyperparameters'.
    - all_media: The list of media variables. Default is None.
    - k: The number of clusters. Default is 'auto'.
    - limit: The maximum number of top solutions to select. Default is 1.
    - weights: The weights for balancing the clusters. Default is None.
    - dim_red: The dimensionality reduction technique to use. Default is 'PCA'.
    - quiet: Whether to suppress print statements. Default is False.
    - export: Whether to export the results. Default is False.
    - seed: The random seed for reproducibility. Default is 123.

    Returns:
    - output: A dictionary containing various outputs such as cluster data, cluster confidence intervals, number of clusters, etc.
    """
    # Set seed for reproducibility
    np.random.seed(seed)

    # Check that cluster_by is a valid option
    if cluster_by not in ['performance', 'hyperparameters']:
        raise ValueError("cluster_by must be either 'performance' or 'hyperparameters'")

    # Load data
    if all_media is None:
        aux = input["mediaVecCollect"].columns
        if "type" in aux:
            type_index = list(aux).index("type")
            all_media = aux[1:type_index]  # Exclude the first column and from "type" onwards, Python uses 0-based indexing
        else:
            all_media = aux[1:-1]  # If "type" is not found, exclude the first and last columns as a fallback

        path = input["plot_folder"]


    # Pareto and ROI data
    x = input["xDecompAgg"]
    if cluster_by == 'hyperparameters':
        x = input["resultHypParam"]
    df = prepare_df(x, all_media, dep_var_type, cluster_by)

    ignore = ["solID", "mape", "mape.qt10", "decomp.rssd", "nrmse", "nrmse_test", "nrmse_train", "nrmse_val", "pareto"]
    # Auto K selected by less than 5% WSS variance (convergence)\
    min_clusters = 3
    limit_clusters = min(len(df) - 1, 30)
    features= df.drop(columns=ignore, errors='ignore')
    df_pca = None
    df_tsne = None
    if k == 'auto':
        try:
            # You must determine the appropriate number of clusters beforehand, as `n_clusters=None` is not valid.
            # This placeholder (e.g., 3) is for demonstration; you need a dynamic method or a fixed value.
            # determined_clusters = determine_optimal_k(features, 20)
            # cls = KMeans(n_clusters=determined_clusters, max_iter=limit_clusters, random_state=seed, tol=0.05).fit(features)
            df, optimal_k, wss, cls, df_pca, df_tsne = clusterKmeans_auto(df, min_clusters, limit_clusters=limit_clusters, seed=seed)
        except Exception as e:
            print(f"Couldn't automatically create clusters: {e}")
            cls = None

        # Ensure `cls` is not `None` before accessing its attributes
        if cls is not None:
            k = cls.n_clusters
        else:
            k = 0  # Or handle this case as needed, perhaps setting it to `min_clusters` or another default

        # Now, proceed with your logic
        if k < min_clusters:
            k = min_clusters
        print(f">> Auto selected k = {k} (clusters) based on minimum WSS variance of {0.05*100}%")


    # Build clusters
    assert k in range(min_clusters, 31), "k is not within the specified range"

    solID = df['solID'].copy()

    # Perform KMeans clustering on the numeric data only
    # try:
    #     cls = KMeans(n_clusters=k, max_iter=limit_clusters, random_state=seed).fit(features)
    # except Exception as e:
    #     print(f"Error during KMeans fitting: {e}")
    #     cls = None

    # If you need to use the cluster labels with the original DataFrame, you can add them back
    if cls is not None:
        # Add the cluster labels to the original DataFrame or to solID as needed
        #df['cluster'] = cls.labels_
        # Or if you want to create a new DataFrame with solID and the cluster labels
        df_with_clusters = pd.DataFrame({'solID': solID, 'cluster': cls.labels_})

    columns_to_ignore = set(ignore + ['cluster'])
    all_columns = set(df.columns)
    all_paid = all_columns - columns_to_ignore
    all_paid = list(all_paid)
    # Select top models by minimum (weighted) distance to zero
    # all_paid = setdiff(names(input.df), [ignore, 'cluster'])
    ts_validation = all(np.isnan(df['nrmse_test']))
    top_sols = clusters_df(df=df, all_paid=all_paid, balance=weights, limit=limit, ts_validation=ts_validation)
    top_sols = top_sols.loc[:, ~top_sols.columns.duplicated()]
    # df, optimal_k, wss = clusterKmeans_auto(df, limit_clusters=limit_clusters, seed=seed)

    # Build in-cluster CI with bootstrap
    ci_list = confidence_calcs(input["xDecompAgg"], df, all_paid, dep_var_type, k, cluster_by, seed=seed)

    output = {
        # 'data': pd.DataFrame({'top_sol': df['solID'].isin(top_sols['solID']), 'cluster': pd.Series(np.arange(k), dtype=int)}),
        'data': df.assign(
            top_sol=df['solID'].isin(top_sols['solID']),
            cluster=pd.Series(np.arange(len(df)), dtype=int)
        ),
        'df_cluster_ci': ci_list['df_ci'],
        'n_clusters': k,
        'boot_n': ci_list['boot_n'],
        'sim_n': ci_list['sim_n'],
        'errors_weights': weights,
        # 'wss': input.nclusters_plot + theme_lares(background='white'),
        'wss': plot_wss_and_save(wss, f'{path}pareto_clusters_wss.png'),
        # corrs is not being used anywhere and there is no 1:1 mapping from R to Python
        'corrs': None,
        'clusters_means': cls.cluster_centers_,
        'clusters_PCA': df_pca,
        'clusters_tSNE': df_tsne,
        'models': top_sols,
        'plot_clusters_ci': plot_clusters_ci(ci_list['sim_collect'], ci_list['df_ci'], dep_var_type, ci_list['boot_n'], ci_list['sim_n']),
        # TODO ADD following vars to the output
        #'plot_models_errors': plot_topsols_errors(df, top_sols, limit, weights),
        #'plot_models_rois': plot_topsols_rois(df, top_sols, all_media, limit)
    }

    # TODO Add below code once plotting is fixed
    # if export:
    #     output['data'].to_csv(f'{path}pareto_clusters.csv', index=False)
    #     output['df_cluster_ci'].to_csv(f'{path}pareto_clusters_ci.csv', index=False)
    #     plt.figure(figsize=(5, 4))
    #     sns.heatmap(output['wss'], annot=True, cmap='coolwarm', xticks=range(k), yticks=range(k), square=True)
    #     plt.savefig(f'{path}pareto_clusters_wss.png', dpi=500, bbox_inches='tight')
    #     get_height = int(np.ceil(k/2)/2)
    #     db = (output['plot_clusters_ci'] / (output['plot_models_rois'] + output['plot_models_errors'])) ## TODO: + plot_layout(heights=[get_height, 1], guides='collect')
    #     suppress_messages(plt.savefig(f'{path}pareto_clusters_detail.png', dpi=500, bbox_inches='tight', width=12, height=4+len(all_paid)*2, limitsize=False))

    return output

def confidence_calcs(xDecompAgg, df, all_paid, dep_var_type, k, cluster_by, boot_n=1000, sim_n=10000, **kwargs):
    """
    This function takes in a bunch of inputs and does some statistical calculations.

    Parameters:
    - xDecompAgg: DataFrame, the input data for statistical calculations
    - cls: object, the cluster object containing cluster information
    - all_paid: list, the list of paid values
    - dep_var_type: str, the type of dependent variable ('conversion' or 'roi_total')
    - k: int, the number of clusters
    - cluster_by: str, the method of clustering ('hyperparameters' or other)
    - boot_n: int, the number of bootstrap iterations (default: 1000)
    - sim_n: int, the number of simulations (default: 10000)
    - **kwargs: additional keyword arguments

    Returns:
    - df_ci: DataFrame, the confidence interval results
    - sim_collect: DataFrame, the simulation results
    - boot_n: int, the number of bootstrap iterations
    - sim_n: int, the number of simulations
    """

    """
    This function takes in a bunch of inputs and does some statistical calculations
    """
    # filter out rows with missing values for total_spend
    filtered_df = xDecompAgg[~xDecompAgg['total_spend'].isna()]
    # join with cluster information
    merged_df = filtered_df.merge(df[['solID', 'cluster']], on='solID', how='left')
    # select relevant columns and group by cluster, solID and rn
    grouped_df_with_n = merged_df[['solID', 'cluster', 'rn', 'roi_total', 'cpa_total', 'robynPareto']].groupby(['cluster', 'rn', 'solID']).size().reset_index(name='n')
    grouped_df_with_solID = merged_df[['solID', 'cluster', 'rn', 'roi_total', 'cpa_total', 'robynPareto']].groupby(['cluster', 'rn', 'solID']).agg({'roi_total': 'mean', 'cpa_total': 'mean', 'robynPareto': 'mean'})
    grouped_df = pd.merge(grouped_df_with_n, grouped_df_with_solID, on=['cluster', 'rn', 'solID'])
    # sort by cluster and rn
    df_clusters_outcome = grouped_df.sort_values(['cluster', 'rn', 'solID'])

    # Initialize lists to store results
    cluster_collect = []
    chn_collect = []
    sim_collect = []

    for j in range(k):
        # Filter outcome data for current cluster
        df_outcome = df_clusters_outcome[df_clusters_outcome['cluster'] == j]

        if len(df_outcome['solID'].unique()) < 3:
            print(f"Cluster {j} does not contain enough models to calculate CI")
        else:

            from .checks import HYPS_NAMES

            # Bootstrap CI
            if cluster_by == 'hyperparameters':
                pattern = '|'.join(["_" + re.escape(hyp_name) for hyp_name in HYPS_NAMES])
                all_paid = np.unique([re.sub(pattern, '', paid) for paid in all_paid])
            for i in all_paid:
                if dep_var_type == 'conversion':
                    # Correctly apply filtering for 'conversion' case
                    df_chn = df_outcome[(df_outcome['rn'] == i) & np.isfinite(df_outcome['cpa_total'])]
                    v_samp = df_chn['cpa_total']
                else:
                    df_chn = df_outcome[df_outcome['rn'] == i]
                    v_samp = df_chn['roi_total']

                boot_mean = np.mean(v_samp)
                boot_se = np.std(v_samp, ddof=1) / np.sqrt(len(v_samp))

                ci_low, ci_up = stats.norm.interval(0.95, loc=boot_mean, scale=boot_se)
                ci_low = max(0, ci_low)

                df_chn_modified = df_chn.assign(ci_low=ci_low, ci_up=ci_up, n=len(v_samp),
                                boot_se=boot_se, boot_mean=boot_mean, cluster=j)
                chn_collect.append(df_chn_modified)

                # Correcting the simulation part
                x_sim = np.random.normal(boot_mean, boot_se, size=sim_n)
                y_sim = norm.pdf(x_sim, boot_mean, boot_se)  # Correct way to simulate 'y_sim' as in R's dnorm

                # Creating and appending the new DataFrame to sim_collect
                sim_df = pd.DataFrame({
                    'cluster': j,
                    'rn': i,
                    'n': len(v_samp),
                    'boot_mean': boot_mean,
                    'x_sim': x_sim,
                    'y_sim': y_sim
                })
                sim_collect.append(sim_df)

            cluster_collect.append({
                f'chn_{j}': chn_collect,
                f'sim_{j}': sim_collect
            })

    all_sim_collect_dfs = []
    all_chn_collect_dfs = []
    for cluster in cluster_collect:
        for key in cluster:
            if key.startswith('sim_'):
                all_sim_collect_dfs.extend(cluster[key])
            if key.startswith('chn_'):
                all_chn_collect_dfs.extend(cluster[key])
    sim_collect = pd.concat(all_sim_collect_dfs, ignore_index=True)
    chn_collect = pd.concat(all_chn_collect_dfs, ignore_index=True)

    sim_collect['cluster_title'] = sim_collect.apply(lambda row: f"Cl.{row['cluster']} (n={row['n']})", axis=1)

    df_ci = chn_collect.drop_duplicates()

    df_ci['cluster_title'] = df_ci.apply(lambda row: f"Cl.{row['cluster']} (n={row['n']})", axis=1)

    # If df_ci needs grouping and summarization similar to what was described previously:
    df_ci_grouped = df_ci.groupby(['rn', 'cluster', 'cluster_title']).agg(
        n=('n', 'first'),
        boot_mean=('boot_mean', 'mean'),
        boot_se=('boot_se', 'mean'),
        ci_low=('ci_low', 'min'),
        ci_up=('ci_up', 'max')
    ).reset_index()

    df_ci_grouped['boot_ci'] = df_ci_grouped.apply(lambda x: f"[{round(x['ci_low'], 2)}, {round(x['ci_up'], 2)}]", axis=1)
    df_ci_grouped['sd'] = df_ci_grouped['boot_se'] * np.sqrt(df_ci_grouped['n'] - 1)
    df_ci_grouped['dist100'] = (df_ci_grouped['ci_up'] - df_ci_grouped['ci_low'] + 2 * df_ci_grouped['boot_se'] * np.sqrt(df_ci_grouped['n'] - 1)) / 99

    # df_ci_grouped now holds the processed data
    df_ci = df_ci_grouped

    return {
        'df_ci': df_ci,
        'sim_collect': sim_collect,
        'boot_n': boot_n,
        'sim_n': sim_n
    }

def errors_scores(df, balance=None, ts_validation=True, **kwargs):
    """
    Calculate the error scores for a given dataframe.

    Parameters:
    - df: DataFrame - The input dataframe containing the error columns.
    - balance: list or None - The balance values for weighting the error scores. If None, no balancing is applied.
    - ts_validation: bool - Flag indicating whether to use the 'nrmse_test' column for error calculation. If False, use 'nrmse_train' column.
    - **kwargs: Additional keyword arguments.

    Returns:
    - scores: DataFrame - The calculated error scores.

    Raises:
    - AssertionError: If the length of balance is not 3.
    - AssertionError: If any of the error columns are not found in the dataframe.
    """
    # Check length of balance
    if balance is not None:
        assert len(balance) == 3, "Balance must have length 3"

    # Check that error columns are in df
    error_cols = ['nrmse_test' if ts_validation else 'nrmse_train', 'decomp.rssd', 'mape']
    assert all(col in df.columns for col in error_cols), f"Error columns {error_cols} not found in df"

    # Normalize balance values
    if balance is not None:
        balance = balance / sum(balance)

    # Select and rename error columns
    scores = df[error_cols].rename(columns={error_cols[0]: 'nrmse'})

    # Replace infinite values with maximum finite value
    scores = scores.apply(lambda row: np.nan_to_num(row, posinf=max(row.dropna().values)))

    # Force normalized values
    scores = scores.apply(lambda row: row / row.max())

    # Replace missing values with 0
    scores = scores.fillna(0)

    # Balance errors
    if balance is not None:
        scores = scores.apply(lambda row: balance * row, axis=1)

    # Calculate error score
    scores = scores.apply(lambda row: np.sqrt(row['nrmse']**2 + row['decomp.rssd']**2 + row['mape']**2), axis=1)

    # scores = scores.apply(lambda row: np.sqrt(row['nrmse']**2 + row['decomp.rssd']**2 + row['mape']**2))

    return scores


def prepare_df(x, all_media, dep_var_type, cluster_by):
    # Initial checks and setup
    if cluster_by == "performance":
        # Check options (assuming check_opts is appropriately defined and used)
        check_opts(all_media, x['rn'].unique())

        # Preparing outcome DataFrame based on dep_var_type
        if dep_var_type == "revenue":
            # Create dummy variables for 'rn', merge with 'roi_total', ensuring 'solID' is included
            dummies = pd.get_dummies(x['rn'])
            outcome = pd.concat([x[['solID', 'roi_total']], dummies], axis=1)
        elif dep_var_type == "conversion":
            # Filter by 'cpa_total' being finite, then proceed similar to 'revenue' case
            filtered = x[pd.to_numeric(x['cpa_total'], errors='coerce').notnull()]
            dummies = pd.get_dummies(filtered['rn'])
            outcome = pd.concat([filtered[['solID', 'cpa_total']], dummies], axis=1)

        # Ensure all_media columns are included by dynamically checking their presence
        outcome = outcome[[col for col in ['solID'] + all_media + list(dummies.columns) if col in outcome.columns]]

        # Merge with error metrics
        errors = x[['solID', 'nrmse', 'nrmse_test', 'nrmse_train', 'decomp.rssd', 'mape']].drop_duplicates()
        outcome = pd.merge(outcome, errors, on='solID', how='left')

    elif cluster_by == "hyperparameters":
        # Include only 'solID', hyperparameters, and specific metrics
        from .checks import HYPS_NAMES

        cols_to_keep = ['solID'] + [col for col in x.columns if any(hyps in col for hyps in HYPS_NAMES + ['nrmse', 'decomp.rssd', 'mape'])]
        outcome = x[cols_to_keep].copy()
    else:
        raise ValueError("Invalid cluster_by parameter")

    return outcome

# def prepare_df(x, all_media, dep_var_type, cluster_by):
#     """
#     Prepare the dataframe for clustering analysis based on the given parameters.

#     Parameters:
#     x (DataFrame): The input dataframe.
#     all_media (list): List of all media options.
#     dep_var_type (str): Type of dependent variable ("revenue" or "conversion").
#     cluster_by (str): Type of clustering ("performance" or "hyperparameters").

#     Returns:
#     DataFrame: The prepared dataframe for clustering analysis.
#     """
#     if cluster_by == "performance":
#         # Check options
#         check_opts(all_media, unique(x['rn']))

#         # Select columns and spread ROI total
#         if dep_var_type == "revenue":
#             outcome = x[['solID', 'rn', 'roi_total']].copy()
#             outcome = pd.get_dummies(outcome, columns=['rn'])
#             outcome = outcome.drop(columns=['rn'])
#             outcome = outcome.select(columns=['solID'] + all_media)
#         elif dep_var_type == "conversion":
#             outcome = x[['solID', 'rn', 'cpa_total']].copy()
#             outcome = outcome[outcome['cpa_total'].isfinite()]
#             outcome = pd.get_dummies(outcome, columns=['rn'])
#             outcome = outcome.drop(columns=['rn'])
#             outcome = outcome.select(columns=['solID'] + all_media)

#         # Remove missing values
#         errors = x.dropna()

#         # Join errors with outcome
#         outcome = pd.merge(outcome, errors, on='solID')
#         outcome = outcome.drop(columns=['nrmse', 'nrmse_test', 'nrmse_train', 'decomp.rssd', 'mape'])
#     else:
#         if cluster_by == "hyperparameters":

#             from .checks import HYPS_NAMES

#             cols_to_keep = ['solID'] + [col for col in x.columns if any(hyp in col for hyp in HYPS_NAMES)]
#             outcome = x[cols_to_keep].copy()

#         else:
#             raise ValueError("Invalid cluster_by parameter")

#     return outcome

def min_max_norm(x, min=0, max=1):
    """
    Performs min-max normalization on the input array.

    Parameters:
    - x: Input array to be normalized.
    - min: Minimum value of the normalized range (default: 0).
    - max: Maximum value of the normalized range (default: 1).

    Returns:
    - Normalized array.

    """
    x = x[np.isfinite(x)]
    if len(x) == 1:
        return x
    a = np.min(x, axis=0)
    b = np.max(x, axis=0)
    return (max - min) * (x - a) / (b - a) + min

##def clusters_df(df, all_paid, balance=rep(1, 3), limit=1, ts_validation=True, **kwargs):
def clusters_df(df, all_paid, balance=None, limit=1, ts_validation=True, **kwargs):
    """
    Calculate the error scores for each cluster in the given dataframe and return the top clusters based on the error scores.

    Parameters:
    - df: pandas DataFrame
        The input dataframe containing the data.
    - all_paid: bool
        A boolean value indicating whether all payments have been made.
    - balance: numpy array, optional
        An array containing the balance values for each cluster. If not provided, it will be set to [1, 1, 1].
    - limit: int, optional
        The maximum number of clusters to return. Defaults to 1.
    - ts_validation: bool, optional
        A boolean value indicating whether to perform time series validation. Defaults to True.
    - **kwargs: keyword arguments
        Additional arguments to be passed to the errors_scores function.

    Returns:
    - pandas DataFrame
        A dataframe containing the cluster, rank, and error score for the top clusters.
    """
    if balance is None:
        balance = np.repeat(1, 3)

    df['error_score'] = errors_scores(df, balance, ts_validation=ts_validation, **kwargs)
    df.fillna(0, inplace=True)
    df = df.groupby('cluster').apply(lambda x: x.sort_values('error_score').head(limit)).reset_index(drop=True)
    df['rank'] = df.groupby('cluster')['error_score'].rank(ascending=False)
    return df[['cluster', 'rank'] + list(df.columns[:-2])]
    # df = df.replace(np.nan, 0)
    # df['error_score'] = errors_scores(df, balance, ts_validation=ts_validation, **kwargs)
    # df = df.groupby('cluster').agg({'error_score': 'mean'})
    # df = df.sort_values('error_score', ascending=False)
    # df = df.head(limit)
    # df['rank'] = df.groupby('cluster').cumcount() + 1
    # return df[['cluster', 'rank', 'error_score']]

def plot_clusters_ci(sim_collect, df_ci, dep_var_type, boot_n, sim_n):
    """
    Plots the clusters with confidence intervals.

    Parameters:
    sim_collect (DataFrame): The simulated data.
    df_ci (DataFrame): The data frame containing the confidence intervals.
    dep_var_type (str): The type of dependent variable ("conversion" or "ROAS").
    boot_n (int): The number of bootstrap results.
    sim_n (int): The number of simulations.

    Returns:
    None
    """
    # Convert dep_var_type to CPA or ROAS
    temp = "CPA" if dep_var_type == "conversion" else "ROAS"

    # Filter complete cases in df_ci
    df_ci = df_ci.dropna()

    # Create a ggplot object TODO: use plotnine to make it work
    ## p = plt.ggplot(sim_collect, aes(x='x_sim', y='rn')) \
    ##    + plt.facet_wrap(~df_ci['cluster_title']) \
    ##    + plt.geom_density_ridges_gradient(scale=3, rel_min_height=0.01, size=0.1) \
    ##    + plt.geom_text(data=df_ci, aes(x='boot_mean', y='rn', label='boot_ci'),
    ##                    position=plt.PositionNudge(x=-0.02, y=0.1),
    ##                    colour='grey30', size=3.5) \
    ##    + plt.geom_vline(xintercept=1, linetype='dashed', size=0.5, colour='grey75')

    # Add a title, subtitle, and labels
    ## TODO: use plotnine to make it work
    ##p += plt.labs(title=f"In-Cluster {temp} & bootstrapped 95% CI",
    ##               subtitle="Sampling distribution of cluster mean",
    ##               x=temp, y="Density", fill=temp,
    ##               caption=f"Based on {boot_n} bootstrap results with {sim_n} simulations")

    # Add a horizontal line for ROAS
    ##if temp == "ROAS":
    ##    p += plt.geom_hline(yintercept=1, alpha=0.5, colour='grey50', linetype='dashed')

    # Set theme
    ##p += plt.theme_lares(background='white', legend='none')

    ##return p

# TODO fix plotting
def plot_topsols_errors(df, top_sols, limit=1, balance=None):
    """
    Plots a heatmap of the correlation matrix for the joined dataframe of `df` and `top_sols`.

    Parameters:
        df (pandas.DataFrame): The main dataframe.
        top_sols (pandas.DataFrame): The dataframe containing top solutions.
        limit (int, optional): The number of top performing models to select. Defaults to 1.
        balance (numpy.ndarray, optional): The weights for balancing the heatmap. Defaults to None.

    Returns:
        None
    """
    # Calculate balance
    if balance is None:
        balance = np.array([1, 1, 1])
    else:
        balance = balance / np.sum(balance)

    temp_df = df.copy()
    temp_df.drop(['cluster', 'error_score'], axis=1, inplace=True, errors='ignore')
    # Join dataframes
    #joined_df = pd.merge(temp_df, top_sols, on='solID', how='left')
    joined_df = pd.merge(temp_df, top_sols, on='solID', how='left', suffixes=('', '_top'))
    joined_df = joined_df[[col for col in joined_df.columns if not col.endswith('_top')]]

    # Calculate alpha and label
    joined_df['alpha'] = np.where(np.isnan(joined_df['cluster']), 0.6, 1)
    #joined_df['label'] = np.where(np.isnan(joined_df['cluster']), np.nan, f"[{joined_df['cluster']}.{joined_df['rank']}]")

    #correlation_df = joined_df.copy()
    #correlation_df = correlation_df.drop(['cluster', 'rank'], axis=1)

    # Plot
    plt.figure(figsize=(10, 6))
    sns.set(style='white')
    sns.heatmap(joined_df.corr(), annot=True, cmap='coolwarm', square=True)

    # Customize x-axis and y-axis ticks
    plt.xticks(ticks=range(len(joined_df.columns)), labels=joined_df.columns)
    plt.yticks(ticks=range(len(joined_df.columns)), labels=joined_df.columns)
    #sns.heatmap(joined_df.corr(), annot=True, cmap='coolwarm',
                  #square=True, xticks=range(3), yticks=range(3),
                  #xlabel='Feature 1', ylabel='Feature 2')
    plt.title(f"Selecting Top {limit} Performing Models by Cluster")
    #plt.subtitle("Based on minimum (weighted) distance to origin")
    plt.xlabel("NRMSE")
    plt.ylabel("DECOMP.RSSD")
    #plt.caption(f"Weights: NRMSE {round(100*balance[0])}%, DECOMP.RSSD {round(100*balance[1])}%, MAPE {round(100*balance[2])}%")
    plt.show()

def plot_topsols_rois(df, top_sols, all_media, limit=1):
    """
    Plot the top performing models' mean metric per media for the given data.

    Parameters:
    df (DataFrame): The dataframe containing the real ROIs.
    top_sols (DataFrame): The dataframe containing the top solutions.
    all_media (list): The list of all media.
    limit (int, optional): The number of top solutions to consider. Defaults to 1.
    """
    # Create a dataframe with the real ROIs
    real_rois = df.drop(columns=['mape', 'nrmse', 'decomp.rssd'])
    real_rois.columns = ['real_' + col for col in real_rois.columns]

    # Join the real ROIs with the top solutions
    top_sols = pd.merge(top_sols, real_rois, left_on='solID', right_on='real_solID', how='left')


    # Create a label column
    top_sols['label'] = np.vectorize(lambda x: f"[{x.cluster}.{x.rank}] {x.solID}")(top_sols)

    # Gather the media and ROI data
    top_sols = pd.melt(top_sols, id_vars=['solID', 'label'], value_vars=['media', 'roi'])

    # Filter out non-real media
    top_sols = top_sols[top_sols['media'].str.startswith('real_')]

    # Remove the 'real_' prefix from the media column
    top_sols['media'] = top_sols['media'].str.replace('real_', '')

    # Plot the data TODO: ggplot?
    ##plt.figure(figsize=(10, 6))
    ##sns.barplot(x=reorder(top_sols['media'], top_sols['roi']), y=top_sols['roi'], data=top_sols)
    ##plt.facet_grid(top_sols['label'] ~ ., scale='free', space='free')
    ##plt.geom_col(color='blue', size=10)
    ##plt.coord_flip()
    ##plt.labs(title='Top Performing Models', x=None, y='Mean metric per media')
    ##plt.theme_lares(background='white')
    ##plt.show()

def bootci(samp, boot_n, seed=1, **kwargs):
    """
    Compute the bootstrap confidence interval for a given sample.

    Parameters:
    samp (array-like): The sample data.
    boot_n (int): The number of bootstrap samples to generate.
    seed (int): The seed for random number generation. Default is 1.
    **kwargs: Additional keyword arguments to be passed to np.mean() and np.std().

    Returns:
    list: A list containing the bootstrap means, confidence interval, and standard error of the mean.
    """
    # Set seed for reproducibility
    np.random.seed(seed)

    # Handle case where samp has only one element
    if len(samp) == 1:
        return [samp, [np.nan, np.nan], np.nan]

    # Compute sample mean and standard deviation
    samp_mean = np.mean(samp, **kwargs)
    samp_std = np.std(samp, **kwargs)

    # Generate bootstrap samples
    boot_samples = np.random.choice(samp, size=(boot_n, len(samp)), replace=True)

    # Compute means of bootstrap samples
    boot_means = np.mean(boot_samples, axis=0)

    # Compute standard error of the mean
    se = np.std(boot_means, axis=0)

    # Compute 95% confidence interval
    me = norm.ppf(0.975, len(samp) - 1) * samp_std
    ci = [samp_mean - me, samp_mean + me]

    # Plot bootstrap distribution (optional)
    # plt.hist(boot_means, bins=30, alpha=0.5, label='Bootstrap')
    # plt.density(boot_means, color='red', label='Density')
    # plt.xlabel('Value')
    # plt.ylabel('Density')
    # plt.title('Bootstrap Distribution')
    # plt.show()

    return [boot_means, ci, se]

================
File: convergence.py
================
# Copyright (c) Meta Platforms, Inc. and its affiliates.

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

####################################################################
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from scipy.stats import norm
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns
import matplotlib.pyplot as plt
from plotnine import ggplot, aes, geom_point, scale_colour_gradient, labs, theme, guides

def robyn_converge(OutputModels, n_cuts=20, sd_qtref=3, med_lowb=2, nrmse_win=(0, 0.998), **kwargs):
    assert n_cuts > min(sd_qtref, med_lowb) + 1, "n_cuts must be greater than min(sd_qtref, med_lowb) + 1"

    df_list = [trial['resultCollect']['resultHypParam'] for trial in OutputModels["trials"]]
    df = pd.concat(df_list, ignore_index=True)
    calibrated = df['mape'].sum() > 0

    # Calculate deciles
    df_melt = pd.melt(df, id_vars=['trial', 'ElapsedAccum'], value_vars=["nrmse", "decomp.rssd", "mape"], var_name="error_type", value_name="value")
    df_melt['error_type'] = df_melt['error_type'].str.upper()
    df_melt = df_melt[(df_melt['value'] > 0) & np.isfinite(df_melt['value'])]

    df_melt.sort_values(by=['trial', 'ElapsedAccum'], inplace=True)
    df_melt.reset_index(drop=True, inplace=True)
    df_melt['iter'] = df_melt.groupby(['error_type', 'trial']).cumcount() + 1
    max_iter = df_melt['iter'].max()
    cuts_labels = range(1, n_cuts + 1)
    df_melt['cuts'] = pd.cut(df_melt['iter'], bins=np.linspace(0, max_iter, n_cuts + 1), labels=cuts_labels, include_lowest=True, ordered=False)



    # Assuming 'ElapsedAccum' and 'trial' columns exist in df for sorting and grouping
    df_melt.sort_values(by=['trial', 'ElapsedAccum'], inplace=True)
    df_melt['iter'] = df_melt.groupby(['error_type', 'trial']).cumcount() + 1
    max_iter = df_melt['iter'].max()
    cuts_labels = range(1, n_cuts + 1)
    df_melt['cuts'] = pd.cut(df_melt['iter'], bins=np.linspace(0, max_iter, n_cuts+1), labels=cuts_labels, include_lowest=True, ordered=True)

    # print(df_melt)
    grouped = df_melt.groupby(['error_type', 'cuts'])
    errors = grouped.agg(
        n=('value', 'size'),
        median=('value', 'median'),
        std=('value', 'std')
    ).reset_index()
    errors['med_var_P'] = errors.groupby('error_type')['median'].transform(lambda x: abs(round(100 * (x - x.shift()) / x, 2)))

    errors['first_med'] = errors.groupby('error_type')['median'].transform('first').abs()
    errors['first_med_avg'] = errors.groupby('error_type').apply(lambda x: abs(x['median'].iloc[:sd_qtref].mean())).reset_index(level=0, drop=True)
    errors['last_med'] = errors.groupby('error_type')['median'].transform('last').abs()
    errors['first_sd'] = errors.groupby('error_type')['std'].transform('first')
    errors['first_sd_avg'] = errors.groupby('error_type')['std'].transform(lambda x: x.iloc[:sd_qtref].mean())

    errors['last_sd'] = errors.groupby('error_type')['std'].transform('last')
    errors['med_thres'] = abs(errors['first_med'] - med_lowb * errors['first_sd_avg'])
    errors['flag_med'] = errors['median'].abs() < errors['med_thres']
    errors['flag_sd'] = errors['std'] < errors['first_sd_avg']


    conv_msg = []
    unique_error_types = errors['error_type'].unique()

    for obj_fun in unique_error_types:
        temp_df = errors[errors['error_type'] == obj_fun].copy()
        temp_df['median'] = temp_df['median'].round(decimals=2)
        last_row = temp_df.iloc[-1]
        greater = ">"  # Equivalent to the R's intToUtf8(8814)

        # Constructing the message
        did_converge = "" if (last_row['flag_sd'] & last_row['flag_med']) else "NOT "
        sd = round(last_row['last_sd'], 2)
        symb_sd = "<=" if last_row['flag_sd'] else greater
        sd_thresh = round(last_row['first_sd_avg'], 2)
        quantile = 'n_cuts'  # Assuming n_cuts is defined somewhere in your context
        qtn_median = round(last_row['last_med'], 2)
        symb_med = "<=" if last_row['flag_med'] else greater
        med_thresh = round(last_row['med_thres'], 2)

        message = f"{last_row['error_type']} {did_converge}converged: sd@qt.{quantile} {sd} {symb_sd} {sd_thresh} & |med@qt.{quantile}| {qtn_median} {symb_med} {med_thresh}"
        conv_msg.append(message)

    for msg in conv_msg:
        print("-", msg)

    max_trial = df['trial'].max()
    trials_word = "trials" if max_trial > 1 else "trial"
    iterations_word = "each" if max_trial > 1 else ""
    nevergrad_algo = OutputModels['metadata']['nevergrad_algo']  # Assuming this is a dictionary and 'nevergrad_algo' is a key
    subtitle = f"{max_trial} {trials_word} with {df_melt['cuts'].max()} iterations {iterations_word} using {nevergrad_algo}"

    sns.set_theme(style="whitegrid")
    g = sns.FacetGrid(df_melt, row='error_type', hue='error_type', aspect=15, height=0.5, palette='GnBu')
    g.map(sns.kdeplot, 'value', bw_adjust=0.5, clip_on=False, fill=True, alpha=1, linewidth=1.5).add_legend()
    g.map(sns.kdeplot, 'value', clip_on=False, color="w", lw=2, bw_adjust=0.5)
    g.map(plt.axhline, y=0, lw=2, clip_on=False)
    g.set_titles("")
    g.set(yticks=[], ylabel="")
    g.despine(bottom=True, left=True)
    for ax, title in zip(g.axes.flat, df_melt['error_type'].unique()):
        ax.set_title(title)
    plt.subplots_adjust(top=0.9)
    g.fig.suptitle("Objective convergence by iterations quantiles", fontsize=16)
    g.fig.subplots_adjust(top=.92)
    g.fig.suptitle(subtitle)

    moo_cloud_plot = (ggplot(df, aes(x='nrmse', y='decomp.rssd', colour='ElapsedAccum'))
                  + scale_colour_gradient(low="skyblue", high="#000080")
                  + labs(title="Multi-objective evolutionary performance" + (" with calibration" if calibrated else ""),
                         subtitle=subtitle,
                         x="NRMSE" + (" [Winsorized]" if max(df['nrmse']) == 1 else ""),
                         y="DECOMP.RSSD",
                         colour="Time [s]",
                         caption='\n'.join(conv_msg))  # Assuming conv_msg is a list of strings
                  + theme(figure_size=(10, 6)))

    if calibrated:
        moo_cloud_plot += (geom_point(aes(size='mape', alpha=1 - df['mape'])))
        moo_cloud_plot += guides(alpha=False, size=False)  # Correctly apply guides at the plot level
    else:
        moo_cloud_plot += geom_point()

    cvg_out = {
        'moo_distrb_plot': g,
        'moo_cloud_plot': moo_cloud_plot,
        'errors': errors,
        'conv_msg': conv_msg
    }

    cvg_out['sd_qtref'] = sd_qtref
    cvg_out['med_lowb'] = med_lowb

    return cvg_out


import matplotlib.pyplot as plt

# test coverage ...
import numpy as np
import pandas as pd
from scipy.optimize import minimize
from scipy.stats import gamma


def gamma_mle(params, x):
    gamma_shape, gamma_scale = params
    return -np.sum(gamma.logpdf(x, shape=gamma_shape, scale=gamma_scale))


def f_geo(a, r, n):
    for i in range(2, n):
        a[i] = a[i - 1] * r
    return a


def nloptr(x0, eval_f, lb, x, opts):
    return minimize(eval_f, x0, method="SLSQP", bounds=lb, x=x, **opts)


def test_cvg():
    # Experiment with gamma distribution fitting

    # Initialize parameters
    gamma_shape = 5
    gamma_scale = 0.7

    # Generate sequence for fitting
    seq_nrmse = f_geo(5, 0.7, 100)

    # Create data frame with true values
    df_nrmse = pd.DataFrame({"x": range(1, 101), "y": seq_nrmse, "type": "true"})

    # Fit gamma distribution using NLOPT
    mod_gamma = nloptr(
        x0=[gamma_shape, gamma_scale],
        eval_f=gamma_mle,
        lb=[0, 0],
        x=seq_nrmse,
        opts={"algorithm": "SLSQP", "maxeval": 1e5},
    )

    # Extract fitted parameters
    gamma_params = mod_gamma.x

    # Generate predicted values
    seq_nrmse_gam = 1 / gamma.pdf(
        seq_nrmse, shape=gamma_params[0], scale=gamma_params[1]
    )
    seq_nrmse_gam = seq_nrmse_gam / (max(seq_nrmse_gam) - min(seq_nrmse_gam))
    seq_nrmse_gam = max(seq_nrmse) * seq_nrmse_gam

    # Create data frame with predicted values
    df_nrmse_gam = pd.DataFrame(
        {"x": range(1, 101), "y": seq_nrmse_gam, "type": "pred"}
    )

    # Combine data frames
    df_nrmse = pd.concat([df_nrmse, df_nrmse_gam], ignore_index=True)

    # Plot true and predicted values
    plt.plot(df_nrmse["x"], df_nrmse["y"], color="blue", label="True")
    plt.plot(df_nrmse["x"], df_nrmse_gam["y"], color="red", label="Predicted")
    plt.legend()
    plt.show()

    return df_nrmse

================
File: data.py
================
# Copyright (c) Meta Platforms, Inc. and its affiliates.

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

####################################################################
import os

import pandas as pd

### Manually Generated


def dt_simulated_weekly():
    csv_file = os.getcwd() + "/../data/dt_simulated_weekly.csv"
    return pd.read_csv(csv_file)


def dt_prophet_holidays():
    csv_file = os.getcwd() + "/../data/dt_prophet_holidays.csv"
    return pd.read_csv(csv_file)

# def dt_simulated_weekly():
#     csv_file = os.getcwd() + "/python/src/data/dt_simulated_weekly.csv"
#     dir_path = os.path.dirname(os.path.realpath(__file__))
#     csv_file = os.path.join(dir_path, '..', 'data', 'dt_simulated_weekly.csv')
#     return pd.read_csv(csv_file)

# def dt_prophet_holidays():
#     csv_file = os.getcwd() + "/python/src/data/dt_prophet_holidays.csv"
#     dir_path = os.path.dirname(os.path.realpath(__file__))
#     csv_file = os.path.join(dir_path, '..', 'data', 'dt_prophet_holidays.csv')
#     return pd.read_csv(csv_file)

================
File: exports.py
================
# Copyright (c) Meta Platforms, Inc. and its affiliates.

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

####################################################################
import warnings
import pandas as pd
import numpy as np
# from the second method
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import os



def robyn_save(input_collect, output_collect, robyn_object=None, select_model=None, dir=None, quiet=False):
    warnings.warn("Function robyn_save() is not supported anymore. Please migrate to robyn_write() and robyn_read()")
    check_robyn_name(robyn_object, quiet)
    if select_model is None:
        select_model = output_collect['selectID']
    if not select_model in output_collect['allSolutions']:
        raise ValueError(f"Input 'select_model' must be one of these values: {', '.join(output_collect['allSolutions'])}")

    # Export as JSON file
    json = robyn_write(input_collect, output_collect, select_model, quiet=quiet)

    # Summarize results
    summary = filter(output_collect['xDecompAgg'], solID==select_model) \
        .select(variable=['rn', 'coef', 'decomp', 'total_spend', 'mean_non0_spend']) \
        .rename(columns={'variable': 'hyperparameter'}) \
        .drop(columns=['solID'])

    # Nice and tidy table format for hyper-parameters
    hyps = filter(output_collect['resultHypParam'], solID==select_model) \
        .select(contains(HYPS_NAMES)) \
        .gather() \
        .separate(channel=['channel', 'none'], sep=r'^.*_', remove=FALSE) \
        .mutate(hyperparameter=lambda x: re.sub(r'^.*_', '', x.channel)) \
        .select(channel, hyperparameter, value) \
        .spread(key='hyperparameter', value='value')

    # Collect other outputs
    values = output_collect.drop(columns=['allSolutions', 'hyper_fixed', 'plot_folder'])
    values['robyn_object'] = robyn_object
    values['select_model'] = select_model
    values['summary'] = summary
    values['errors'] = json['ExportedModel']['errors']
    values['hyper_df'] = hyps
    values['hyper_updated'] = output_collect['hyper_updated']
    values['window'] = [input_collect['window_start'], input_collect['window_end']]
    values['periods'] = input_collect['rollingWindowLength']
    values['interval'] = input_collect['intervalType']
    values['adstock'] = input_collect['adstock']
    values['plot'] = robyn_onepagers(input_collect, output_collect, select_model, quiet=quiet, export=FALSE)

    # Append other outputs to the list
    output = [values]

    # Rename columns
    if input_collect['dep_var_type'] == 'conversion':
        colnames(output[0]['summary']) = re.sub(r'roi_', 'cpa_', colnames(output[0]['summary']))

    # Set class
    class_(output) <- c('robyn_save', class(output))

    # Overwrite existing file if necessary
    if robyn_object is not None:
        if file.exists(robyn_object):
            if not quiet:
                answer = askYesNo(f'{robyn_object} already exists. Are you certain to overwrite it?')
            else:
                answer = True
            if answer is False or is.na(answer):
                message(f'Stopped export to avoid overwriting {robyn_object}')
                return output
            else:
                saveRDS(output, file=robyn_object)
                if not quiet: message(f'Exported results: {robyn_object}')
        else:
            saveRDS(output, file=robyn_object)
            if not quiet: message(f'Exported results: {robyn_object}')

    return output



def robyn_save(x, *args):
    print("Exported file: {x['robyn_object']}")
    print("Exported model: {x['select_model']}")
    print("Window: {x['window'][1]} to {x['window'][2]} ({x['periods']} {x['interval']}s)")

    errors = pd.DataFrame({'errors': [f"R2 ({x['ExportedModel']['ts_validation']}): {x['errors']['rsq_train']}, {x['errors']['rsq_test']}"
                             "| NRMSE = {x['errors']['nrmse']}"
                             "| DECOMP.RSSD = {x['errors']['decomp.rssd']}"
                             "| MAPE = {x['errors']['mape']}"
                            ]})
    print(errors)

    print("Summary Values on Selected Model:")
    print(x['summary'].mutate(decomp=lambda x: 100*x['decomp']).replace(np.inf, 0).replace(np.nan, "-").astype({'decomp': float}))

    print("Hyper-parameters:")
    print(pd.DataFrame({'Adstock': x['adstock']}))
    print(x['hyper_df'])



def plot_robyn_save(x, *args, **kwargs):
    plt.plot(x['plot'][0], *args, **kwargs)


def robyn_load(robyn_object, select_build=None, quiet=False):
    """
    Load a Robyn model from a saved RDS file.
    """
    if isinstance(robyn_object, str) or isinstance(robyn_object, list):
        # If the input is a string or a list, we assume it's a file path or a list of Robyn objects
        Robyn = read_rds(robyn_object)
        object_path = os.path.dirname(robyn_object)
    else:
        # If the input is not a string or a list, we assume it's a Robyn object
        Robyn = robyn_object
        object_path = None

    select_build_all = range(len(Robyn))
    if select_build is None:
        select_build = max(select_build_all)
        if not quiet:
            print(f"Loaded Model: {select_build}")

    if select_build not in select_build_all or len(select_build) != 1:
        raise ValueError(f"Input 'select_build' must be one value of {select_build_all}")

    list_name = "listInit" if select_build == 0 else f"listRefresh{select_build}"
    InputCollect = Robyn[list_name]["InputCollect"]
    OutputCollect = Robyn[list_name]["OutputCollect"]
    select_model = OutputCollect["selectID"]

    output = {
        "Robyn": Robyn,
        "InputCollect": InputCollect,
        "OutputCollect": OutputCollect,
        "select_model": select_model,
        "objectPath": object_path,
        "robyn_object": robyn_object
    }
    return output

================
File: inputs.py
================
# Copyright (c) Meta Platforms, Inc. and its affiliates.

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

####################################################################
import numpy as np
import pandas as pd
import datetime as dt
from prophet import Prophet
from sklearn.preprocessing import StandardScaler

import matplotlib
## Prevents plot windows showing up.
matplotlib.use('qtagg')

## from sklearn.model_selection import AIC, BIC ## Manual, there are no AIC or BIC functions in sklearn
## from sklearn.metrics import r2_score

from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import scale

## Added lmfit to use nlsLM in Python.
## Manually added curve_fit for nlsLM fitting
## lmfit for nonlinear least squares fitting
from lmfit import create_params, fit_report, minimize
## Uses from scipy.optimize import curve_fit
## from scipy.stats import linregress
from lmfit.models import LinearModel

## Manually added ggplot
from plotnine import ggplot, aes, labs, geom_point, geom_line, theme_gray, scale_x_continuous, scale_y_continuous

## Manually added imports
##check_nas, check_varnames, check_datevar, check_depvar, check_prophet, check_context, check_paidmedia, check_organicvars, check_factorvars
from .checks import *



def robyn_inputs(
    dt_input=None,
    dep_var=None,
    dep_var_type=None,
    date_var="auto",
    paid_media_spends=None,
    paid_media_vars=None,
    paid_media_signs=None,
    organic_vars=None,
    organic_signs=None,
    context_vars=None,
    context_signs=None,
    factor_vars=None,
    dt_holidays=None,
    prophet_vars=None,
    prophet_signs=None,
    prophet_country=None,
    adstock=None,
    hyperparameters=None,
    window_start=None,
    window_end=None,
    calibration_input=None,
    json_file=None,
    InputCollect=None,
    **kwargs,
):
    """
    robyn_inputs function in Python.

    This function is used to collect and validate the input parameters for the Robyn model.

    Parameters:
    - dt_input: The input data as a pandas DataFrame.
    - dep_var: The dependent variable name.
    - dep_var_type: The type of the dependent variable (e.g., "continuous", "binary", "count").
    - date_var: The name of the date variable in the input data. Default is "auto".
    - paid_media_spends: The names of the paid media spend variables.
    - paid_media_vars: The names of the paid media variables.
    - paid_media_signs: The signs (+/-) for the paid media variables.
    - organic_vars: The names of the organic media variables.
    - organic_signs: The signs (+/-) for the organic media variables.
    - context_vars: The names of the context variables.
    - context_signs: The signs (+/-) for the context variables.
    - factor_vars: The names of the factor variables.
    - dt_holidays: The holidays data as a pandas DataFrame.
    - prophet_vars: The names of the variables to be used in the Prophet model.
    - prophet_signs: The signs (+/-) for the Prophet variables.
    - prophet_country: The country for the Prophet model.
    - adstock: The adstock parameters.
    - hyperparameters: The hyperparameters for the model.
    - window_start: The start date of the modeling window.
    - window_end: The end date of the modeling window.
    - calibration_input: The calibration input data as a pandas DataFrame.
    - json_file: The path to a JSON file containing the input parameters.
    - InputCollect: The collected input parameters.

    Returns:
    - InputCollect: The collected and validated input parameters.

    """
    # Use case 3: running robyn_inputs() with json_file
    if json_file is not None:
        json = robyn_read(json_file, step=1, **kwargs)
        if dt_input is None or dt_holidays is None:
            raise ValueError("Provide 'dt_input' and 'dt_holidays'")
        for i in range(len(json["InputCollect"])):
            assign(json["InputCollect"][i], json["InputCollect"][i])

    # Use case 1: running robyn_inputs() for the first time
    if InputCollect is None:
        ## dt_input = pd.as_tibble(dt_input), Manual, commenting out to see if tibble required
        if dt_holidays is not None:
            dt_holidays = dt_holidays  ## pd.as_tibble(dt_holidays) Manual, commenting out to see if tibble required
            # mutate(ds = as.Date(.data$ds, origin = "1970-01-01"))
            dt_holidays["ds"] = pd.to_datetime(dt_holidays["ds"])  ## Manually added
        else:
            dt_holidays = None

        # Check for NA values
        check_nas(dt_input)
        check_nas(dt_holidays)

        # Check vars names (duplicates and valid)
        check_varnames(
            dt_input,
            dt_holidays,
            dep_var,
            date_var,
            context_vars,
            paid_media_spends,
            organic_vars,
        )

        # Check date input (and set dayInterval and intervalType)
        date_input = check_datevar(dt_input, date_var)
        print(date_input)
        dt_input = date_input["dt_input"]  # sorted date by ascending
        date_var = date_input["date_var"]  # when date_var = "auto"
        dayInterval = date_input["dayInterval"]
        intervalType = date_input["intervalType"]

        # Check dependent var
        check_depvar(dt_input, dep_var, dep_var_type)

        # Check prophet
        if dt_holidays is None or prophet_vars is None:
            dt_holidays = prophet_vars = prophet_country = prophet_signs = None

        prophet_signs = check_prophet(
            dt_holidays, prophet_country, prophet_vars, prophet_signs, dayInterval
        )

        # Check baseline variables (and maybe transform context_signs)
        ## R lists has names, therefore, there is no need to get the "context_signs" field from context, can directly use the context_signs.
        ## context = check_context(dt_input, context_vars, context_signs)
        ## context_signs = context["context_signs"]
        context_signs = check_context(dt_input, context_vars, context_signs)

        # Check paid media variables (set mediaVarCount and maybe transform paid_media_signs)
        if paid_media_vars is None:
            paid_media_vars = paid_media_spends

        paidmedia = check_paidmedia(
            dt_input, paid_media_vars, paid_media_signs, paid_media_spends
        )
        paid_media_signs = paidmedia["paid_media_signs"]
        mediaVarCount = paidmedia["mediaVarCount"]
        ## exposure_vars = paid_media_vars - paid_media_spends, manually changed diff of lists didn't work.
        exposure_vars = np.setdiff1d(
            paid_media_vars, paid_media_spends
        )  ## Manually added

        # Check organic media variables (and maybe transform organic_signs)
        organic = check_organicvars(dt_input, organic_vars, organic_signs)
        organic_signs = organic["organic_signs"]

        # Check factor_vars
        factor_vars = check_factorvars(dt_input, factor_vars, context_vars, organic_vars)

        # Check all vars
        ## Manually changed to +
        ## all_media = [paid_media_spends, organic_vars]
        all_media = paid_media_spends + organic_vars
        ## Manually added following line to return prophet vars to lowercase
        prophet_vars = [(v.lower()) for v in prophet_vars]  ## Manually added
        ## all_ind_vars = [tolower(prophet_vars), context_vars, all_media]
        ## all_ind_vars = [prophet_vars, context_vars, all_media], this creates multip dimensional data, need list
        all_ind_vars = prophet_vars + context_vars + all_media
        check_allvars(all_ind_vars)

        # Check data dimension
        check_datadim(dt_input, all_ind_vars, rel=10)

        # Check window_start & window_end (and transform parameters/data)
        windows = check_windows(dt_input, date_var, all_media, window_start, window_end)
        dt_input = windows["dt_input"]
        window_start = windows["window_start"]
        rollingWindowStartWhich = windows["rollingWindowStartWhich"]
        refreshAddedStart = windows["refreshAddedStart"]
        window_end = windows["window_end"]
        rollingWindowEndWhich = windows["rollingWindowEndWhich"]
        rollingWindowLength = windows["rollingWindowLength"]

        # Check adstock
        adstock = check_adstock(adstock)

        # Check calibration and iters/trials
        calibration_input = check_calibration(
            dt_input,
            date_var,
            calibration_input,
            dayInterval,
            dep_var,
            window_start,
            window_end,
            paid_media_spends,
            organic_vars,
        )

        # Not used variables
        ## Manual corrections
        ## unused_vars = [var for var in colnames(dt_input) if var not in [dep_var, date_var, context_vars, paid_media_vars, paid_media_spends, organic_vars]]
        all_vars = [dep_var, date_var]
        all_vars += context_vars + paid_media_vars + paid_media_spends + organic_vars
        unused_vars = list()
        for var in dt_input.columns.values:
            if var not in all_vars:
                unused_vars.append(var)

        # Check for no-variance columns on raw data (after removing not-used)
        ## Manual correction needed, there is no use of select in python, or -all_of
        ## Meaning select the df except the unused_vars
        ## check_novar(select(dt_input, -all_of(unused_vars)))
        check_novar_columns = dt_input.columns.difference(unused_vars)

        check_novar(dt_input[check_novar_columns])

        # Calculate total media spend used to model
        ## Manual: there is no select method for Dataframe, and reverse order selection is not
        ## paid_media_total = dt_input[rollingWindowEndWhich:rollingWindowLength].select(paid_media_vars).sum()
        paid_media_df = dt_input.loc[
            (rollingWindowLength - 1):(rollingWindowEndWhich - 1), paid_media_vars
        ]
        paid_media_total = sum(
            paid_media_df.select_dtypes(include="number").sum(axis=0)
        )

        # Collect input
        InputCollect = {
            "dt_input": dt_input,
            "dt_holidays": dt_holidays,
            "dt_mod": None,
            "dt_modRollWind": None,
            "xDecompAggPrev": None,
            "date_var": date_var,
            "dayInterval": dayInterval,  ## Manually commented out since it is not NULL in the origiinal code. None,
            "intervalType": intervalType,  ## Manually commented out since it is not NULL in the origiinal code. None,
            "dep_var": dep_var,
            "dep_var_type": dep_var_type,
            "prophet_vars": prophet_vars,  ## Manually commented out since already done above. .lower(),
            "prophet_signs": prophet_signs,
            "prophet_country": prophet_country,
            "context_vars": context_vars,
            "context_signs": context_signs,
            "paid_media_vars": paid_media_vars,
            "paid_media_signs": paid_media_signs,
            "paid_media_spends": paid_media_spends,
            "paid_media_total": paid_media_total,  ## Manually commented out , it was None, since code inferred piece by piece it wasn't there
            "mediaVarCount": mediaVarCount,  ## Manually commented out , it was None, since code inferred piece by piece it wasn't there,
            "exposure_vars": exposure_vars,  ## Manually commented out , it was None, since code inferred piece by piece it wasn't there,,
            "organic_vars": organic_vars,
            "organic_signs": organic_signs,
            "all_media": all_media,  ## Manually commented out , it was None, since code inferred piece by piece it wasn't there,,
            "all_ind_vars": all_ind_vars,  ## Manually commented out , it was None, since code inferred piece by piece it wasn't there,,
            "factor_vars": factor_vars,
            "unused_vars": unused_vars,  ## Manually commented out , it was None, since code inferred piece by piece it wasn't there,,
            "window_start": window_start,
            "rollingWindowStartWhich": rollingWindowStartWhich,
            "window_end": window_end,
            "rollingWindowEndWhich": rollingWindowEndWhich,
            "rollingWindowLength": rollingWindowLength,
            "totalObservations": dt_input.shape[
                0
            ],  ## Manually commented out totalObservations, it is the count of rows in dt_input
            "refreshAddedStart": refreshAddedStart,
            "adstock": adstock,
            "hyperparameters": hyperparameters,
            "calibration_input": calibration_input,
            "custom_params": dict(),  ## custom_params, it is an empty list in original code
        }

        # Check hyperparameters
        ## Manual Fix ...
        if hyperparameters is not None:
            # Running robyn_inputs() for the 1st time & 'hyperparameters' provided --> run robyn_engineering()
            hyperparameters = check_hyperparameters(
                hyperparameters, adstock, paid_media_spends, organic_vars, exposure_vars
            )
            InputCollect = robyn_engineering(InputCollect, ...)

    else:
        # Check for legacy (deprecated) inputs
        check_legacy_input(InputCollect)
        # Check calibration data
        ## Manually corrected access to dict
        calibration_input = check_calibration(
            dt_input=InputCollect['dt_input'],
            date_var=InputCollect['date_var'],
            calibration_input=calibration_input,
            dayInterval=InputCollect['dayInterval'],
            dep_var=InputCollect['dep_var'],
            window_start=InputCollect['window_start'],
            window_end=InputCollect['window_end'],
            paid_media_spends=InputCollect['paid_media_spends'],
            organic_vars=InputCollect['organic_vars'],
        )

        # Update calibration_input
        if not calibration_input is None:
            InputCollect['calibration_input'] = calibration_input

        # Update hyperparameters
        if not hyperparameters is None:
            InputCollect['hyperparameters'] = hyperparameters

        # Check for hyperparameters
        ## if not InputCollect['hyperparameters'] and not hyperparameters:
        if not 'hyperparameters' in InputCollect.keys() and hyperparameters is None:
            raise ValueError("Must provide hyperparameters in robyn_inputs()")
        else:
        # Conditional output 2.1
        ## if InputCollect['hyperparameters']:
            if 'hyperparameters' in InputCollect.keys():
                # Update & check hyperparameters
                ## if not InputCollect['hyperparameters']
                if InputCollect['hyperparameters'] is None:
                    InputCollect['hyperparameters'] = hyperparameters
                InputCollect['hyperparameters'] = check_hyperparameters(
                    InputCollect['hyperparameters'],
                    InputCollect['adstock'],
                    InputCollect['all_media'],
                )

            # Run robyn_engineering()
            ## InputCollect = robyn_engineering(InputCollect, *args, **kwargs)
            InputCollect = robyn_engineering(InputCollect)

        # Check for no-variance columns (after filtering modeling window)
        select_columns = list(set([col for col in InputCollect['dt_mod'].columns.values if col not in InputCollect['unused_vars']]))
        temp_df = InputCollect['dt_mod'][select_columns]
        dt_mod_model_window = temp_df.loc[(temp_df['ds'] >= InputCollect['window_start']) & (temp_df['ds'] <= InputCollect['window_end'])]
        ##dt_mod_model_window = InputCollect['dt_mod'].select(
        ##    -InputCollect.unused_vars
        ##).filter(ds >= InputCollect.window_start, ds <= InputCollect.window_end)

        check_novar(dt_mod_model_window, InputCollect)

    # Handle JSON file input
    if json_file is not None:
        pending = [x for x in json_file["InputCollect"] if x not in InputCollect]
        InputCollect.extend(pending)

    # Save versions,
    ## Manual: Too R type of operation to save data in R studio.
    ## Manual: Versions etc.
    ## Manual: ver = pd.Series(utils.packageVersion('Robyn')).astype(str)
    ## Manual: rver = utils.sessionInfo()['R.version']
    ## Manual origin = 'dev' if utils.packageDescription('Robyn')['Repository'] is None else 'stable'
    ## InputCollect['version'] = f'Robyn ({origin}) v{ver} [R-{rver.major}.{rver.minor}]'

    # Set class
    ## Manual: Commenting out since return data should present robyn_inputs to InputCollect
    ## InputCollect['class'] = ['robyn_inputs', InputCollect['class']]
    ## return InputCollect
    return {"robyn_inputs": InputCollect}


def print_robyn_inputs(
    x,
    *,
    mod_vars=None,
    range=None,
    windows=None,
    custom_params=None,
    prophet=None,
    unused=None,
    hyps=None,
):
    """
    Print Robyn inputs.

    Args:
        x: The input data.
        mod_vars: The modified variables.
        range: The date range.
        windows: The model window.
        custom_params: The custom parameters.
        prophet: The Prophet configuration.
        unused: The unused variables.
        hyps: The hyper-parameter ranges.

    Returns:
        None
    """
    # Set mod_vars
    if mod_vars is None:
        mod_vars = ", ".join(setdiff(x.dt_mod.columns, ["ds", "dep_var"]))

    # Set range
    if range is None:
        range = pd.Timestamp(x.dt_input.iloc[0, 0]).strftime("%Y-%m-%d:%H:%M:%S")

    # Set windows
    if windows is None:
        windows = f"{x.window_start}:{x.window_end}"

    # Set custom_params
    if custom_params is None:
        custom_params = ""
    else:
        custom_params = ", ".join(x["custom_params"])

    # Set prophet
    if prophet is None:
        prophet = "Deactivated"
    else:
        prophet = f"{prophet} on {x.prophet_country}"

    # Set unused
    if unused is None:
        unused = "None"
    else:
        unused = ", ".join(x.unused_vars)

    # Set hyps
    if hyps is None:
        hyps = "Hyper-parameters: Not set yet"
    else:
        hyps = "Hyper-parameters ranges:\n" + "\n".join(x.hyperparameters)

    # Print Robyn inputs
    print(
        f"Total Observations: {x.nrow(x.dt_input)} ({x.intervalType}s)\n"
        f"Input Table Columns ({x.ncol(x.dt_input)}):\n"
        f"  Date: {x.date_var}\n"
        f"  Dependent: {x.dep_var} [{x.dep_var_type}]\n"
        f"  Paid Media: {', '.join(x.paid_media_vars)}\n"
        f"  Paid Media Spend: {', '.join(x.paid_media_spends)}\n"
        f"  Context: {', '.join(x.context_vars)}\n"
        f"  Organic: {', '.join(x.organic_vars)}\n"
        f"  Prophet (Auto-generated): {prophet}\n"
        f"  Unused variables: {unused}\n"
        f"  Date Range: {range}\n"
        f"  Model Window: {windows} ({x.rollingWindowEndWhich - x.rollingWindowStartWhich + 1} {x.intervalType}s)\n"
        f"  With Calibration: {x.calibration_input is not None}\n"
        f"  Custom parameters: {custom_params}\n"
        f"  Adstock: {x.adstock}\n"
        f"{hyps}"
    )


def robyn_engineering(x, quiet=False):
    """
    Performs feature engineering for the Robyn model.

    Args:
        x (dict): The input dictionary containing various data frames and variables.
        quiet (bool, optional): If True, suppresses the printing of recommendations and warnings. Defaults to False.

    Returns:
        tuple: A tuple containing the following:
            - mod_nls_collect (pd.DataFrame): A data frame containing the collected NLS models.
            - plot_nls_collect (pd.DataFrame): A data frame containing the collected NLS plots.
            - yhat_collect (pd.DataFrame): A data frame containing the collected yhat values.
    """
    print(">> Running Robyn feature engineering...")
    # InputCollect
    input_collect = x

    # check_InputCollect
    check_input_collect(input_collect)

    # dt_input
    ## dt_input = select(input_collect, -any_of(input_collect.unused_vars))
    used_columns = [var for var in input_collect['dt_input'].columns if var not in input_collect['unused_vars']]
    dt_input = input_collect['dt_input'][used_columns]

    # paid_media_vars
    paid_media_vars = input_collect['paid_media_vars']

    # paid_media_spends
    ##paid_media_spends = input_collect.paid_media_spends
    paid_media_spends = input_collect['paid_media_spends']

    # factor_vars
    ##factor_vars = input_collect.factor_vars
    factor_vars = input_collect['factor_vars']

    # rollingWindowStartWhich
    ##rolling_window_start_which = input_collect.rollingWindowStartWhich
    rolling_window_start_which = input_collect['rollingWindowStartWhich']

    # rollingWindowEndWhich
    ##rolling_window_end_which = input_collect.rollingWindowEndWhich
    rolling_window_end_which = input_collect['rollingWindowEndWhich']

    # dt_inputRollWind
    ##dt_input_roll_wind = dt_input[rolling_window_start_which:rolling_window_end_which,]
    dt_input_roll_wind = dt_input.loc[(rolling_window_start_which-1):(rolling_window_end_which-1),]

    # dt_transform
    dt_transform = dt_input
    ## colnames(dt_transform)[colnames(dt_transform) == input_collect.date_var] = "ds"
    ## colnames(dt_transform)[colnames(dt_transform) == input_collect.dep_var] = "dep_var"
    dt_transform = dt_transform.rename(columns={input_collect['date_var']:'ds', input_collect['dep_var']:'dep_var'})
    dt_transform = dt_transform.sort_values(by=['ds'])  ## manually changed
    # arrange(dt_transform, .data.ds) ## Manual what is this? arrange method used for sorting the data frame by date in R

    # dt_transformRollWind
    dt_transform_roll_wind = dt_transform.iloc[(rolling_window_start_which-1):(rolling_window_end_which-1),]

    # exposure_selector
    ## exposure_selector = paid_media_spends != paid_media_vars
    ## names(exposure_selector) = paid_media_vars ## Manual: names sets the column names of a data frame
    exposure_selector = list()
    for i, val in enumerate(paid_media_spends):
        ## exposure_selector[paid_media_vars[i]] = (val != paid_media_vars[i])
        exposure_selector.append(val != paid_media_vars[i])

    # modNLSCollect, plotNLSCollect, yhatCollect
    ## Manually added if case
    if any(exposure_selector):
        mod_nls_collect = []
        plot_nls_collect = []
        yhat_collect = []

        # mediaCostFactor
        # media_cost_factor = col_sums(dt_input_roll_wind[paid_media_spends], na.rm=True) / col_sums(dt_input_roll_wind[paid_media_vars], na.rm=True)
        ## media_cost_factor = col_sums(dt_input_roll_wind[paid_media_spends], True) / col_sums(dt_input_roll_wind[paid_media_vars], True)
        media_cost_factor = list()
        for i in range(len(paid_media_spends)):
            media_cost_factor.append(np.divide(np.sum(dt_input_roll_wind[paid_media_spends[i]]), np.sum(dt_input_roll_wind[paid_media_vars[i]])))

        # for loop
        ## for i in range(input_collect.mediaVarCount):
        for i in range(input_collect['mediaVarCount']):
            if exposure_selector[i]:
                # Run models (NLS and/or LM)
                ## dt_spend_mod_input = subset(dt_input_roll_wind, select=c(paid_media_spends[i], paid_media_vars[i]))
                dt_spend_mod_input = dt_input_roll_wind[[paid_media_spends[i], paid_media_vars[i]]]
                results = fit_spend_exposure(dt_spend_mod_input, media_cost_factor[i], paid_media_vars[i])

                # Compare NLS & LM, takes LM if NLS fits worse
                ##mod = results.res
                mod = results['res']
                ## Manual:
                exposure_selector[i] = False if mod['rsq_nls'] is None else mod['rsq_nls'][0] > mod['rsq_lm'][0]

                # Data to create plot
                dt_plot_nls = pd.DataFrame(
                    {
                        'channel': paid_media_vars[i],
                        'yhat_nls': results['yhatNLS'] if exposure_selector[i] else results['yhatLM'],  ## Manual : Wrong translation, manual correction
                        'yhat_lm': results['yhatLM'],
                        'y': results['data']['exposure'],
                        'x': results['data']['spend'],
                    }
                )

                caption = f"nls: AIC = {mod['aic_nls'].values[0]} | R2 = {mod['rsq_nls'].values[0]}\nlm: AIC = {mod['aic_lm'].values[0]} | R2 = {mod['rsq_lm'].values[0]}"

                ## dt_plot_nls = dt_plot_nls.pivot_longer(
                ##    cols=c("yhat_nls", "yhat_lm"), names_to="models", values_to="yhat")
                ## dt_plot_nls = pd.DataFrame({'yhat' = results['yhatNLS'] + results['yhatLM']})
                ## pivot_longer corresponding method is melt https://pandas.pydata.org/docs/reference/api/pandas.melt.html
                dt_plot_nls = pd.melt(dt_plot_nls, id_vars=['channel', 'y', 'x'], value_vars=['yhat_nls', 'yhat_lm'], var_name='models', value_name='yhat', ignore_index=False)
                ## dt_plot_nls["models"] = dt_plot_nls["models"].str.remove(
                ##    tolower(dt_plot_nls["models"]), "yhat"
                ##)
                ## + theme_lares(background="white", legend="top")
                ## ggplot in Python https://realpython.com/ggplot-python/
                models_plot = (
                    ggplot(dt_plot_nls)
                    + aes(x="x", y="y", color="models")
                    + geom_point()
                    + geom_line(aes(y="yhat", x="x", color="models"))
                    + labs(
                        title="Exposure-Spend Models Fit Comparison",
                        x=f"Spend [{paid_media_spends[i]}]",
                        y=f"Exposure [{paid_media_vars[i]}]",
                        caption=caption,
                        color="Model",
                        )
                        + theme_gray()
                        + scale_x_continuous()
                        + scale_y_continuous()
                )

                # Save results into modNLSCollect, plotNLSCollect, yhatCollect
                mod_nls_collect.append(mod)
                plot_nls_collect.append(models_plot)
                yhat_collect.append(dt_plot_nls)

        # bind_rows
        mod_nls_collect = pd.concat(mod_nls_collect)
        #plot_nls_collect = pd.concat(plot_nls_collect)
        yhat_collect = pd.concat(yhat_collect)
        repeat_factor = len(yhat_collect) // len(dt_transform_roll_wind)
        yhat_collect['ds'] = dt_transform_roll_wind['ds'].repeat(repeat_factor).reset_index(drop=True)

    else: ## Manually added else case wasn't translated, possibly due to large function.
        mod_nls_collect = None
        plot_nls_collect = None
        yhat_collect = None

    # Give recommendations and show warnings
    ## if not quiet:
    if mod_nls_collect is not None and not quiet:
        threshold = 0.8
        ## Manually added these
        these = None
        final_print = None ## False
        ## metrics = ["R2 (nls)", "R2 (lm)"]
        ## names = ["rsq_nls", "rsq_lm"]
        metrics = ["rsq_nls", "rsq_lm"]
        for m in range(len(metrics)):
            metric_name = metrics[m]
            temp = np.where(mod_nls_collect[metric_name] < threshold)[0]
            if len(temp) > 0:
                final_print = True
                ## these = x.iloc[temp, 0]
                these = x['channel'][temp]

        ## Manually corrected
        if final_print == True:
            print(f"NOTE: potential improvement on splitting channels for better exposure fitting.")
            print(f"Threshold (Minimum R2) = {threshold}")
            print(f"Check: InputCollect$modNLS$plots outputs")
            print(f"Weak relationship for: {v2t(these)} and their spend")

    # Clean & aggregate data
    #factor_vars = ["var1", "var2", "var3"]  # Replace with actual factor variables
    #if len(factor_vars) > 0:
    #    x = pd.get_dummies(x, drop_first=True, columns=factor_vars)
    x_df = pd.DataFrame(x['dt_input'])
    for col in factor_vars:
        if col in x_df.columns:
            x_df[col] = x_df[col].astype('category')


    # Initialize empty lists to store custom parameters and prophet arguments
    custom_params = dict()
    prophet_args = list()

    # Extract prophet variables and custom parameters from InputCollect
    prophet_vars = input_collect["prophet_vars"]
    custom_params = input_collect.get("custom_params", dict())

    # Remove empty strings and ellipsis from custom parameters
    ## custom_params = [param for name, param in custom_params.items() if param != "" and param != "..."]
    temp = dict()
    for param, val in custom_params.items():
        if not (param != "" and param != "..."):
            temp[param] = val

    custom_params = temp

    # Compute prophet arguments
    robyn_run_args = {
        "InputCollect", "dt_hyper_fixed", "json_file", "ts_validation",
        "add_penalty_factor", "refresh", "seed", "quiet", "cores", "trials",
        "iterations", "rssd_zero_penalty", "objective_weights",
        "nevergrad_algo", "intercept", "intercept_sign", "lambda_control", "outputs"
    }
    robyn_outputs_args = {
        "input_collect", "output_models", "pareto_fronts",
        "calibration_constraint", "plot_folder", "plot_folder_sub",
        "plot_pareto", "csv_out", "clusters", "select_model",
        "ui", "export", "all_sol_json", "quiet", "refresh"
    }
    robyn_inputs_args = {
        "dt_input", "dep_var", "dep_var_type", "date_var",
        "paid_media_spends", "paid_media_vars", "paid_media_signs",
        "organic_vars", "organic_signs", "context_vars", "context_signs",
        "factor_vars", "dt_holidays", "prophet_vars", "prophet_signs",
        "prophet_country", "adstock", "hyperparameters", "window_start",
        "window_end", "calibration_input", "json_file", "InputCollect"
    }
    robyn_refresh_args = {
        "json_file", "robyn_object", "dt_input", "dt_holidays",
        "refresh_steps", "refresh_mode", "refresh_iters", "refresh_trials",
        "plot_folder", "plot_pareto", "version_prompt", "export",
        "calibration_input", "objective_weights"
    }

    combined_args = (
        robyn_run_args |
        robyn_outputs_args |
        robyn_inputs_args |
        robyn_refresh_args
    )

    exclude_set = {"", "..."}

    robyn_args = combined_args - exclude_set

    # Compute custom prophet arguments
    prophet_custom_args = set(custom_params.keys()) - set(robyn_args)

    # Print message with custom prophet parameters
    if len(prophet_custom_args) > 0:
        print(f"Using custom prophet parameters: {', '.join(prophet_custom_args)}")

    # Decompose data using prophet
    dt_transform = prophet_decomp(
        dt_transform=dt_transform,
        dt_holidays=input_collect["dt_holidays"],
        prophet_country=input_collect["prophet_country"],
        prophet_vars=prophet_vars,
        prophet_signs=input_collect["prophet_signs"],
        factor_vars=input_collect["factor_vars"],
        context_vars=input_collect["context_vars"],
        organic_vars=input_collect["organic_vars"],
        paid_media_spends=input_collect["paid_media_spends"],
        intervalType=input_collect["intervalType"],
        dayInterval=input_collect["dayInterval"],
        custom_params=custom_params,
    )

    # Finalize enriched input
    ## dt_transform = dt_transform.subset(select=["ds", "dep_var", input_collect["all_ind_vars"]])
    subset = input_collect["all_ind_vars"]
    subset.append("ds")
    subset.append("dep_var")
    subset = list(set(subset))
    dt_transform = dt_transform[subset]

    input_collect["dt_mod"] = dt_transform
    input_collect["dt_modRollWind"] = dt_transform.iloc[(rolling_window_start_which-1):(rolling_window_end_which),]
    input_collect["dt_inputRollWind"] = dt_input_roll_wind
    input_collect["modNLS"] = {  ## Manual: added dict.
        "results": mod_nls_collect,
        "yhat": yhat_collect,
        "plots": plot_nls_collect
    }

    return input_collect


def prophet_decomp(
    dt_transform,
    dt_holidays,
    prophet_country,
    prophet_vars,
    prophet_signs,
    factor_vars,
    context_vars,
    organic_vars,
    paid_media_spends,
    intervalType,
    dayInterval,
    custom_params = dict(),
):
    """
    Decomposes a time series using the Prophet algorithm.

    Args:
        dt_transform (pandas.DataFrame): The input time series data.
        dt_holidays (pandas.DataFrame): The holiday data.
        prophet_country (str): The country for which the holidays are defined.
        prophet_vars (list): The variables to include in the Prophet model.
        prophet_signs (list): The signs of the variables in the Prophet model.
        factor_vars (list): The factor variables to include in the model.
        context_vars (list): The context variables to include in the model.
        organic_vars (list): The organic variables to include in the model.
        paid_media_spends (list): The paid media spends variables to include in the model.
        intervalType (str): The type of interval for the time series data.
        dayInterval (int): The interval between each data point.
        custom_params (dict, optional): Custom parameters for the Prophet model. Defaults to an empty dictionary.

    Returns:
        pandas.DataFrame: The transformed dataframe with decomposed time series.
    """
    # Check prophet
    check_prophet(
        dt_holidays, prophet_country, prophet_vars, prophet_signs, dayInterval
    )

    # Recurrence
    ## recurrence = dt_transform.select(["y" = "dep_var"]).rename(columns={"y": "dep_var"}) ## how to do select ???
    recurrence = dt_transform[["ds", "dep_var"]]
    recurrence.rename(columns={"dep_var": "y"}, inplace=True)

    # Holidays
    holidays = set_holidays(dt_transform, dt_holidays, intervalType)

    # Use trend, holiday, season, monthly, weekday, and weekly seasonality
    use_trend = True if "trend" in prophet_vars else False
    use_holiday = True if "holiday" in prophet_vars else False
    use_season = True if "season" in prophet_vars or "yearly.seasonality" in prophet_vars else False
    use_monthly = True if "monthly" in prophet_vars else False
    ## use_weekday = "weekday" in prophet_vars or "weekly.seasonality" in prophet_vars
    use_weekday = True if "weekday" in prophet_vars or "weekly.seasonality" in prophet_vars else False

    # Bind columns
    ## dt_regressors = pd.concat([recurrence,
    ##        pd.select(
    ##            dt_transform, all_of(c(paid_media_spends, context_vars, organic_vars))
    ##        ),
    ##    ],
    ##    axis=1,
    ##)
    dt_regressors = pd.concat([recurrence, dt_transform[paid_media_spends + context_vars + organic_vars]], axis=1)

    # Mutate date column
    dt_regressors["ds"] = pd.to_datetime(dt_regressors["ds"])

    # Prophet parameters
    prophet_params = {
        "holidays": holidays[holidays["country"] == prophet_country],
        "yearly_seasonality": custom_params["yearly_seasonality"] if "yearly_seasonality" in custom_params.keys() else use_season,
        "weekly_seasonality": custom_params["weekly_seasonality"] if "weekly_seasonality" in custom_params.keys() else use_weekday,
        "daily_seasonality": False, ## No hourly model allowed
    }

    custom_params["yearly_seasonality"] = None
    custom_params["weekly_seasonality"] = None
    # Append custom parameters
    ## prophet_params.update(custom_params)

    # Create prophet model
    ## https://facebook.github.io/prophet/docs/quick_start.html#python-api
    modelRecurrence = Prophet(**prophet_params)

    # Add seasonality
    if use_monthly:
        modelRecurrence.add_seasonality(name="monthly", period=30.5, fourier_order=5)

    ## Manually added this section
    if factor_vars is not None and len(factor_vars) > 0:
        dt_ohse_temp = dt_regressors[factor_vars]
        dt_ohse = pd.get_dummies(dt_ohse_temp, columns=factor_vars, drop_first=False, dtype=int)

        temp_names = [col for col in dt_ohse.columns if not col.endswith('_na')]
        dt_ohse = dt_ohse[temp_names]
        ohe_names = list(dt_ohse.columns)

        for addreg in ohe_names:
            modelRecurrence.add_regressor(addreg)

        temp_names = [col for col in dt_regressors.columns if col not in factor_vars]
        dt_ohe = pd.concat([dt_regressors[temp_names], dt_ohse], axis=1)

        mod_ohe = modelRecurrence.fit(dt_ohe)
        dt_forecastRegressor = mod_ohe.predict(dt_ohe)
        select_columns = [column for column in dt_forecastRegressor.columns.values if not "_lower" in column or not "_upper" in column]
        forecastRecurrence = dt_forecastRegressor[select_columns]

        def custom_scale(array):
            scaler = StandardScaler()
            non_zero_mask = array != 0
            #scaled_non_zeros = scaler.fit_transform(array[non_zero_mask].reshape(-1, 1)).flatten()
            scaled_non_zeros = scaler.fit_transform(array[non_zero_mask].to_numpy().reshape(-1, 1)).flatten()

            scaled_array = np.copy(array)
            scaled_array[non_zero_mask] = scaled_non_zeros
            return scaled_array

        for aggreg in factor_vars:
            ohRegNames = [var for var in forecastRecurrence.columns.values if var.startswith(aggreg)]
            get_reg = forecastRecurrence[ohRegNames].sum(axis=1)
            dt_transform[aggreg] = custom_scale(get_reg)
    else:
        if dayInterval == 1:
            warnings.warn("Currently, there's a known issue with prophet that may crash this use case.\n Read more here: https://github.com/facebookexperimental/Robyn/issues/472")

        # Fit model
        mod = modelRecurrence.fit(dt_regressors)

        # Forecast
        forecastRecurrence = mod.predict(dt_regressors)


    # Add trend, season, monthly, and weekday features
    if use_trend:
        dt_transform["trend"] = forecastRecurrence['trend']
    if use_season:
        dt_transform["season"] = forecastRecurrence['yearly']
    if use_monthly:
        dt_transform["monthly"] = forecastRecurrence["monthly"]
    if use_weekday:
        dt_transform["weekday"] = forecastRecurrence["weekly"]
    if use_holiday:
        dt_transform["holiday"] = forecastRecurrence["holidays"]

    # Return transformed dataframe
    return dt_transform


def fit_spend_exposure(dt_spendModInput, mediaCostFactor, paid_media_var):
    """
    Fits the spend-exposure model using two different approaches: Michaelis-Menten model and linear model comparison.

    Parameters:
    - dt_spendModInput (DataFrame): Input data with two columns: 'spend' and 'exposure'.
    - mediaCostFactor (float): Factor to adjust the media cost.
    - paid_media_var (str): Name of the paid media variable.

    Returns:
    - output (dict): Dictionary containing the model results and other relevant information.
    """
    # Check if the input data has the correct shape
    if dt_spendModInput.shape[1] != 2:
        raise ValueError("Pass only 2 columns")

    # Set the column names
    dt_spendModInput = dt_spendModInput.rename(columns = {dt_spendModInput.columns.values[0] : "spend", dt_spendModInput.columns.values[1]: "exposure"})

    # Model 1: Michaelis-Menten model Vmax * spend/(Km + spend)
    try:
        # Initialize the model parameters
        nlsStartVal = { ## Manually converted to dict
            "Vmax" : max(dt_spendModInput["exposure"]),
            "Km" : max(dt_spendModInput["exposure"]) / 2,
        }

        # Fit the model using non-linear least squares
        ## https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html
        ## https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html
        ## https://www.rdocumentation.org/packages/minpack.lm/versions/1.2-4/topics/nlsLM
        ## https://lmfit.github.io/lmfit-py/model.html
        ## https://lmfit.github.io/lmfit-py/fitting.html#lmfit.minimizer.MinimizerResult

        ## nlsLM in R corresponds to lmfit in python
        def michaelis_menten(params, spend, exposure):
            vmax = params["vmax"]
            km = params["km"]
            model = vmax * spend / (km + spend)
            return exposure - model

        ## modNLS = curve_fit(michaelis_menten, dt_spendModInput["spend"], dt_spendModInput["exposure"], method='lm')
        ## popt, pconv, infodict, mesg, ier = modNLS

        start_params = create_params(vmax = nlsStartVal["Vmax"], km = nlsStartVal["Km"])
        modNLS = minimize(michaelis_menten, start_params, method='leastsq', args=(dt_spendModInput["spend"], dt_spendModInput["exposure"]) )
        ## modNLSparams = modNLS.make_params(vmax = nlsStartVal["Vmax"], km = nlsStartVal["Km"])

        # Get the predicted values
        ## yhatNLS = modNLS.predict()
        def predict_nls(spend, vmax, km):
            return vmax * spend / (km + spend)


        yhatNLS = predict_nls(dt_spendModInput["spend"], modNLS.params['vmax'].value, modNLS.params['km'].value)

        ## modNLSSum = modNLS.fit(dt_spendModInput["exposure"], params=modNLSparams, spend=dt_spendModInput["spend"])
        ## modNLS = nlsLM(
        ##    exposure=nlsStartVal["Vmax"]
        ##    * dt_spendModInput["spend"]
        ##    / (nlsStartVal["Km"] + dt_spendModInput["spend"]),
        ##    data=dt_spendModInput,
        ##    start=nlsStartVal,


        # Get the model summary
        ## modNLSSum = summary(modNLS)
        modNLSSum = modNLS

        # Calculate the R-squared value
        rsq_nls = r2_score(dt_spendModInput["exposure"], yhatNLS)

        # Check if the model is a good fit
        if rsq_nls < 0.7:
            print(
                f"Spend-exposure fitting for {paid_media_var} has rsq = {round(rsq_nls, 4)}. To increase the fit, try splitting the variable. Otherwise consider using spend instead."
            )
    except Exception as e:
        print(f"Error fitting Michaelis-Menten model: {e}")
        modNLS = None
        yhatNLS = None
        modNLSSum = None
        rsq_nls = None

    # Model 2: Build linear model comparison model
    ## Manually add the callback model
    ## https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression
    ## https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html#scipy.stats.linregress
    ## https://lmfit.github.io/lmfit-py/builtin_models.html

    ## modLM = LinearRegression().fit(exposure=dt_spendModInput["spend"] - 1, data=dt_spendModInput)
    ## modLM = linregress(linear_model(dt_spendModInput["spend"]), dt_spendModInput["exposure"])
    ## def predict_linear_model(spend, lm_model):
        ## return lm_model.intercept + lm_model.slope * spend
    ## yhatLM = modLM.predict()

    modLM = LinearModel()
    lm_params = modLM.make_params(intercept=-1, slope=1)
    modLMSum = modLM.fit(dt_spendModInput["exposure"], lm_params, x=dt_spendModInput["spend"])

    ## modLMSum = summary(modLM)
    yhatLM = modLM.eval(lm_params, x=dt_spendModInput["spend"])
    ##rsq_lm = modLMSum.r2_score
    ##rsq_lm = modLMSum.rvalue ** 2
    rsq_lm = modLMSum.rsquared

    ## Manually added
    if rsq_lm is None or np.isnan(rsq_lm):
        raise ValueError(f"Check if {paid_media_var} contains only zeros")

    # Calculate the AIC and BIC values
    ## aic_nls = AIC(modNLS) if modNLS else np.nan
    ## aic_lm = AIC(modLM)
    ## bic_nls = BIC(modNLS) if modNLS else np.nan
    ## bic_lm = BIC(modLM)

    # Create the output dictionary
    output = {
        "res": pd.DataFrame(
            {
                ## "channel": [paid_media_var], corrected outputs
                "channel": [paid_media_var],
                "Vmax": [modNLS.params['vmax'].value if modNLS else np.nan],
                "Km": [modNLS.params['km'].value if modNLS else np.nan],
                "aic_nls": [modNLS.aic if modNLS is not None else None],
                "aic_lm": [modLMSum.aic],
                "bic_nls": [modNLS.bic if modNLS is not None else None],
                "bic_lm": [modLMSum.bic],
                "rsq_nls": [rsq_nls if modNLS is not None else None],
                "rsq_lm": [rsq_lm],
                "coef_lm": [modLMSum.summary()['best_values']['slope']],
            }
        ),
        "yhatNLS": yhatNLS,
        "modNLS": modNLS,
        "yhatLM": yhatLM,
        "modLM": modLM,
        "data": dt_spendModInput,
        "type": ["mm" if modNLS else "lm"],
    }

    return output


def set_holidays(dt_transform, dt_holidays, intervalType):
    """
    Sets the holidays based on the given interval type.

    Args:
        dt_transform (DataFrame): The transformed date DataFrame.
        dt_holidays (DataFrame): The holidays DataFrame.
        intervalType (str): The interval type. Can be one of: "day", "week", "month".

    Returns:
        DataFrame: The holidays DataFrame with aggregated holiday information.

    Raises:
        ValueError: If the intervalType is not one of the valid options.
        ValueError: If the week start is not Monday or Sunday for the "week" intervalType.
        ValueError: If the monthly data does not have the first day of the month as the datestamp.

    """
    opts = ["day", "week", "month"]
    if intervalType not in opts:
        raise ValueError(
            "Pass a valid 'intervalType'. Any of: {}, {}".format(opts, intervalType)
        )

    if intervalType == "day":
        holidays = dt_holidays
    elif intervalType == "week":
        ## week_start = dt_transform.ds.dt.weekday(1)
        week_start = dt_transform['ds'].dt.dayofweek[0] + 1
        if week_start not in [1, 7]:
            raise ValueError("Week start has to be Monday or Sunday")

        ## holidays = dt_holidays['ds'].dt.floor(
        ##    dt.Date(dt_transform.ds.dt.strftime("%Y-%m-%d"), origin="1970-01-01"),
        ##    unit="week",
        ##    week_start=week_start,
        ##)
        dt_holidays['ds'] = dt_holidays['ds'] - pd.to_timedelta(dt_holidays['ds'].dt.dayofweek, unit='d')

        ## holidays = holidays.dt.select(
        ##    holidays.ds, holidays.holiday, holidays.country, holidays.year
        ##)
        holidays = dt_holidays[['ds', 'holiday', 'country', 'year']]

        ##holidays = holidays.dt.groupby(
        ##    holidays.ds, holidays.country, holidays.year
        ##).agg({"holiday": lambda x: ", ".join(x), "n": "count"})

        holidays = holidays.groupby(['ds', 'country', 'year']).agg({'holiday':[' ,'.join, 'count']}).reset_index()
        temp = pd.DataFrame()
        temp['ds'] = holidays['ds']
        temp['country'] = holidays['country']
        temp['year'] = holidays['year']
        temp['holiday'] = holidays['holiday']['join']
        temp['n'] = holidays['holiday']['count']
        holidays = temp

    elif intervalType == "month":
        if not all(dt_transform['ds'].dt.is_month_start):
            raise ValueError(
                "Monthly data should have first day of month as datestampe, e.g.'2020-01-01'"
            )
        dt_holidays['ds'] = dt_holidays['ds'] - pd.to_timedelta(dt_holidays['ds'].dt.days_in_month, unit='d')
        holidays = holidays[['ds', 'holiday', 'country', 'year']]
        holidays = holidays.groupby(['ds', 'country', 'year']).agg({'holiday':[' ,'.join, 'count']}).reset_index()
        temp = df.DataFrame()
        temp['ds'] = holidays['ds']
        temp['country'] = holidays['country']
        temp['year'] = holidays['year']
        temp['holiday'] = holidays['holiday']['join']
        temp['n'] = holidays['holiday']['count']
        holidays = temp

    return holidays

================
File: json.py
================
# Copyright (c) Meta Platforms, Inc. and its affiliates.

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

####################################################################
import os
import json
import pandas as pd
import numpy as np
import re
import warnings

def robyn_write(InputCollect, OutputCollect=None, select_model=None, dir=None, export=True, quiet=False, pareto_df=None):
    """
    A function that takes in various inputs and exports a Robyn model.

    Parameters:
    - InputCollect: A robyn_inputs object.
    - OutputCollect: A robyn_outputs object (optional).
    - select_model: A string representing the selected model (optional).
    - dir: A valid directory path (optional).
    - export: A boolean indicating whether to export the model (default is True).
    - quiet: A boolean indicating whether to print export messages (default is False).
    - pareto_df: A DataFrame containing 'solID' and 'cluster' columns (optional).

    Returns:
    - ret: A dictionary containing the exported model data.
    """
    # Checks
    if not isinstance(InputCollect, robyn_inputs):
        raise ValueError("InputCollect must be a robyn_inputs object")
    if OutputCollect and not isinstance(OutputCollect, robyn_outputs):
        raise ValueError("OutputCollect must be a robyn_outputs object")
    if select_model and not isinstance(select_model, str):
        raise ValueError("select_model must be a string")
    if dir and not os.path.isdir(dir):
        raise ValueError("dir must be a valid directory path")
    if export and not os.path.isdir(dir):
        os.makedirs(dir, exist_ok=True)

    # InputCollect JSON
    ## ret = {}
    ret = dict()
    skip = [x for x in range(len(InputCollect)) if isinstance(InputCollect[x], (list, np.ndarray)) or (isinstance(InputCollect[x], str) and InputCollect[x].startswith("robyn_"))]
    ret["InputCollect"] = InputCollect[:len(InputCollect)-len(skip)]

    # ExportedModel JSON
    if OutputCollect:
        ##collect = {}
        collect = dict()
        collect["ts_validation"] = OutputCollect.ts_validation
        collect["train_timestamp"] = OutputCollect.train_timestamp
        collect["export_timestamp"] = pd.Timestamp.now()
        collect["run_time"] = f"{OutputCollect.run_time} min"
        collect["outputs_time"] = f"{OutputCollect.outputs_time} min"
        collect["total_time"] = f"{OutputCollect.run_time + OutputCollect.outputs_time} min"
        collect["total_iters"] = OutputCollect.iterations * OutputCollect.trials
        collect["conv_msg"] = re.sub(r":.*", "", OutputCollect.convergence.conv_msg)
        if "clusters" in OutputCollect:
            collect["clusters"] = OutputCollect.clusters.n_clusters
        skip = [x for x in range(len(OutputCollect)) if isinstance(OutputCollect[x], (list, np.ndarray)) or (isinstance(OutputCollect[x], str) and OutputCollect[x].startswith("robyn_"))]
        collect = {k: v for k, v in collect.items() if k not in skip}
        ret["ModelsCollect"] = collect

    # Model associated data
    if select_model:
        outputs = {}
        outputs["select_model"] = select_model
        outputs["summary"] = filter(OutputCollect.xDecompAgg, solID=select_model)
        outputs["errors"] = filter(OutputCollect.resultHypParam, solID=select_model)
        outputs["hyper_values"] = OutputCollect.resultHypParam.loc[select_model]
        outputs["hyper_updated"] = OutputCollect.hyper_updated
        ret["ExportedModel"] = outputs
    else:
        select_model = "models"

    if not dir.exists(dir) and export:
        dir.create(dir, recursive=True)

    filename = f"{dir}/RobynModel-{select_model}.json"
    filename = re.sub(r"//", "/", filename)
    ## ret.class = ["robyn_write", ret.class]
    ##ret.attr("json_file") = filename
    if export is not None:
        if quiet is False:
            print(f">> Exported model {select_model} as {filename}")

        if pareto_df is not None:
            if not all([x in pareto_df.columns for x in ("solID", "cluster")]):
                warnings.warn("Input 'pareto_df' is not a valid data.frame; must contain 'solID' and 'cluster' columns.")
            else:
                all_c = set(pareto_df['cluster'])
                pareto_df = [pareto_df[pareto_df.cluster == x] for x in all_c]
                ## names(pareto_df) = paste0("cluster", all_c)
                pareto_df.rename(columns={"cluster": "cluster"}, inplace=True)
                ret["OutputCollect"]["all_sols"] = pareto_df

    write_json(ret, filename, pretty=True, digits=10)
    return ret

def print_robyn_write(x, *args, **kwargs):
    """
    Print various information related to the exported model and its performance.

    Args:
        x: The exported model object.
        *args: Additional positional arguments.
        **kwargs: Additional keyword arguments.
    """
    # Print exported directory, model, and window information
    print("Exported directory: {x.ExportedModel.plot_folder}")
    print("Exported model: {x.ExportedModel.select_model}")
    print("Window: {start} to {end} ({periods} {type}s)")

    # Print time series validation information
    val = x.ExportedModel.ts_validation
    print("Time Series Validation: {val} (train size = {val_detail})")

    # Print model performance and errors
    errors = x.ExportedModel.errors
    print("Model's Performance and Errors:")
    print("    {errors}")

    # Print summary values on selected model
    print("Summary Values on Selected Model:")
    print(x.ExportedModel.summary.drop(columns=["boot", "ci_"]).rename(columns={"performance": "ROI"}).mutate(decompPer=lambda x: format_num(100*x, pos="%")).replace(to_replace="NA", value="-").to_dataframe())

    # Print hyper-parameters
    print("Hyper-parameters:")
    print("    Adstock: {x.InputCollect.adstock}")

    # Print nice and tidy table format for hyper-parameters
    hyper_df = x.ExportedModel.hyper_values.drop(columns=["lambda", "penalty"]).gather().separate(key="key", into=["channel", "none"], sep=r"_", remove=False).mutate(hyperparameter=lambda x: gsub("^.*_", "", x.key)).select(channel=["channel", "hyperparameter"], value=["value"]).spread(key="hyperparameter", value="value")
    print(hyper_df)

def robyn_read(json_file=None, step=1, quiet=False):
    """
    Reads a JSON file and performs some modifications on the data.

    Args:
        json_file (str): The path to the JSON file to be read.
        step (int): The step value.
        quiet (bool): If True, suppresses the print statement.

    Returns:
        dict or None: The modified JSON data if `json_file` is provided, otherwise returns `json_file`.
    
    Raises:
        ValueError: If `json_file` is not a string or does not end with ".json".
        FileNotFoundError: If the specified `json_file` does not exist.
    """
    if json_file is not None:
        if not isinstance(json_file, str):
            raise ValueError("JSON file must be a string")
        if not json_file.endswith(".json"):
            raise ValueError("JSON file must be a valid .json file")
        if not os.path.exists(json_file):
            raise FileNotFoundError("JSON file can't be imported: {}".format(json_file))
        json = json.loads(open(json_file, "r").read())
        json["InputCollect"] = [x for x in json["InputCollect"] if len(x) > 0]
        json["ExportedModel"] = json["ModelsCollect"] + [json["ExportedModel"]]
        if not quiet:
            print("Imported JSON file successfully: {}".format(json_file))
        return json
    return json_file

def robyn_read(x, *args, **kwargs):
    # Extract input collect
    a = x['InputCollect']

    # Create a string to print
    str_to_print = """

########### InputCollect ############



Date: {a['date_var']}

Dependent: {a['dep_var']} [{a['dep_var_type']}]

Paid Media: {', '.join(a['paid_media_vars'])}

Paid Media Spend: {', '.join(a['paid_media_spends'])}

Context: {', '.join(a['context_vars'])}

Organic: {', '.join(a['organic_vars'])}

Prophet (Auto-generated): {a['prophet']}

Unused variables: {a['unused_vars']}

Model Window: {', '.join(a['window_start'], a['window_end'], sep=':')} ({a['rollingWindowEndWhich'] - a['rollingWindowStartWhich'] + 1} {a['intervalType']}s)

With Calibration: {not a['calibration_input'].isnull()}

Custom parameters: {a['custom_params']}



Adstock: {a['adstock']}

{hyps}

""".format(**a)

    # Print the string
    print(str_to_print)

    # Check if there's an exported model
    if x['ExportedModel'] is not None:
        # Create a new dataframe with the exported model
        temp = x.copy()

        # Set the class of the dataframe to "robyn_write"
        temp.attrs['class'] = 'robyn_write'

        # Print a blank line
        print()

        # Print the exported model
        print(temp)

    # Return an invisible object
    return pd.Series([], index=[0])

def robyn_recreate(json_file, quiet=False, *args, **kwargs):
    """
    Recreates a model based on the provided JSON file.

    Parameters:
    - json_file (str): The path to the JSON file.
    - quiet (bool): Whether to suppress console output. Default is False.
    - *args: Additional positional arguments.
    - **kwargs: Additional keyword arguments.

    Returns:
    - list: A list containing the InputCollect and OutputCollect objects.
    """
    # Read JSON file
    json = json.load(open(json_file, 'r'))

    # Extract model name
    model_name = json['ExportedModel']['select_model']
    print(f">>> Recreating model {model_name}")

    # Create list of arguments
    args = list(args)

    # Check if InputCollect is provided
    if 'InputCollect' not in args:
        # If not, create it using robyn_inputs
        InputCollect = robyn_inputs(json_file, quiet=quiet, *args, **kwargs)
        # Run model using robyn_run
        OutputCollect = robyn_run(InputCollect, json_file, export=False, quiet=quiet, *args, **kwargs)
    else:
        # If InputCollect is provided, use it
        InputCollect = args.pop('InputCollect')
        OutputCollect = robyn_run(InputCollect, json_file, export=False, quiet=quiet, *args, **kwargs)

    # Return list of InputCollect and OutputCollect
    return [InputCollect, OutputCollect]

def robyn_chain(json_file):
    """
    Extracts chain data from a JSON file and returns it as a dictionary.

    Parameters:
    json_file (str): The path to the JSON file.

    Returns:
    dict: A dictionary containing the extracted chain data.
    """
    # Read JSON data from file
    json_data = json.load(open(json_file, 'r'))

    # Extract IDs from JSON data
    ids = [json_data['InputCollect']['refreshChain'], json_data['ExportedModel']['select_model']]

    # Extract plot folder from JSON data
    plot_folder = json_data['ExportedModel']['plot_folder']

    # Split plot folder into parts
    temp = re.split('/', plot_folder)[1]

    # Extract chain from plot folder
    chain = temp[re.search('Robyn_', temp).start():]

    # If chain is empty, use the last part of the plot folder
    if not chain:
        chain = temp[temp != ''].pop()

    # Remove chain from plot folder
    base_dir = re.sub(f'/{chain}', '', plot_folder)

    # Initialize list to store chain data
    chain_data = dict()

    # Iterate over chain and read JSON files
    for i in range(len(chain)):
        if i == len(chain) - 1:
            json_new = json_data
        else:
            file = f'RobynModel-{json_new["InputCollect"]["refreshSourceID"]}.json'
            filename = os.path.join(base_dir, *chain[1:i], file)
            json_new = json.load(open(filename, 'r'))

        # Add JSON data to chain data
        chain_data[json_new['ExportedModel']['select_model']] = json_new

    # Reverse chain data
    chain_data = chain_data[::-1]

    # Extract plot folders from chain data
    dirs = [json_new['ExportedModel']['plot_folder'] for json_new in chain_data]

    # Create JSON file names
    json_files = [os.path.join(dir, f'RobynModel-{name}.json') for dir, name in zip(dirs, chain_data)]

    # Add JSON file names to chain data
    chain_data['json_files'] = json_files

    # Add chain to chain data
    chain_data['chain'] = ids

    # Check if chain and chain data match
    if len(ids) != len(chain_data):
        warnings.warn('Can\'t replicate chain-like results if you don\'t follow Robyn\'s chain structure')

    return chain_data

================
File: model.py
================
# Copyright (c) Meta Platforms, Inc. and its affiliates.

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

####################################################################
import time
import pandas as pd
import numpy as np
import nevergrad as ng
import random
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import Lasso, lasso_path
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from nevergrad.optimization import optimizerlib
import multiprocessing
from functools import partial
from tqdm import tqdm
import re
#from glmnet import glmnet
from sklearn.linear_model import Ridge
import logging
from math import sqrt
import math
from tqdm import tqdm
import warnings
warnings.simplefilter('ignore')
import sys
from collections import defaultdict
from datetime import datetime

## Robyn imports
from .inputs import hyper_names
from .checks import check_hyper_fixed, check_legacy_input, check_run_inputs, check_iteration, check_obj_weight, LEGACY_PARAMS, HYPS_OTHERS, check_adstock, check_parallel, check_init_msg
from .json import robyn_read ## name conflict?
from .outputs import robyn_outputs
from .transformation import run_transformations
from .calibration import robyn_calibrate
from .convergence import robyn_converge
from .plots import ts_validation_fun

## Manually added
from time import strftime, localtime
from scipy.stats import uniform
from itertools import repeat

def robyn_run(InputCollect=None,
              dt_hyper_fixed=None,
              json_file=None,
              ts_validation=False,
              add_penalty_factor=False,
              refresh=False,
              seed=123,
              quiet=False,
              cores=None,
              trials=5,
              iterations=2000,
              rssd_zero_penalty=True,
              objective_weights=None,
              nevergrad_algo="TwoPointsDE",
              intercept=True,
              intercept_sign="non_negative",
              lambda_control=None,
              outputs=False,
              *args,
              **kwargs
              ):

    if outputs:
        OutputModels = robyn_run(
            InputCollect=InputCollect,
            dt_hyper_fixed=dt_hyper_fixed,
            json_file=json_file,
            add_penalty_factor=add_penalty_factor,
            ts_validation=ts_validation,
            refresh=refresh,
            seed=seed,
            quiet=quiet,
            cores=cores,
            trials=trials,
            iterations=iterations,
            rssd_zero_penalty=rssd_zero_penalty,
            objective_weights=objective_weights,
            nevergrad_algo=nevergrad_algo,
            intercept=intercept,
            intercept_sign=intercept_sign,
            lambda_control=lambda_control,
            outputs=False
        )
        OutputCollect = robyn_outputs(InputCollect, OutputModels) ##, *args, **kwargs)

        return {
            "OutputModels": OutputModels,
            "OutputCollect": OutputCollect
        }

    ## t0 = time.time()
    t0 = strftime("%Y-%m-%d %H:%M:%S", localtime())

    # Use previously exported model using json_file
    if json_file is not None:
        # InputCollect <- robyn_inputs(json_file = json_file, dt_input = dt_input, dt_holidays = dt_holidays)
        if InputCollect is None:
            InputCollect = robyn_inputs(json_file=json_file) ##, *args, **kwargs)

        json_data = robyn_read(json_file, step=2, quiet=True)
        dt_hyper_fixed = json_data['ExportedModel']['hyper_values']
        for key, value in json_data['ExportedModel'].items():
            globals()[key] = value
        # Select specific columns from a DataFrame
        bootstrap = DataFrame(json_data['ExportedModel']['summary']).filter(items=['variable', 'boot_mean', 'ci_low', 'ci_up'])
        if seed is None or len(str(seed)) == 0:
            seed = 123
        dt_hyper_fixed['solID'] = json_data['ExportedModel']['select_model']
    else:
        bootstrap = None

    #####################################
    #### Set local environment

    # Check for 'hyperparameters' in InputCollect
    if "hyperparameters" not in InputCollect.keys() or InputCollect['hyperparameters'] is None:
        raise ValueError("Must provide 'hyperparameters' in robyn_inputs()'s output first")

    # Check and warn on legacy inputs
    InputCollect = check_legacy_input(InputCollect, cores, iterations, trials, intercept_sign, nevergrad_algo)
    # Overwrite values imported from InputCollect
    legacyValues = {k: v for k, v in InputCollect.items() if v is not None and k in LEGACY_PARAMS}
    if legacyValues:
        for key, value in InputCollect.items():
            globals()[key] = value

    # Handling cores
    max_cores = max(1, multiprocessing.cpu_count())
    if cores is None:
        cores = max_cores - 1  # Leave at least one core free
    elif cores > max_cores:
        print(f"Warning: Max possible cores in your machine is {max_cores} (your input was {cores})")
        cores = max_cores
    if cores == 0:
        cores = 1

    hyps_fixed = dt_hyper_fixed is not None
    if hyps_fixed:
        trials = iterations = 1
    check_run_inputs(cores, iterations, trials, intercept_sign, nevergrad_algo)
    check_iteration(InputCollect['calibration_input'], iterations, trials, hyps_fixed, refresh)
    init_msgs_run(InputCollect, refresh, quiet, lambda_control=None)
    objective_weights = check_obj_weight(InputCollect['calibration_input'], objective_weights, refresh)

    # Prepare hyper-parameters
    hyper_collect = hyper_collector(
        InputCollect,
        hyper_in=InputCollect['hyperparameters'],
        ts_validation=ts_validation,
        add_penalty_factor=add_penalty_factor,
        cores=cores,
        dt_hyper_fixed=dt_hyper_fixed
    )
    InputCollect['hyper_updated'] = hyper_collect['hyper_list_all']

    # Run robyn_mmm() for each trial
    OutputModels = robyn_train(
        InputCollect, hyper_collect,
        cores=cores, iterations=iterations, trials=trials,
        intercept_sign=intercept_sign, intercept=intercept,
        nevergrad_algo=nevergrad_algo,
        dt_hyper_fixed=dt_hyper_fixed,
        ts_validation=ts_validation,
        add_penalty_factor=add_penalty_factor,
        rssd_zero_penalty=rssd_zero_penalty,
        objective_weights=objective_weights,
        refresh=refresh, seed=seed, quiet=quiet
    )

    OutputModels['metadata']['hyper_fixed'] = hyper_collect['all_fixed']['hyper_fixed']
    OutputModels['metadata']['bootstrap'] = bootstrap
    OutputModels['metadata']['refresh'] = refresh
    OutputModels['metadata']['train_timestamp'] = time.time()
    OutputModels['metadata']['cores'] = cores
    OutputModels['metadata']['iterations'] = iterations
    OutputModels['metadata']['trials'] = trials
    OutputModels['metadata']['intercept'] = intercept
    OutputModels['metadata']['intercept_sign'] = intercept_sign
    OutputModels['metadata']['nevergrad_algo'] = nevergrad_algo
    OutputModels['metadata']['ts_validation'] = ts_validation
    OutputModels['metadata']['add_penalty_factor'] = add_penalty_factor
    OutputModels['metadata']['hyper_updated'] = hyper_collect['hyper_list_all']

    # Handling different output conditions
    if dt_hyper_fixed is None:
        output = OutputModels
    elif not hyper_collect['all_fixed']:
        # Direct output & not all fixed hyperparameters, including refresh mode
        output = robyn_outputs(InputCollect, OutputModels, refresh=refresh, *args, **kwargs)
    else:
        # Direct output & all fixed hyperparameters, thus no cluster
        output = robyn_outputs(InputCollect, OutputModels, clusters=False, *args, **kwargs)

    # Check convergence when more than 1 iteration
    if not hyper_collect['all_fixed']['hyper_fixed']:
        output["convergence"] = robyn_converge(OutputModels, *args, **kwargs)
        output["ts_validation_plot"] = ts_validation_fun(OutputModels, *args, **kwargs)
    else:
        if "solID" in dt_hyper_fixed:
            output["selectID"] = dt_hyper_fixed["solID"]
        else:
            output["selectID"] = OutputModels['trial1']['resultCollect']['resultHypParam']['solID']
        if not quiet:
            print(f"Successfully recreated model ID: {output['selectID']}")

    # Save hyper-parameters list
    output["hyper_updated"] = hyper_collect['hyper_list_all']
    output["seed"] = seed

    # Report total timing
    t0_datetime = datetime.strptime(t0, '%Y-%m-%d %H:%M:%S')
    current_time = datetime.fromtimestamp(time.time())
    time_diff_seconds = (current_time - t0_datetime).total_seconds()
    runTime = round(time_diff_seconds / 60, 2)

    if not quiet and iterations > 1:
        print(f"Total run time: {runTime} mins")

    output['__class__'] = "robyn_models"  # Assuming the need to store class information
    return output

#' @rdname robyn_run
#' @aliases robyn_run
#' @param x \code{robyn_models()} output.
#' @export
def print_robyn_models(x):
    is_fixed = all(len(h) == 1 for h in x['hyper_updated'].values())
    total_iters = f"({nrow(x['trial1']['resultCollect']['resultHypParam'])} real)" if "trial1" in x else "(1 real)"
    iters = ", ".join(map(str, x['convergence']['errors']['cuts'][-2:])) if x.get('convergence') else "1"
    fixed = " (fixed)" if is_fixed else ""
    convergence = "\n  ".join(x['convergence']['conv_msg']) if not is_fixed else "Fixed hyper-parameters"
    hypers = flatten_hyps(x['hyper_updated'])

    print(f"""
    Total trials: {x['trials']}
    Iterations per trial: {x['iterations']} {total_iters}
    Runtime (minutes): {x.get('runTime')}
    Cores: {x['cores']}

    Updated Hyper-parameters{fixed}:
    {hypers}

    Nevergrad Algo: {x['nevergrad_algo']}
    Intercept: {x['intercept']}
    Intercept sign: {x['intercept_sign']}
    Time-series validation: {x['ts_validation']}
    Penalty factor: {x['add_penalty_factor']}
    Refresh: {bool(x.get('refresh'))}

    Convergence on last quantile (iters {iters}):
        {convergence}
    """)

    def nrow(df):
        # Assuming df is a DataFrame or similar structure
        return len(df)

    def flatten_hyps(hyp_dict):
        # This function should transform the hyperparameters dictionary into a string
        # Assuming a simple implementation here; modify as needed
        return '\n  '.join(f'{k}: {v}' for k, v in hyp_dict.items())

    if "robyn_outputs" in x.get('__class__', []):
        clusters_info = ""
        if "clusters" in x:
            clusters_info = f"Clusters (k = {x['clusters']['n_clusters']}): {', '.join(x['clusters']['models']['solID'])}"

        print(f"""
            Plot Folder: {x.get('plot_folder')}
            Calibration Constraint: {x.get('calibration_constraint')}
            Hyper-parameters fixed: {x.get('hyper_fixed')}
            Pareto-front ({x.get('pareto_fronts')}) All solutions ({len(x.get('allSolutions', []))}): {', '.join(x.get('allSolutions', []))}
            {clusters_info}
        """)

####################################################################
#' Train Robyn Models
#'
#' \code{robyn_train()} consumes output from \code{robyn_input()}
#' and runs the \code{robyn_mmm()} on each trial.
#'
#' @inheritParams robyn_run
#' @param hyper_collect List. Containing hyperparameter bounds. Defaults to
#' \code{InputCollect$hyperparameters}.
#' @return List. Iteration results to include in \code{robyn_run()} results.
#' @export
def robyn_train(InputCollect, hyper_collect, cores, iterations, trials,
                intercept_sign, intercept, nevergrad_algo, dt_hyper_fixed=None,
                ts_validation=True, add_penalty_factor=False, objective_weights=None,
                rssd_zero_penalty=True, refresh=False, seed=123, quiet=False):

    hyper_fixed = hyper_collect['all_fixed']

    OutputModels = {
        "trials": [],
        "metadata": {}
    }

    if hyper_fixed['hyper_fixed'] == True:
        OutputModels["trials"].append(robyn_mmm(
            InputCollect=InputCollect,
            hyper_collect=hyper_collect,
            iterations=iterations,
            cores=cores,
            nevergrad_algo=nevergrad_algo,
            intercept_sign=intercept_sign,
            intercept=intercept,
            dt_hyper_fixed=dt_hyper_fixed,
            ts_validation=ts_validation,
            add_penalty_factor=add_penalty_factor,
            rssd_zero_penalty=rssd_zero_penalty,
            objective_weights=objective_weights,
            seed=seed,
            quiet=quiet
        ))
        OutputModels["trials"][0]['trial'] = 1

        if "solID" in dt_hyper_fixed:
            these = ["resultHypParam", "xDecompVec", "xDecompAgg", "decompSpendDist"]
            for tab in these:
                OutputModels["trials"][0]['resultCollect'][tab]['solID'] = dt_hyper_fixed['solID']

    else:
        # Run robyn_mmm() for each trial if hyperparameters are not all fixed
        check_init_msg(InputCollect, cores)
        if not quiet:
            calibration_phrase = "with calibration using" if InputCollect['calibration_input'] is not None else "using"
            print(f">>> Starting {trials} trials with {iterations} iterations each {calibration_phrase} {nevergrad_algo} nevergrad algorithm...")

        for ngt in range(1, trials + 1):  # Python uses 0-based indexing, so range is adjusted
            if not quiet:
                print(f"  Running trial {ngt} of {trials}")
            model_output = robyn_mmm(
                InputCollect=InputCollect,
                hyper_collect=hyper_collect,
                iterations=iterations,
                cores=cores,
                nevergrad_algo=nevergrad_algo,
                intercept_sign=intercept_sign,
                intercept=intercept,
                ts_validation=ts_validation,
                add_penalty_factor=add_penalty_factor,
                rssd_zero_penalty=rssd_zero_penalty,
                objective_weights=objective_weights,
                refresh=refresh,
                trial=ngt,
                seed=seed + ngt,
                quiet=quiet
            )
            check_coef0 = any(value == float('inf') for value in model_output['resultCollect']['decompSpendDist']['decomp.rssd'])
            if check_coef0:
                # Assuming model_output['resultCollect']['decompSpendDist'] is a DataFrame or similar
                num_coef0_mod = len(model_output['resultCollect']['decompSpendDist'][model_output['resultCollect']['decompSpendDist']['decomp.rssd'].apply(lambda x: x == float('inf'))].drop_duplicates(subset=['iterNG', 'iterPar']))
                num_coef0_mod = min(num_coef0_mod, iterations)
                if not quiet:
                    print(f"This trial contains {num_coef0_mod} iterations with all media coefficient = 0. "
                          "Please reconsider your media variable choice if the pareto choices are unreasonable.\n"
                          "   Recommendations:\n"
                          "1. Increase hyperparameter ranges for 0-coef channels to give Robyn more freedom\n"
                          "2. Split media into sub-channels, and/or aggregate similar channels, and/or introduce other media\n"
                          "3. Increase trials to get more samples")
            model_output['trial'] = ngt
            OutputModels["trials"].append(model_output)

    for i in range(len(OutputModels["trials"])):
        OutputModels["trials"][i]['name'] = f"trial{i + 1}"

    return OutputModels

####################################################################
#' Core MMM Function
#'
#' \code{robyn_mmm()} function activates Nevergrad to generate samples of
#' hyperparameters, conducts media transformation within each loop, fits the
#' Ridge regression, calibrates the model optionally, decomposes responses
#' and collects the result. It's an inner function within \code{robyn_run()}.
#'
#' @inheritParams robyn_run
#' @inheritParams robyn_allocator
#' @param hyper_collect List. Containing hyperparameter bounds. Defaults to
#' \code{InputCollect$hyperparameters}.
#' @param iterations Integer. Number of iterations to run.
#' @param trial Integer. Which trial are we running? Used to ID each model.
#' @return List. MMM results with hyperparameters values.
#' @export
def robyn_mmm(InputCollect,
              hyper_collect,
              iterations,
              cores,
              nevergrad_algo,
              intercept_sign,
              intercept=True,
              ts_validation=True,
              add_penalty_factor=False,
              objective_weights=None,
              dt_hyper_fixed=None,
              rssd_zero_penalty=True,
              refresh=False,
              trial=1,
              seed=123,
              quiet=False):

    ## This is not necessary as nevergrad is being used in R with Interface.
    ## try:
    ##    import nevergrad as ng
    ## except ImportError:
    ##    raise ImportError("You must have the nevergrad python library installed.\n"
    ##                      "Please check the installation instructions: "
    ##                      "https://github.com/facebookexperimental/Robyn/blob/main/demo/install_nevergrad.R")

    #if isinstance(seed, int):
    #    np.random.seed(124)
    #    random.seed(124)

    ################################################
    #### Collect hyperparameters

    ##hypParamSamName = list(hyper_collect['hyper_list_all'].keys())
    hypParamSamName = list(hyper_collect['hyper_list_all'].keys())
    # Optimization hyper-parameters
    hyper_bound_list_updated = hyper_collect['hyper_bound_list_updated']
    hyper_bound_list_updated_name = list(hyper_bound_list_updated.keys())
    hyper_count = len(hyper_bound_list_updated_name)
    # Fixed hyper-parameters
    hyper_bound_list_fixed = hyper_collect['hyper_bound_list_fixed']
    hyper_bound_list_fixed_name = list(hyper_bound_list_fixed.keys())
    hyper_count_fixed = len(hyper_bound_list_fixed_name)
    dt_hyper_fixed_mod = hyper_collect['dt_hyper_fixed_mod']
    hyper_fixed = hyper_collect['all_fixed']

    ################################################
    #### Setup environment

    ##if InputCollect.get('dt_mod') is None:
    if 'dt_mod' not in InputCollect.keys():
        raise ValueError("Run InputCollect['dt_mod'] = robyn_engineering() first to get the dt_mod")

    # Since the condition is always TRUE, we directly assign the variables
    dt_mod = InputCollect['dt_mod']
    xDecompAggPrev = InputCollect['xDecompAggPrev']
    rollingWindowStartWhich = InputCollect.get('rollingWindowStartWhich')
    rollingWindowEndWhich = InputCollect.get('rollingWindowEndWhich')
    refreshAddedStart = InputCollect.get('refreshAddedStart')
    dt_modRollWind = InputCollect.get('dt_modRollWind')
    refresh_steps = InputCollect.get('refresh_steps')
    rollingWindowLength = InputCollect.get('rollingWindowLength')
    paid_media_spends = InputCollect.get('paid_media_spends')
    organic_vars = InputCollect.get('organic_vars')
    context_vars = InputCollect.get('context_vars')
    prophet_vars = InputCollect.get('prophet_vars')
    adstock = InputCollect.get('adstock')
    context_signs = InputCollect.get('context_signs')
    paid_media_signs = InputCollect.get('paid_media_signs')
    prophet_signs = InputCollect.get('prophet_signs')
    organic_signs = InputCollect.get('organic_signs')
    calibration_input = InputCollect.get('calibration_input')
    optimizer_name = nevergrad_algo
    i = None  # For parallel iterations (globalVar)

    ################################################
    #### Get spend share

    dt_inputTrain = InputCollect['dt_input'].iloc[(rollingWindowStartWhich-1):rollingWindowEndWhich]
    temp = dt_inputTrain[paid_media_spends]
    dt_spendShare = pd.DataFrame({
        'rn': paid_media_spends,
        'total_spend': temp.sum(),
        'mean_spend': temp.mean()
    })
    dt_spendShare['spend_share'] = dt_spendShare['total_spend'] / dt_spendShare['total_spend'].sum()

    # When not refreshing, dt_spendShareRF = dt_spendShare
    ## refreshAddedStartWhich = dt_modRollWind[dt_modRollWind['ds'] == refreshAddedStart].index[0]
    ## Return the index which is equal to refreshAddedStart
    refreshAddedStartWhich = dt_modRollWind.index[dt_modRollWind['ds'] == refreshAddedStart].tolist()[0] - (rollingWindowStartWhich - 1)

    temp = dt_inputTrain[paid_media_spends].iloc[(refreshAddedStartWhich):rollingWindowLength]
    dt_spendShareRF = pd.DataFrame({
        'rn': paid_media_spends,
        'total_spend': temp.sum(),
        'mean_spend': temp.mean()
    })
    dt_spendShareRF['spend_share'] = dt_spendShareRF['total_spend'] / dt_spendShareRF['total_spend'].sum()

    # Join both dataframes into a single one
    ##dt_spendShare = dt_spendShare.merge(dt_spendShareRF, on='rn', suffixes=('', '_refresh'))
    dt_spendShare = dt_spendShare.join(dt_spendShareRF, on='rn', how='left', rsuffix='_refresh')

    ################################################
    #### Get lambda

    lambda_min_ratio = 0.0001  # default value from glmnet
    # Assuming dt_mod is a DataFrame and dep_var is the name of the dependent variable column
    ##X = dt_mod[drop(columns=['ds', 'dep_var'])
    select_columns = [col for col in dt_mod.columns.values if col not in ['ds', 'dep_var']]
    X = dt_mod[select_columns].to_numpy()
    y = dt_mod[['dep_var']].to_numpy()
    # Generate a sequence of lambdas for regularization
    lambdas = lasso_path(X, y, eps=lambda_min_ratio)[0]  # lasso_path returns alphas which are equivalent to lambdas
    lambda_max = lambdas.max() * 0.1
    lambda_min = lambda_max * lambda_min_ratio

    # Start Nevergrad loop
    t0 = time.time()

    # Set iterations
    ##if not hyper_fixed:
    if hyper_fixed['hyper_fixed'] == False:
        iterTotal = iterations
        iterPar = cores
        iterNG = int(np.ceil(iterations / cores))  # Sometimes the progress bar may not get to 100%
    else:
        iterTotal = iterPar = iterNG = 1

    # Start Nevergrad optimizer
    ##if not hyper_fixed:
    if hyper_fixed['hyper_fixed'] == False:
        my_tuple = tuple([hyper_count])
        instrumentation = ng.p.Array(shape=my_tuple, lower=0, upper=1)
        optimizer = optimizerlib.registry[optimizer_name](instrumentation, budget=iterTotal, num_workers=cores)
        print(instrumentation)
        print(iterTotal)
        # Set multi-objective dimensions for objective functions (errors)
        if calibration_input is None:
            optimizer.tell(ng.p.MultiobjectiveReference(), tuple([1, 1]))
            if objective_weights is None:
                objective_weights = tuple([1, 1])
            else:
                objective_weights = tuple([objective_weights[0], objective_weights[1]])
            optimizer.set_objective_weights(objective_weights)
        else:
            optimizer.tell(ng.p.MultiobjectiveReference(), tuple([1, 1, 1]))
            if objective_weights is None:
                objective_weights = tuple([1, 1, 1])
            else:
                objective_weights = tuple([objective_weights[0], objective_weights[1], objective_weights[2]])
            optimizer.set_objective_weights(objective_weights)

    result_collect_ng = []
    cnt = 0
    pb = None

    ##if not hyper_fixed and not quiet:
    ## May not be necessary since tqdm is used.
    if hyper_fixed['hyper_fixed'] == False and quiet == False:
        pbar = tqdm(total=iterTotal, desc="Progress", leave=True)
    #    pb = range(iterTotal)

    sys_time_dopar = None

    try:
        sys_time_dopar = time.time()
        for lng in tqdm(range(iterTotal), desc="Optimization Progress"):
            nevergrad_hp = {}
            nevergrad_hp_val = {}
            hypParamSamList = []
            hypParamSamNG = dict()

            ##if not hyper_fixed:
            if hyper_fixed['hyper_fixed'] == False:
                # Setting initial seeds (co = cores)
                ##for co in range(1, iterPar + 1):  # co = 1
                for co in range(0, iterPar):  # co = 1
                    # Get hyperparameter sample with ask (random)
                    nevergrad_hp[co] = optimizer.ask()
                    nevergrad_hp_val[co] = nevergrad_hp[co].value

                    # Scale sample to given bounds using uniform distribution
                    ## [True if var in hyper_bound_list_updated_name else False for var in hyper_bound_list_updated.keys()]
                    for hypNameLoop in hyper_bound_list_updated_name:
                        index = hyper_bound_list_updated_name.index(hypNameLoop)
                        channelBound = hyper_bound_list_updated[hypNameLoop]

                        hyppar_value = round(nevergrad_hp_val[co][index], 10)

                        if len(channelBound) > 1:
                            ##hypParamSamNG[hypNameLoop] = uniform(hyppar_value, min(channelBound), max(channelBound))
                            hypParamSamNG[hypNameLoop] = uniform.ppf(hyppar_value, loc=min(channelBound), scale=max(channelBound) - min(channelBound))
                        else:
                            hypParamSamNG[hypNameLoop] = hyppar_value

                    hypParamSamList.append(pd.DataFrame(hypParamSamNG, index=[0])) ## .T)

                hypParamSamNG = pd.concat(hypParamSamList, ignore_index=True)
                hypParamSamNG.columns = hyper_bound_list_updated_name

                # Add fixed hyperparameters
                if hyper_count_fixed != 0:
                    train_size_value = dt_hyper_fixed_mod[1][0][0]
                    hypParamSamNG['train_size'] = [train_size_value] * len(hypParamSamNG)
                    valid_columns = set(hypParamSamNG.columns).intersection(hypParamSamName)
                    hypParamSamNG = hypParamSamNG[list(valid_columns)]

            else:
                hypParamSamNG = dt_hyper_fixed_mod[hypParamSamName]

            # Initialize lists to collect results
            nrmse_collect = []
            decomp_rssd_collect = []
            mape_lift_collect = []

            ## robyn_iterations()
            # Define a function to run robyn_iterations
            ## def run_robyn_iterations(i):
            ##   return robyn_iterations(i)


            def robyn_iterations(iteration, hypParamSamNG, adstock):  # i=1
                t1 = time.time()
                i = iteration

                # Get hyperparameter sample
                hypParamSam = hypParamSamNG.iloc[i - 1]  # Adjusted for 0-based indexing

                # Check and transform adstock
                adstock = check_adstock(adstock)

                # Transform media for model fitting
                temp = run_transformations(InputCollect, hypParamSam, adstock)
                dt_modSaturated = temp['dt_modSaturated']
                dt_saturatedImmediate = temp['dt_saturatedImmediate']
                dt_saturatedCarryover = temp['dt_saturatedCarryover']

                # Split train & test and prepare data for modeling
                dt_window = dt_modSaturated

                # Contrast matrix because glmnet does not treat categorical variables (one hot encoding)
                y_window = list(dt_window['dep_var'].values)
                select_columns = [var for var in dt_window.columns.values if var != 'dep_var' and var != 'index']
                x_window = pd.get_dummies(dt_window[select_columns]) ##, columns=select_columns) ##TODO: better to use OneHotEncoding or not?
                ## x_window = lares.ohse(dt_window.drop('dep_var', axis=1)).values  # Assuming ohse returns a DataFrame

                y_train = y_val = y_test = y_window
                x_train = x_val = x_test = x_window

                train_size = hypParamSam['train_size'] ##.values[0]
                val_size = test_size = (1 - train_size) / 2
                if train_size < 1:
                    ## train_size_index = int(train_size * len(dt_window))
                    train_size_index = int(np.floor(np.quantile(range(len(dt_window)), [train_size])[0]))
                    ## val_size_index = train_size_index + int(val_size * len(dt_window))
                    val_size_index = train_size_index + int(np.floor(val_size * len(dt_window)))
                    y_train = y_window[:train_size_index]
                    y_val = y_window[train_size_index:val_size_index]
                    y_test = y_window[val_size_index:]
                    x_train = x_window[:train_size_index]
                    x_val = x_window[train_size_index:val_size_index]
                    x_test = x_window[val_size_index:]
                else:
                    y_val = y_test = x_val = x_test = None

                # Define and set sign control
                select_columns = [var for var in dt_window.columns.values if var != 'dep_var' and var != 'index']
                dt_sign = dt_window[select_columns]
                x_sign = prophet_signs + context_signs + paid_media_signs + organic_signs #TODO: why signs are not same as in R??
                check_factor = dt_sign.applymap(lambda x: isinstance(x, pd.CategoricalDtype)) ##TODO: In R all returns False
                lower_limits = [0] * len(prophet_signs)
                upper_limits = [1] * len(prophet_signs)
                for s in range(len(prophet_signs), len(x_sign)):
                    if s in check_factor.index:
                        level_n = len(dt_sign.iloc[:, s].astype('category').cat.categories)
                        if level_n <= 1:
                            raise ValueError("All factor variables must have more than 1 level")
                        lower_vec = [0] * (level_n - 1) if x_sign[s] == "positive" else [-float('inf')] * (level_n - 1)
                        upper_vec = [0] * (level_n - 1) if x_sign[s] == "negative" else [float('inf')] * (level_n - 1)
                        lower_limits.extend(lower_vec)
                        upper_limits.extend(upper_vec)
                    else:
                        lower_limits.append(0 if x_sign[s] == "positive" else -float('inf'))
                        upper_limits.append(0 if x_sign[s] == "negative" else float('inf'))

                # Fit ridge regression with nevergrad's lambda
                lambda_hp = float(hypParamSamNG['lambda'].iloc[i-1])
                ##if not hyper_fixed:
                if hyper_fixed['hyper_fixed'] == False:
                    lambda_scaled = lambda_min + (lambda_max - lambda_min) * lambda_hp
                else:
                    lambda_scaled = lambda_hp

                if add_penalty_factor:
                    penalty_factor = hypParamSamNG.iloc[i, [col.endswith("_penalty") for col in hypParamSamNG.columns]]
                else:
                    penalty_factor = [1] * x_train.shape[1]

                mod_out = model_refit(
                    x_train, y_train,
                    x_val, y_val,
                    x_test, y_test,
                    lambda_scaled,
                    lower_limits,
                    upper_limits,
                    intercept,
                    intercept_sign,
                    penalty_factor
                )

                decompCollect = model_decomp(
                    coefs=mod_out["coefs"],
                    y_pred=mod_out["y_pred"],
                    dt_modSaturated=dt_modSaturated,
                    dt_saturatedImmediate=dt_saturatedImmediate,
                    dt_saturatedCarryover=dt_saturatedCarryover,
                    dt_modRollWind=dt_modRollWind,
                    refreshAddedStart=refreshAddedStart
                )

                nrmse = mod_out["nrmse_val"] if ts_validation else mod_out["nrmse_train"]
                mape = 0
                df_int = mod_out["df_int"]

                # MAPE: Calibration error
                if calibration_input is not None:
                    liftCollect = robyn_calibrate(
                        calibration_input=calibration_input,
                        df_raw=dt_mod,
                        hypParamSam=hypParamSam,
                        wind_start=rollingWindowStartWhich,
                        wind_end=rollingWindowEndWhich,
                        dayInterval=InputCollect["dayInterval"],
                        adstock=adstock,
                        xDecompVec=decompCollect["xDecompVec"],
                        coefs=decompCollect["coefsOutCat"]
                    )
                    mape = liftCollect["mape_lift"].mean()

                df_xDecompAgg = decompCollect["xDecompAgg"][decompCollect["xDecompAgg"].index.isin(paid_media_spends)]
                dt_spendShare_idx = dt_spendShare.set_index('rn')
                dt_decompSpendDist = df_xDecompAgg.join(dt_spendShare_idx, how='inner')

                selected_columns = [
                    'coefs', 'xDecompAgg', 'xDecompPerc', 'xDecompMeanNon0', 'xDecompMeanNon0Perc',
                    'xDecompAggRF', 'xDecompPercRF', 'xDecompMeanNon0RF', 'xDecompMeanNon0PercRF',
                    'pos', 'total_spend', 'mean_spend', 'spend_share', 'mean_spend_refresh'
                ]

                dt_decompSpendDist = dt_decompSpendDist[selected_columns]

                dt_decompSpendDist = dt_decompSpendDist.assign(
                    effect_share=dt_decompSpendDist['xDecompPerc'] / dt_decompSpendDist['xDecompPerc'].sum(),
                    effect_share_refresh=dt_decompSpendDist['xDecompPercRF'] / dt_decompSpendDist['xDecompPercRF'].sum()
                )

                if not refresh:
                    decomp_rssd = sqrt(sum((dt_decompSpendDist["effect_share"] - dt_decompSpendDist["spend_share"])**2))

                    # Penalty for models with more 0-coefficients
                    if rssd_zero_penalty:
                        is_0eff = (dt_decompSpendDist["effect_share"].round(4) == 0)
                        share_0eff = sum(is_0eff) / len(dt_decompSpendDist["effect_share"])
                        decomp_rssd = decomp_rssd * (1 + share_0eff)
                else:
                    dt_decompRF = dt_decompSpendDist[["rn", "effect_share"]].merge(
                        xDecompAggPrev[["rn", "decomp_perc_prev"]],
                        on="rn"
                    )
                    decomp_rssd_media = dt_decompRF[dt_decompRF["rn"].isin(paid_media_spends)]["decomp_perc"].mean()
                    decomp_rssd_nonmedia = dt_decompRF[~dt_decompRF["rn"].isin(paid_media_spends)]["decomp_perc"].mean()
                    decomp_rssd = decomp_rssd_media + decomp_rssd_nonmedia / (1 - refresh_steps / rollingWindowLength)

                # Handle the case when all media in this iteration have 0 coefficients
                if math.isnan(decomp_rssd):
                    decomp_rssd = math.inf
                    dt_decompSpendDist["effect_share"] = 0

                # Initialize resultCollect list
                resultCollect = {}

                # Create a common DataFrame with shared values
                common = pd.DataFrame({
                    "rsq_train": mod_out["rsq_train"],
                    "rsq_val": mod_out["rsq_val"],
                    "rsq_test": mod_out["rsq_test"],
                    "nrmse_train": mod_out["nrmse_train"],
                    "nrmse_val": mod_out["nrmse_val"],
                    "nrmse_test": mod_out["nrmse_test"],
                    "nrmse": nrmse,
                    "decomp.rssd": decomp_rssd,
                    "mape": mape,
                    "lambda": lambda_scaled,
                    "lambda_hp": lambda_hp,
                    "lambda_max": lambda_max,
                    "lambda_min_ratio": lambda_min_ratio,
                    "solID": f"{trial}_{lng}_{i}",
                    "trial": trial,
                    "iterNG": lng,
                    "iterPar": i
                }, index=[0])

                total_common = common.shape[1]
                split_common = common.columns.get_loc("lambda_min_ratio")
                hypParamSam_df = hypParamSam.drop('lambda').to_frame().T

                current_time = time.time()
                Elapsed = current_time - t1
                ElapsedAccum = current_time - t0

                hypParamSam_df_reset = hypParamSam_df.reset_index(drop=True)
                common_reset = common.reset_index(drop=True)

                result_df = hypParamSam_df_reset.join(
                    common_reset.iloc[:, :split_common]
                )

                result_df = result_df.assign(
                    pos=decompCollect['xDecompAgg']['pos'].prod(),  # Adjust according to actual structure
                    Elapsed=Elapsed,
                    ElapsedAccum=ElapsedAccum
                )

                result_df = result_df.join(
                    common_reset.iloc[:, split_common:total_common]
                )

                dresult_hyp_param_df = pd.DataFrame(result_df)
                list_of_dicts = dresult_hyp_param_df.to_dict(orient='records')
                resultCollect["resultHypParam"] = list_of_dicts[0]

                repeated_common_reset = pd.concat([common_reset] * len(decompCollect["xDecompAgg"]), ignore_index=True)
                decompCollect_xDecompAgg_reset = decompCollect["xDecompAgg"].reset_index()
                resultCollect["xDecompAgg"] = pd.concat([decompCollect_xDecompAgg_reset, repeated_common_reset], axis=1)

                if liftCollect is not None:
                    for column in common_reset.columns:
                        liftCollect[column] = common_reset.iloc[0][column]
                    resultCollect["liftCalibration"] = liftCollect

                repeated_common_reset_decompSpendDist = pd.DataFrame([common_reset.iloc[0]] * len(dt_decompSpendDist))
                resultCollect["decompSpendDist"] = pd.concat([dt_decompSpendDist.reset_index(), repeated_common_reset_decompSpendDist.reset_index()], axis=1)
                resultCollect.update(common_reset.to_dict())

                return resultCollect


            cores = 1 ## TODO Remove to test MULTIPROCESSING when Completed Debugging robyn_iterations
            # Parallel processing
            if cores == 1:
                ##dopar_collect = [run_robyn_iterations(i) for i in range(1, iterPar + 1)]
                dopar_collect = [robyn_iterations(i,hypParamSamNG, adstock) for i in range(1, iterPar + 1)]
            else:
                # Create a pool of worker processes
                if check_parallel() and hyper_fixed['hyper_fixed'] == False: ##not hyper_fixed:
                    pool = multiprocessing.Pool(processes=cores)
                else:
                    pool = multiprocessing.Pool(processes=1)

                # Use the pool to run robyn_iterations in parallel
                ##dopar_collect = pool.map(run_robyn_iterations, range(1, iterPar + 1))
                dopar_collect = pool.map(robyn_iterations, zip(range(1, iterPar + 1),
                                                               InputCollect,
                                                               hypParamSamNG,
                                                               prophet_signs,
                                                               context_signs,
                                                               paid_media_signs,
                                                               organic_signs,
                                                               adstock))
                pool.close()
                pool.join()

            # Collect nrmse, decomp.rssd, and mape.lift from the results
            for result in dopar_collect:
                nrmse_collect.append(result['nrmse'])
                decomp_rssd_collect.append(result['decomp.rssd'])
                mape_lift_collect.append(result['mape'])

            # Update optimizer objectives if not hyper_fixed
            ##if not hyper_fixed:
            if hyper_fixed['hyper_fixed'] == False:
                if calibration_input is None:
                    for co in range(1, iterPar + 1):
                        nrmse_collect_value = next(iter(nrmse_collect[co].values()), None)
                        decomp_rssd_collect_value = next(iter(decomp_rssd_collect[co].values()), None)
                        optimizer.tell(nevergrad_hp[co - 1], [nrmse_collect_value, decomp_rssd_collect_value])
                else:
                    for co in range(iterPar):  # iterPar is the total count, range() is 0-based
                        nrmse_collect_value = next(iter(nrmse_collect[co].values()), None)
                        decomp_rssd_collect_value = next(iter(decomp_rssd_collect[co].values()), None)
                        mape_lift_collect_value = next(iter(mape_lift_collect[co].values()), None)
                        optimizer.tell(nevergrad_hp[co], [nrmse_collect_value, decomp_rssd_collect_value, mape_lift_collect_value])

            result_collect_ng.append(dopar_collect)

            if not quiet:
                cnt += iterPar
                if hyper_fixed['hyper_fixed'] == False:
                    pbar.update(1)

    except Exception as err:
        if len(result_collect_ng) > 1:
            msg = "Error while running robyn_mmm(); providing PARTIAL results"
            print("Warning:", msg)
            print("Error:", err)
            sys_time_dopar = [time.time() - t0] * 3
        else:
            raise err

    pbar.close()
    # Stop the cluster to avoid memory leaks
    ## TODO: Is it necessary for Python?
    ## stop_implicit_cluster()
    ## register_do_seq()
    ## get_do_par_workers()

    if hyper_fixed['hyper_fixed'] == False:
        elapsed_time = time.time() - sys_time_dopar
        print("\r", f"\n  Finished in {round(elapsed_time / 60, 2)} mins")
        sys.stdout.flush()

    # Final result collect
    result_collect = {}

    # Construct resultHypParam
    resultHypParamDefaultDict = defaultdict(list)
    for group in result_collect_ng:
        for element in group:
            for key, value in element["resultHypParam"].items():
                resultHypParamDefaultDict[key].append(value)
    resultHypParam = dict(resultHypParamDefaultDict)
    result_collect["resultHypParam"] = pd.DataFrame.from_dict(resultHypParamDefaultDict)

    # Construct xDecompAgg
    rn_list = []
    xDecompAggDefaultDict = defaultdict(list)
    for group in result_collect_ng:
        for element in group:
            df = element['xDecompAgg']
            for _, row in df.iterrows():
                for col in df.columns:
                    xDecompAggDefaultDict[col].append(row[col])
                rn_list.append(row['rn'])

    xDecompAggDefaultDict['rn'] = rn_list
    result_collect["xDecompAgg"] = pd.DataFrame.from_dict(xDecompAggDefaultDict)
    cols = ['rn'] + [col for col in result_collect["xDecompAgg"] if col != 'rn']
    result_collect["xDecompAgg"] = result_collect["xDecompAgg"][cols]

    # Construct liftCalibration
    liftCalibrationDefaultDict = defaultdict(list)
    for group in result_collect_ng:
        for element in group:
            df = element['liftCalibration']
            for col in df.columns:
                liftCalibrationDefaultDict[col].extend(df[col].tolist())
    liftCalibrationDF = pd.DataFrame.from_dict(liftCalibrationDefaultDict)
    sortedLiftCalibrationDF = liftCalibrationDF.sort_values(by=['mape', 'liftMedia', 'liftStart'])
    result_collect["liftCalibration"] = pd.DataFrame.from_dict(sortedLiftCalibrationDF)

    # Construct decompSpendDist
    rn_list = []
    decompSpendDistDefaultDict = defaultdict(list)
    for group in result_collect_ng:
        for element in group:
            df = element['decompSpendDist']
            for _, row in df.iterrows():
                for col in df.columns:
                    decompSpendDistDefaultDict[col].append(row[col])
                rn_list.append(row['rn'])
    decompSpendDistDefaultDict['rn'] = rn_list
    result_collect["decompSpendDist"] = pd.DataFrame.from_dict(decompSpendDistDefaultDict)
    cols = ['rn'] + [col for col in result_collect["decompSpendDist"] if col != 'rn']
    result_collect["decompSpendDist"] = result_collect["decompSpendDist"][cols]

    # Skil following line for now as it looks a bug in the R code to me.
    # ["mape"] is a column of result_collect["liftCalibration"]. In R is always initialized to 0
    #result_collect["iter"] = len(result_collect["mape"])
    result_collect["elapsed.min"] = sys_time_dopar / 60

    # Adjust accumulated time
    result_collect["resultHypParam"]["ElapsedAccum"] = (
        result_collect["resultHypParam"]["ElapsedAccum"]
        - min(result_collect["resultHypParam"]["ElapsedAccum"])
        + result_collect["resultHypParam"]["Elapsed"].iloc[
            result_collect["resultHypParam"]["ElapsedAccum"].idxmin()
        ]
    )

    return {
        "resultCollect": result_collect,
        "hyperBoundNG": hyper_bound_list_updated,
        "hyperBoundFixed": hyper_bound_list_fixed,
    }


def model_decomp(coefs, y_pred, dt_modSaturated, dt_saturatedImmediate,
                 dt_saturatedCarryover, dt_modRollWind, refreshAddedStart):
    # Input for decomp
    y = dt_modSaturated['dep_var']
    x = dt_modSaturated.drop(columns=['dep_var'])

    #print("--------- coefs")
    #print(coefs)
    intercept = coefs.loc['Intercept', 's0']
    #print("--------- intercept")
    #print(intercept)
    x_name = x.columns
    x_factor = [col for col in x_name if isinstance(x[col][0], str)]

    coeffs_series = coefs['s0'].drop('Intercept')
    x_decomp = x.multiply(coeffs_series, axis=1)
    intercept_column = pd.Series([coefs.loc['Intercept', 's0']] * len(x_decomp), index=x_decomp.index)
    x_decomp.insert(0, 'intercept', intercept_column)
    x_decomp.reset_index(drop=True, inplace=True)
    if 'index' in x_decomp.columns:
        x_decomp.drop(columns=['index'], inplace=True)

    y_y_pred_df = pd.DataFrame({'y': y, 'y_pred': y_pred})
    x_decomp_out = pd.concat([dt_modRollWind[['ds']].reset_index(), y_y_pred_df.reset_index(), x_decomp.reset_index()], axis=1).drop(columns=['index'])
    media_features = coefs.index.intersection(dt_saturatedImmediate.columns)
    media_decomp_immediate = dt_saturatedImmediate[media_features].multiply(coefs.loc[media_features, 's0'], axis=1)
    media_decomp_carryover = dt_saturatedCarryover[media_features].multiply(coefs.loc[media_features, 's0'], axis=1)

    # Output decomp
    y_hat = x_decomp.sum(axis=1, skipna=True)
    y_hat_scaled = np.abs(x_decomp).sum(axis=1, skipna=True)
    x_decomp_out_perc_scaled = np.abs(x_decomp).divide(y_hat_scaled, axis=0)
    x_decomp_out_scaled = x_decomp_out_perc_scaled.multiply(y_hat, axis=0)

    existing_cols = ['intercept'] + [col for col in x_name if col in x_decomp_out.columns]
    temp = x_decomp_out[existing_cols]

    x_decomp_out_agg = temp.sum()

    x_decomp_out_agg_perc = x_decomp_out_agg / y_hat.sum()
    x_decomp_out_agg_mean_non0 = temp.apply(lambda x: 0 if np.isnan(np.mean(x[x > 0])) else np.mean(x[x != 0]))
    x_decomp_out_agg_mean_non0[np.isnan(x_decomp_out_agg_mean_non0)] = 0
    numeric_df = x_decomp_out_agg_mean_non0.apply(pd.to_numeric, errors='coerce')
    x_decomp_out_agg_mean_non0 = x_decomp_out_agg_mean_non0.astype(float)
    x_decomp_out_agg_mean_non0_perc = x_decomp_out_agg_mean_non0 / x_decomp_out_agg_mean_non0.sum().sum()

    refresh_added_start_which = x_decomp_out.index[x_decomp_out['ds'] == refreshAddedStart].tolist()[0]
    refresh_added_end = x_decomp_out['ds'].max()
    refresh_added_end_which = x_decomp_out.index[x_decomp_out['ds'] == refresh_added_end].tolist()[0]

    existing_cols = ['intercept'] + [col for col in x_name if col in x_decomp_out.columns]
    temp = x_decomp_out[existing_cols]
    temp = temp.loc[refresh_added_start_which:refresh_added_end_which]
    x_decomp_out_agg_rf = temp.sum()
    y_hat_rf = y_hat.loc[refresh_added_start_which:refresh_added_end_which]
    x_decomp_out_agg_perc_rf = x_decomp_out_agg_rf / y_hat_rf.sum()
    x_decomp_out_agg_mean_non0_rf = temp.apply(lambda x: 0 if np.isnan(np.mean(x[x > 0])) else np.mean(x[x != 0]))
    x_decomp_out_agg_mean_non0_rf[np.isnan(x_decomp_out_agg_mean_non0_rf)] = 0
    x_decomp_out_agg_mean_non0_perc_rf = x_decomp_out_agg_mean_non0_rf / x_decomp_out_agg_mean_non0_rf.sum().sum()

    coefs_out_cat = pd.DataFrame({
        'rn': coefs.index,
        's0': coefs['s0']
    }).reset_index(drop=True)
    coefs_out_cat.set_index('rn', inplace=True)

    if len(x_factor) > 0:
        for factor in x_factor:
            coefs_out_cat['rn'] = coefs_out_cat['rn'].apply(lambda x: re.sub(f"{factor}.*", factor, x))

    x_decomp_out_agg = x_decomp_out_agg.rename({'intercept': 'Intercept'})
    coefs_out = coefs_out_cat.copy()
    coefs_out.rename(columns={'s0': 'coefs'}, inplace=True)

    decomp_out_agg = coefs_out.copy()
    decomp_out_agg['xDecompAgg'] = x_decomp_out_agg.values
    decomp_out_agg['xDecompPerc'] = x_decomp_out_agg_perc.values
    decomp_out_agg['xDecompMeanNon0'] = x_decomp_out_agg_mean_non0.values
    decomp_out_agg['xDecompMeanNon0Perc'] = x_decomp_out_agg_mean_non0_perc.values
    decomp_out_agg['xDecompAggRF'] = x_decomp_out_agg_rf.values
    decomp_out_agg['xDecompPercRF'] = x_decomp_out_agg_perc_rf.values
    decomp_out_agg['xDecompMeanNon0RF'] = x_decomp_out_agg_mean_non0_rf.values
    decomp_out_agg['xDecompMeanNon0PercRF'] = x_decomp_out_agg_mean_non0_perc_rf.values
    decomp_out_agg['pos'] = x_decomp_out_agg >= 0
    decomp_collect = {
        'xDecompVec': x_decomp_out,
        'xDecompVec.scaled': x_decomp_out_scaled,
        'xDecompAgg': decomp_out_agg,
        'coefsOutCat': coefs_out_cat,
        'mediaDecompImmediate': media_decomp_immediate.assign(ds=x_decomp_out['ds'], y=x_decomp_out['y']),
        'mediaDecompCarryover': media_decomp_carryover.assign(ds=x_decomp_out['ds'], y=x_decomp_out['y'])
    }
    return decomp_collect

def model_refit(x_train, y_train, x_val, y_val, x_test, y_test, lambda_, lower_limits, upper_limits, intercept=True, intercept_sign="non_negative", penalty_factor=None, *args, **kwargs):

    ridge = Ridge(alpha=lambda_, fit_intercept=intercept, random_state=42)
    ridge.fit(x_train, y_train)

    if intercept_sign == "non_negative " and ridge.intercept_ < 0:
        ridge = Ridge(alpha=lambda_, fit_intercept=False, random_state=42)
        ridge.fit(x_train, y_train)
        intercept = False

    coefs = np.append(ridge.intercept_, ridge.coef_) if intercept else ridge.coef_
    feature_names = ['Intercept'] + list(x_train.columns) if intercept else x_train.columns

    coefs_df = pd.DataFrame(coefs, index=feature_names, columns=['s0'])

    def get_adjusted_r2(y_true, y_pred, n_features, n_samples):
        r2 = r2_score(y_true, y_pred)
        adjusted_r2 = 1 - (1-r2)*(n_samples-1)/(n_samples-n_features-1)
        return adjusted_r2

    def get_nrmse(y_true, y_pred):
        y_true = np.array(y_true)  # Convert y_true to numpy array
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        nrmse = rmse / (y_true.max() - y_true.min())  # Now you can use .max() and .min()
        return nrmse

    y_train_pred = ridge.predict(x_train)
    n_samples, n_features = x_train.shape
    adjusted_r2_train = get_adjusted_r2(y_train, y_train_pred, n_features, n_samples)
    nrmse_train = get_nrmse(y_train, y_train_pred)

    results = {'train': (y_train_pred, adjusted_r2_train, nrmse_train)}

    for split, x, y in (('val', x_val, y_val), ('test', x_test, y_test)):
        if x is not None:
            y_pred = ridge.predict(x)
            adjusted_r2 = get_adjusted_r2(y, y_pred, x.shape[1], len(y))
            nrmse = get_nrmse(y, y_pred)
            results[split] = (y_pred, adjusted_r2, nrmse)
        else:
            results[split] = (None, None, None)

    mod_out = {
        "rsq_train": results['train'][1],
        "rsq_val": results['val'][1],
        "rsq_test": results['test'][1],
        "nrmse_train": results['train'][2],
        "nrmse_val": results['val'][2],
        "nrmse_test": results['test'][2],
        "coefs": coefs_df,
        "y_train_pred": results['train'][0],
        "y_val_pred": results['val'][0],
        "y_test_pred": results['test'][0],
        "y_pred": np.concatenate([y for y, _, _ in results.values() if y is not None]),
        "mod": ridge,
        "df_int": intercept
    }

    return mod_out

# def model_refit(x_train, y_train, x_val, y_val, x_test, y_test, lambda_, lower_limits, upper_limits, intercept=True, intercept_sign="non_negative", penalty_factor=None, *args, **kwargs):

#     ridge = Ridge(alpha=lambda_, fit_intercept=intercept, random_state=42)
#     ridge.fit(x_train, y_train)

#     if ridge.fit_intercept:
#         coefs = np.append(ridge.intercept_, ridge.coef_)
#         feature_names = ['Intercept'] + list(x_train.columns)
#     else:
#         coefs = ridge.coef_
#         feature_names = x_train.columns

#     coefs_df = pd.DataFrame(coefs, index=feature_names, columns=['s0'])

#     def get_adjusted_r2(y_true, y_pred, n_features, n_samples):
#         r2 = r2_score(y_true, y_pred)
#         adjusted_r2 = 1 - (1-r2)*(n_samples-1)/(n_samples-n_features-1)
#         return adjusted_r2

#     def get_nrmse(y_true, y_pred):
#         y_true = np.array(y_true)
#         mse = mean_squared_error(y_true, y_pred)
#         rmse = np.sqrt(mse)
#         nrmse = rmse / (y_true.max() - y_true.min())
#         return nrmse

#     if 'intercept_sign' in locals() or 'intercept_sign' in globals():
#         if intercept_sign == "non_negative" and ridge.intercept_ < 0:
#             ridge_no_intercept = Ridge(alpha=0.1, fit_intercept=False, random_state=42)
#             ridge_no_intercept.fit(x_train, y_train)
#             ridge = ridge_no_intercept

#     y_train_pred = ridge.predict(x_train)
#     n_samples, n_features = x_train.shape
#     adjusted_r2_train = get_adjusted_r2(y_train, y_train_pred, n_features, n_samples)
#     nrmse_train = get_nrmse(y_train, y_train_pred)

#     if x_val is not None:
#         y_val_pred = ridge.predict(x_val)
#         adjusted_r2_val = get_adjusted_r2(y_val, y_val_pred, x_val.shape[1], len(y_val))
#         nrmse_val = get_nrmse(y_val, y_val_pred)
#     else:
#         adjusted_r2_val = nrmse_val = None

#     if x_test is not None:
#         y_test_pred = ridge.predict(x_test)
#         adjusted_r2_test = get_adjusted_r2(y_test, y_test_pred, x_test.shape[1], len(y_test))
#         nrmse_test = get_nrmse(y_test, y_test_pred)
#     else:
#         adjusted_r2_test = nrmse_test = None

#     if x_val is not None and x_test is not None:
#         y_pred = np.concatenate([y_train_pred, y_val_pred, y_test_pred])
#     elif x_val is not None:
#         y_pred = np.concatenate([y_train_pred, y_val_pred])
#     elif x_test is not None:
#         y_pred = np.concatenate([y_train_pred, y_test_pred])
#     else:
#         y_pred = y_train_pred

#     mod_out = {
#         "rsq_train": adjusted_r2_train,
#         "rsq_val": adjusted_r2_val,
#         "rsq_test": adjusted_r2_test,
#         "nrmse_train": nrmse_train,
#         "nrmse_val": nrmse_val,
#         "nrmse_test": nrmse_test,
#         #"coefs": lasso.coef_.reshape(-1, 1),
#         "coefs": coefs_df,
#         "y_train_pred": y_train_pred,
#         "y_val_pred": y_val_pred,
#         "y_test_pred": y_test_pred,
#         "y_pred": y_pred,
#         "mod": ridge,
#         "df_int": intercept
#     }

#     return mod_out


def get_rsq(true, predicted, p, df_int, n_train=None):
    sse = np.sum((true - predicted) ** 2)
    tss = np.sum((true - np.mean(true)) ** 2)

    if n_train is None:
        n = len(true)
    else:
        n = n_train

    rsq = 1 - (sse / (n - df_int - p)) / (tss / (n - 1))
    return rsq


def lambda_seq(x, y, seq_len=100, lambda_min_ratio=0.0001):
    def mysd(y):
        return np.sqrt(np.sum((y - np.mean(y)) ** 2) / len(y))

    # Standardize the features
    scaler = StandardScaler()
    sx = scaler.fit_transform(x)

    # Check for NaN columns and replace with zeros
    check_nan = np.all(np.isnan(sx), axis=0)
    for i, is_nan in enumerate(check_nan):
        if is_nan:
            sx[:, i] = 0

    # Standardize the target variable (assuming it's already centered)
    sy = y

    # Calculate lambda_max
    lambda_max = np.max(np.abs(np.sum(sx * sy, axis=0))) / (0.001 * x.shape[0])

    # Create a logarithmic sequence of lambdas
    lambda_max_log = np.log(lambda_max)
    log_step = (lambda_max_log - np.log(lambda_max * lambda_min_ratio)) / (seq_len - 1)
    log_seq = np.linspace(lambda_max_log, np.log(lambda_max * lambda_min_ratio), num=seq_len)
    lambdas = np.exp(log_seq)

    return lambdas


def hyper_collector(InputCollect, hyper_in, ts_validation, add_penalty_factor, cores, dt_hyper_fixed=None):
    # Fetch hyper-parameters based on media
    hypParamSamName = hyper_names(adstock=InputCollect['adstock'], all_media=InputCollect['all_media'])

    # Manually add other hyper-parameters
    ##hypParamSamName.extend(HYPS_OTHERS)
    hypParamSamName += HYPS_OTHERS

    # Add penalty factor hyper-parameters names
    for_penalty = [col for col in InputCollect['dt_mod'].columns.values if col not in ['ds', 'dep_var']]
    if add_penalty_factor:
        ##for_penalty = InputCollect['dt_mod'].drop(columns=['ds', 'dep_var']).columns.tolist()
        penalty_names = ['penalty_' + name for name in for_penalty]
        hypParamSamName += penalty_names

    # Check hyper_fixed condition + add lambda + penalty factor hyper-parameters names
    all_fixed = check_hyper_fixed(InputCollect, dt_hyper_fixed, add_penalty_factor)
    hypParamSamName = all_fixed['hyp_param_sam_name']
    if not all_fixed['hyper_fixed']:
        hyper_bound_list = {}
        for param_name in hypParamSamName:
            if param_name in hyper_in.keys(): ## Added since R automatically creates an empty list don't raise an error
                hyper_bound_list[param_name] = hyper_in[param_name]
            else:
                hyper_bound_list[param_name] = list()

        # Add unfixed lambda hyperparameter manually
        ##if len(hyper_bound_list.get("lambda", [])) != 1:
        if len(hyper_bound_list["lambda"]) != 1:
            hyper_bound_list["lambda"] = [0, 1]

        # Add unfixed train_size hyperparameter manually
        if ts_validation:
            if "train_size" not in hyper_bound_list.keys():
                hyper_bound_list["train_size"] = [0.5, 0.8]
            print(f"Time-series validation with train_size range of {hyper_bound_list['train_size'][0]*100}% - {hyper_bound_list['train_size'][1]*100}% of the data...")
        else:
            if "train_size" in hyper_bound_list.keys():
                print("Warning: Provided train_size but ts_validation = FALSE. Time series validation inactive.")

            hyper_bound_list["train_size"] = [1]
            print("Fitting time series with all available data...")

        # Add unfixed penalty.factor hyperparameters manually
        ## for_penalty = InputCollect['dt_mod'].drop(columns=['ds', 'dep_var']).columns.tolist()
        penalty_names = [name + "_penalty" for name in for_penalty]
        if add_penalty_factor:
            for penalty in penalty_names:
                ##if len(hyper_bound_list.get(penalty, [])) != 1:
                if len(hyper_bound_list[penalty]) != 1:
                    hyper_bound_list[penalty] = [0, 1]

        # Get hyperparameters for Nevergrad
        ## hyper_bound_list_updated = {k: v for k, v in hyper_bound_list.items() if len(v) == 2}
        hyper_bound_list_updated = dict()
        for key, val in hyper_bound_list.items():
            if len(val) == 2:
                hyper_bound_list_updated[key] = val

        # Get fixed hyperparameters
        ##hyper_bound_list_fixed = {k: v for k, v in hyper_bound_list.items() if len(v) == 1}
        hyper_bound_list_fixed = dict()
        for key, val in hyper_bound_list.items():
            if len(val) == 1:
                hyper_bound_list_fixed[key] = val

        # Combine updated and fixed hyperparameters
        hyper_list_bind = {**hyper_bound_list_updated, **hyper_bound_list_fixed}
        hyper_list_all = dict() ##{}
        for param_name in hypParamSamName:
            if param_name in hyper_list_bind.keys():
                hyper_list_all[param_name] = hyper_list_bind[param_name]
            else:
                hyper_list_all[param_name] = []

        # Prepare a DataFrame for fixed hyperparameters
        ##dt_hyper_fixed_mod = pd.DataFrame({k: [v[0]] * cores for k, v in hyper_bound_list_fixed.items()})
        dt_hyper_fixed_mod = pd.DataFrame(hyper_bound_list_fixed.items())

    else:
        # Initialize hyper_bound_list_fixed
        hyper_bound_list_fixed = dict()
        for param_name in hypParamSamName:
            if param_name in dt_hyper_fixed.columns.values:
                hyper_bound_list_fixed[param_name] = dt_hyper_fixed[param_name].values.to_list()
            else:
                hyper_bound_list_fixed[param_name] = list()
            ##hyper_bound_list_fixed[param_name] = dt_hyper_fixed.get(param_name, [])

        # Update hyper_list_all and hyper_bound_list_updated
        hyper_list_all = hyper_bound_list_fixed ##.copy()
        ##hyper_bound_list_updated = {k: v for k, v in hyper_bound_list_fixed.items() if len(v) == 2}
        hyper_bound_list_updated = dict()
        for key, val in hyper_bound_list.items():
            if len(val) == 2:
                hyper_bound_list_updated[key] = val

        # Set cores to 1
        cores = 1

        # Prepare a DataFrame for fixed hyperparameters
        ## pd.DataFrame({k: v for k, v in hyper_bound_list_fixed.items()}, index=[0])
        dt_hyper_fixed_mod = dt_hyper_fixed_mod = pd.DataFrame(hyper_bound_list_fixed.items())

    return {
        "hyper_list_all": hyper_list_all,
        "hyper_bound_list_updated": hyper_bound_list_updated,
        "hyper_bound_list_fixed": hyper_bound_list_fixed,
        "dt_hyper_fixed_mod": dt_hyper_fixed_mod,
        "all_fixed": all_fixed
    }


def init_msgs_run(InputCollect, refresh, quiet=False, lambda_control=None):
    if lambda_control is not None:
        logging.info("Input 'lambda_control' deprecated in v3.6.0; lambda is now selected by hyperparameter optimization")

    if not quiet:
        # First message
        logging.info(f"Input data has {len(InputCollect['dt_mod'])} {InputCollect['intervalType']}s in total: {InputCollect['dt_mod']['ds'].min()} to {InputCollect['dt_mod']['ds'].max()}")

        # Calculate depth
        if 'refreshDepth' in InputCollect:
            depth = InputCollect['refreshDepth']
        elif 'refreshCounter' in InputCollect:
            depth = InputCollect['refreshCounter']
        else:
            depth = 0

        # Update refresh
        refresh = int(depth) > 0

        # Second message
        model_type = "Initial" if not refresh else f"Refresh #{depth}"
        logging.info(f"{model_type} model is built on rolling window of {InputCollect['rollingWindowLength']} {InputCollect['intervalType']}: {InputCollect['window_start']} to {InputCollect['window_end']}")

    if refresh:
        logging.info(f"Rolling window moving forward: {InputCollect['refresh_steps']} {InputCollect['intervalType']}s")

================
File: outputs.py
================
# Copyright (c) Meta Platforms, Inc. and its affiliates.

    # This source code is licensed under the MIT license found in the
    # LICENSE file in the root directory of this source tree.

    ####################################################################
    import os
    import time
    import pandas as pd
    import re
    from datetime import datetime

    from .json import robyn_write
    from .cluster import robyn_clusters
    from .checks import check_dir, check_calibconstr
    from .pareto import robyn_pareto

    def robyn_outputs(input_collect,
                    output_models,
                    pareto_fronts="auto",
                    calibration_constraint=0.1,
                    plot_folder=None,
                    plot_folder_sub=None,
                    plot_pareto=True,
                    csv_out="pareto",
                    clusters=True,
                    select_model="clusters",
                    ui=False,
                    export=True,
                    all_sol_json=False,
                    quiet=False,
                    refresh=False):
        """
        Runs the Robyn Pareto algorithm on the given output models and collects the results.

        Args:
            input_collect (dict): The input collection.
            output_models (object): The output models.
            pareto_fronts (str, optional): The number of Pareto fronts to calculate. Defaults to "auto".
            calibration_constraint (float, optional): The calibration constraint. Defaults to 0.1.
            plot_folder (str, optional): The folder to save the plots. Defaults to None.
            plot_folder_sub (str, optional): The subfolder within the plot folder. Defaults to None.
            plot_pareto (bool, optional): Whether to plot the Pareto fronts. Defaults to True.
            csv_out (str, optional): The type of CSV output. Defaults to "pareto".
            clusters (bool, optional): Whether to calculate clusters for model selection. Defaults to True.
            select_model (str, optional): The model selection method. Defaults to "clusters".
            ui (bool, optional): Whether to enable the user interface. Defaults to False.
            export (bool, optional): Whether to export the results. Defaults to True.
            all_sol_json (bool, optional): Whether to export all solutions as JSON. Defaults to False.
            quiet (bool, optional): Whether to suppress console output. Defaults to False.
            refresh (bool, optional): Whether to refresh the results. Defaults to False.

        Returns:
            dict: The collected output results.
        """

        t0 = time.time()

        if plot_folder is None:
            plot_folder = os.getcwd()

        plot_folder = check_dir(plot_folder)

        # Check calibration constrains
        calibrated = 'calibration_input' in input_collect and input_collect['calibration_input'] is not None
        all_fixed = len(output_models["trials"][0]["hyperBoundFixed"]) == len(output_models["hyper_updated"])
        if not all_fixed:
            calibration_constraint = check_calibconstr(calibration_constraint, output_models["metadata"]["iterations"], output_models["metadata"]["trials"], input_collect["robyn_inputs"]["calibration_input"], refresh=refresh)

        #####################################
        #### Run robyn_pareto on OutputModels

        total_models = output_models["metadata"]["iterations"] * output_models["metadata"]["trials"]
        if not isinstance(output_models["metadata"]["hyper_fixed"], bool):
            print(f"Running Pareto calculations for {total_models} models on {pareto_fronts} fronts...")

        pareto_results = robyn_pareto(input_collect, output_models, pareto_fronts=pareto_fronts, calibration_constraint=calibration_constraint, quiet=quiet, calibrated=calibrated, refresh=refresh)
        pareto_fronts = pareto_results["pareto_fronts"]
        all_solutions = pareto_results["pareto_solutions"]

        #####################################
        #### Gather the results into output object

        all_pareto = {
            "resultHypParam": pareto_results["resultHypParam"],
            "xDecompAgg": pareto_results["xDecompAgg"],
            "resultCalibration": pareto_results["resultCalibration"],
            "plotDataCollect": pareto_results["plotDataCollect"],
            "df_caov_pct": pareto_results["df_caov_pct_all"]
        }

        # Set folder to save outputs
        depth = 0 if "refreshDepth" not in input_collect["robyn_inputs"] else input_collect["robyn_inputs"]["refreshDepth"]
        folder_var = "init" if not int(depth) > 0 else "rf" + str(depth)

        if plot_folder_sub is None:
            plot_folder_sub = "Robyn_" + datetime.now().strftime("%Y%m%d%H%M") + "_" + folder_var

        plot_folder = re.sub("//+", "/", f"{plot_folder}/{plot_folder_sub}/")

        if not os.path.exists(plot_folder) and export:
            print(f"Creating directory for outputs: {plot_folder}")
            os.makedirs(plot_folder)

        # Final results object
        OutputCollect = {
            'resultHypParam': pareto_results['resultHypParam'][pareto_results['resultHypParam']['solID'].isin(all_solutions)],
            'xDecompAgg': pareto_results['xDecompAgg'][pareto_results['xDecompAgg']['solID'].isin(all_solutions)],

            #"xDecompAgg": pareto_results["xDecompAgg"].loc[pareto_results["solID"].isin(allSolutions)],
            "mediaVecCollect": pareto_results["mediaVecCollect"],
            "xDecompVecCollect": pareto_results["xDecompVecCollect"],
            #"resultCalibration": None if not calibrated else pareto_results["resultCalibration"].loc[pareto_results["solID"].isin(allSolutions)],
            "resultCalibration": pareto_results["resultCalibration"][pareto_results["resultCalibration"]["solID"].isin(all_solutions)] if calibrated else None,
            "allSolutions": all_solutions,
            "allPareto": all_pareto,
            "calibration_constraint": calibration_constraint,
            "OutputModels": output_models,
            "cores": output_models["metadata"]["cores"],
            "iterations": output_models["metadata"]["iterations"],
            "trials": output_models["trials"],
            "intercept_sign": output_models["metadata"]["intercept_sign"],
            "nevergrad_algo": output_models["metadata"]["nevergrad_algo"],
            "add_penalty_factor": output_models["metadata"]["add_penalty_factor"],
            "seed": output_models["seed"],
            "UI": None,
            "pareto_fronts": pareto_fronts,
            'hyper_fixed': output_models["metadata"]['hyper_fixed'],
            "plot_folder": plot_folder
        }
        OutputCollect.keys()

        # Cluster results and amend cluster output
        if clusters:
            if not quiet:
                print(">>> Calculating clusters for model selection using Pareto fronts...")
            clusterCollect = robyn_clusters(
                OutputCollect,
                dep_var_type=input_collect["robyn_inputs"]["dep_var_type"],
                quiet=quiet,
                export=export
            )
            OutputCollect["resultHypParam"] = pd.merge(
                OutputCollect["resultHypParam"],
                clusterCollect["data"].loc[clusterCollect["data"]["solID"].isin(all_solutions)],
                on="solID"
            )
            OutputCollect["xDecompAgg"] = pd.merge(
                OutputCollect["xDecompAgg"],
                clusterCollect["data"].loc[clusterCollect["data"]["solID"].isin(all_solutions)],
                on="solID"
            )
            OutputCollect["mediaVecCollect"] = pd.merge(
                OutputCollect["mediaVecCollect"],
                clusterCollect["data"].loc[clusterCollect["data"]["solID"].isin(all_solutions)],
                on="solID"
            )
            OutputCollect["xDecompVecCollect"] = pd.merge(
                OutputCollect["xDecompVecCollect"],
                clusterCollect["data"].loc[clusterCollect["data"]["solID"].isin(all_solutions)],
                on="solID"
            )
            if calibrated:
                OutputCollect["resultCalibration"] = pd.merge(
                    OutputCollect["resultCalibration"],
                    clusterCollect["data"].loc[clusterCollect["data"]["solID"].isin(all_solutions)],
                    on="solID"
                )
            OutputCollect["clusters"] = clusterCollect

        # TODO Add export code to enable plotting
        # if export:
        #     try:
        #         message(">>> Collecting {} pareto-optimum results into: {}".format(len(all_solutions), plot_folder))
        #         all_plots = robyn_plots(input_collect, output_collect, export=export, quiet=quiet)
        #         message(">> Exporting general plots into directory...")
        #         if csv_out in ["all", "pareto"]:
        #             message(">> Exporting {} results as CSVs into directory...".format(csv_out))
        #             robyn_csv(input_collect, output_collect, csv_out, export=export, calibrated=calibrated)
        #         if plot_pareto:
        #             message(">>> Exporting pareto one-pagers into directory...")
        #             select_model = select_model if not clusters or output_collect["clusters"] is None else None
        #             pareto_onepagers = robyn_onepagers(input_collect, output_collect, select_model=select_model, quiet=quiet, export=export)
        #         if all_sol_json:
        #             pareto_df = output_collect["resultHypParam"].filter(pandas.notnull(pandas.Series(["cluster"]))).select(["solID", "cluster", "top_sol"]).sort_values(by=["cluster", "top_sol"], ascending=False).drop(columns=["solID"])
        #         else:
        #             pareto_df = None
        #         ##attr(output_collect, "runTime") = round(difftime(sys.time(), t0, units="mins"), 2)
        #         output_collect["runTime"] = round(difftime(sys.time(), t0, units="mins"), 2)
        #         robyn_write(input_collect, output_collect, dir=plot_folder, quiet=quiet, pareto_df=pareto_df, export=export)
        #         if ui and plot_pareto:
        #             output_collect["UI"] = {"pareto_onepagers": pareto_onepagers}
        #         output_collect["UI"] = output_collect.get("UI", pandas.DataFrame()) if ui else None
        #     except Exception as e:
        #         message("Failed exporting results, but returned model results anyways: {}".format(e))
        ##if not is.null(output_models["hyper_updated"]):
        if output_models["hyper_updated"] is not None:
            OutputCollect["hyper_updated"] = output_models["hyper_updated"]
        ##attr(output_collect, "runTime") = round(difftime(sys.time(), t0, units="mins"), 2)
        # OutputCollect["runTime"] = round(difftime(sys.time(), t0, units="mins"), 2)
        ##class(output_collect) = ["robyn_outputs", class(output_collect)]
        ##??output_collect["robyn_outputs"] = output_collect
        ##return(invisible(output_collect))
        return OutputCollect

    def print_robyn_outputs(x, *args, **kwargs):
        """
        Print various outputs related to Robyn.

        Parameters:
        - x: Robyn object
        - *args: Additional positional arguments
        - **kwargs: Additional keyword arguments
        """
        print("Plot Folder: {x.plot_folder}")
        print("Calibration Constraint: {x.calibration_constraint}")
        print("Hyper-parameters fixed: {x.hyper_fixed}")
        print("Pareto-front ({x.pareto_fronts}) All solutions ({len(x.allSolutions)}): {', '.join(x.allSolutions)}")
        if "clusters" in x.keys():
            print("Clusters (k = {x.clusters.n_clusters}): {', '.join(x.clusters.models.solID)}")
        else:
            print("")

    def robyn_csv(input_collect, output_collect, csv_out=None, export=True, calibrated=False):
        """
        Export data from Robyn outputs to CSV files.

        Args:
            input_collect (InputCollect): The input collection object.
            output_collect (robyn_outputs): The output collection object.
            csv_out (str or None, optional): The type of CSV files to export. Defaults to None.
            export (bool, optional): Whether to export the data to CSV files. Defaults to True.
            calibrated (bool, optional): Whether the data is calibrated. Defaults to False.
        """
        if export:
            # Check that OutputCollect has the correct class
            assert isinstance(output_collect, robyn_outputs)

            # Get the temp all dataframe
            temp_all = output_collect.allPareto

            # Get the plot folder
            plot_folder = output_collect.plot_folder

            # Write the pareto hyperparameters and aggregated data to CSV
            if "pareto" in csv_out:
                pd.write_csv(output_collect.resultHypParam, os.path.join(plot_folder, "pareto_hyperparameters.csv"))
                pd.write_csv(output_collect.xDecompAgg, os.path.join(plot_folder, "pareto_aggregated.csv"))
                if calibrated:
                    pd.write_csv(output_collect.resultCalibration, os.path.join(plot_folder, "pareto_calibration.csv"))

            # Write the all hyperparameters and aggregated data to CSV
            if "all" in csv_out:
                pd.write_csv(temp_all.resultHypParam, os.path.join(plot_folder, "all_hyperparameters.csv"))
                pd.write_csv(temp_all.xDecompAgg, os.path.join(plot_folder, "all_aggregated.csv"))
                if calibrated:
                    pd.write_csv(temp_all.resultCalibration, os.path.join(plot_folder, "all_calibration.csv"))

            # Write the raw data and transformation matrices to CSV
            ## if not is.null(csv_out):
            if csv_out is not None:
                pd.write_csv(input_collect.dt_input, os.path.join(plot_folder, "raw_data.csv"))
                pd.write_csv(output_collect.mediaVecCollect, os.path.join(plot_folder, "pareto_media_transform_matrix.csv"))
                pd.write_csv(output_collect.xDecompVecCollect, os.path.join(plot_folder, "pareto_alldecomp_matrix.csv"))

================
File: pareto.py
================
# Copyright (c) Meta Platforms, Inc. and its affiliates.

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

####################################################################
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm

from .allocator import fx_objective, get_hill_params
from .cluster import errors_scores
from .response import robyn_response
from .transformation import adstock_weibull, saturation_hill, transform_adstock, run_transformations
#from .model import model_decomp


def robyn_pareto(InputCollect, OutputModels, pareto_fronts="auto", min_candidates=100, calibration_constraint=0.1, quiet=False, calibrated=False, **kwargs):
    hyper_fixed = OutputModels["metadata"]["hyper_fixed"]
    OutModels = [trial for trial in OutputModels["trials"] if "resultCollect" in trial]
    df_list = [item['resultCollect']['resultHypParam'].assign(trial=item['trial']) for item in OutModels]

    resultHypParam = pd.concat(df_list, ignore_index=True)

    # xDecompAgg = pd.concat([x.resultCollect.xDecompAgg.assign(trial=x.trial) for x in OutModels])
    xDecompAgg = pd.concat([
        x['resultCollect']['xDecompAgg'].assign(trial=x['trial'])
        for x in OutModels
    ])

    if calibrated:
        resultCalibration = pd.concat([x['resultCollect']['liftCalibration'].assign(trial=x['trial']).rename(columns={"liftMedia": "rn"}) for x in OutModels])
    else:
        resultCalibration = None

    if not hyper_fixed:
        df_names = ["resultHypParam", "xDecompAgg"] if not calibrated else ["resultHypParam", "xDecompAgg", "resultCalibration"]

        for df_name in df_names:
            concatenated_dfs = pd.concat([
                x["resultCollect"][df_name].assign(iterations=(x["resultCollect"]["resultHypParam"]['iterNG'] - 1) * OutputModels['metadata']['cores'] + x["resultCollect"]["resultHypParam"]['iterPar']) for x in OutModels
            ], ignore_index=True)
            globals()[df_name] = concatenated_dfs

    elif hyper_fixed and calibrated:
        df_names = ["resultCalibration"]

        for df_name in df_names:
            concatenated_dfs = pd.concat([
                x["resultCollect"][df_name].assign(iterations=(x["resultCollect"]["resultHypParam"]['iterNG'] - 1) * OutputModels['metadata']['cores'] + x["resultCollect"]["resultHypParam"]['iterPar'])
                for x in OutModels
            ], ignore_index=True)

            globals()[df_name] = concatenated_dfs

    # If recreated model, inherit bootstrap results
    if len(xDecompAgg.solID.unique()) == 1 and "boot_mean" not in xDecompAgg.columns:
        bootstrap = OutputModels.bootstrap
        if bootstrap is not None:
            xDecompAgg = xDecompAgg.merge(bootstrap, on=["rn", "variable"])

    xDecompAggCoef0 = xDecompAgg.loc[
        xDecompAgg.rn.isin(InputCollect["robyn_inputs"]["paid_media_spends"])
    ].groupby("solID").agg({"coefs": lambda x: (x.min() == 0).any()})

    if not hyper_fixed:
        mape_lift_quantile10 = np.quantile(resultHypParam["mape"], calibration_constraint, overwrite_input=True)
        nrmse_quantile90 = np.quantile(resultHypParam["nrmse"], 0.90, overwrite_input=True)
        decomprssd_quantile90 = np.quantile(resultHypParam["decomp.rssd"], 0.90, overwrite_input=True)

        resultHypParam = pd.merge(resultHypParam, xDecompAggCoef0, on="solID")
        resultHypParam["mape.qt10"] = (resultHypParam["mape"] <= mape_lift_quantile10) & (resultHypParam["nrmse"] <= nrmse_quantile90) & (resultHypParam["decomp.rssd"] <= decomprssd_quantile90)

        resultHypParamPareto = resultHypParam[resultHypParam["mape.qt10"] == True]
        fronts = float('inf') if 'auto' in pareto_fronts else pareto_fronts

        paretoResults = pareto_front(resultHypParamPareto["nrmse"], resultHypParamPareto["decomp.rssd"], fronts=fronts, sort=False)

        # resultHypParamPareto = pd.merge(resultHypParamPareto, paretoResults, on=["nrmse", "decomp.rssd"])
        resultHypParamPareto = pd.merge(resultHypParamPareto, paretoResults,
                                left_on=["nrmse", "decomp.rssd"],
                                right_on=["x", "y"],
                                how="left")
        resultHypParamPareto = resultHypParamPareto.rename(columns={"pareto_front": "robynPareto"})
        resultHypParamPareto = resultHypParamPareto.sort_values(["iterNG", "iterPar", "nrmse"]).reset_index(drop=True)
        resultHypParamPareto = resultHypParamPareto[["solID", "robynPareto"]]
        resultHypParamPareto = resultHypParamPareto.groupby("solID", as_index=False).first()



        resultHypParam = pd.merge(resultHypParam, resultHypParamPareto, on="solID")

    else:
        resultHypParam["mape.qt10"] = True
        resultHypParam["robynPareto"] = 1
        resultHypParam["coef0"] = np.nan

    resultHypParam["error_score"] = errors_scores(resultHypParam, ts_validation=OutputModels["metadata"]["ts_validation"], **kwargs)

    xDecompAgg = pd.merge(xDecompAgg, resultHypParam.filter(['robynPareto','solID'], axis=1), on='solID')
    # decompSpendDist = pd.concat([pd.DataFrame({'decompSpendDist': x["resultCollect"]["decompSpendDist"], 'trial': x["trial"]}) for x in OutModels])
    # decompSpendDist = pd.concat([
    #     pd.DataFrame({
    #         'decompSpendDist': [x["resultCollect"]["decompSpendDist"]],
    #         'trial': [x["trial"]]
    #     }) for x in OutModels
    # ])
    decompSpendDistList = []
    for x in OutModels:
        # Assuming x["resultCollect"]["decompSpendDist"] is already a DataFrame
        df_temp = x["resultCollect"]["decompSpendDist"].copy()
        df_temp['trial'] = x["trial"]  # Add 'trial' as a new column to each DataFrame
        decompSpendDistList.append(df_temp)

    # Concatenate all the DataFrames to stack them vertically
    decompSpendDist = pd.concat(decompSpendDistList, ignore_index=True)

    if not hyper_fixed:
        decompSpendDist['solID'] = decompSpendDist['trial'].astype(str) + '_' + \
                               decompSpendDist['iterNG'].astype(str) + '_' + \
                               decompSpendDist['iterPar'].astype(str)

    decompSpendDist = pd.merge(decompSpendDist, resultHypParam[['robynPareto', 'solID']], on='solID', how='left')

    if True:
        # TODO: Handle parallel execution
        # if OutputModels["metadata"]["cores"] > 1:
        #     from concurrent.futures import ProcessPoolExecutor
        #     with ProcessPoolExecutor(max_workers=OutputModels["metadata"]["cores"]) as executor:
        #         pareto_fronts = list(executor.map(get_pareto_fronts, pareto_fronts))
        # else:
        #     pareto_fronts = get_pareto_fronts(pareto_fronts)

        if hyper_fixed:
            pareto_fronts = 1

        if pareto_fronts == 'auto':
            n_pareto = resultHypParam[~resultHypParam.robynPareto.isna()].shape[0]
            if n_pareto <= min_candidates and resultHypParam.shape[0] > 1 and not calibrated:
                raise ValueError(f"Less than {min_candidates} candidates in pareto fronts. Increase iterations to get more model candidates or decrease min_candidates in robyn_output()")
            # auto_pareto = resultHypParam[~resultHypParam.robynPareto.isna()].groupby('robynPareto').agg({'solID': 'nunique'}).assign(n_cum=lambda df: df['solID'].cumsum()).query(f'n_cum >= {min_candidates}').iloc[[0]]
            auto_pareto = resultHypParam[~resultHypParam['robynPareto'].isna()] \
                .groupby('robynPareto', as_index=False) \
                .agg({'solID': 'nunique'}) \
                .rename(columns={'solID': 'n'}) \
                .assign(n_cum=lambda df: df['n'].cumsum()) \
                .query(f'n_cum >= {min_candidates}') \
                .iloc[0]
            print(auto_pareto)
            # print(f">> Automatically selected {auto_pareto.robynPareto.values[0]} Pareto-fronts to contain at least {min_candidates} pareto-optimal models ({auto_pareto.solID.values[0]})")
            print(f">> Automatically selected {auto_pareto['robynPareto']} Pareto-fronts to contain at least {min_candidates} pareto-optimal models ({auto_pareto['n_cum']})")

            pareto_fronts = int(auto_pareto['robynPareto'])


        pareto_fronts_vec = np.arange(1, pareto_fronts+1)

        decompSpendDistPar = decompSpendDist[decompSpendDist['robynPareto'].isin(pareto_fronts_vec)]
        resultHypParamPar = resultHypParam[resultHypParam['robynPareto'].isin(pareto_fronts_vec)]
        xDecompAggPar = xDecompAgg[xDecompAgg['robynPareto'].isin(pareto_fronts_vec)]
        respN = None

    if not quiet:
        print(f">>> Calculating response curves for all models' media variables ({decompSpendDistPar.shape[0]})...")

    if OutputModels["metadata"]["cores"] > 1:
        resp_collect = pd.concat(
            [run_dt_resp(respN, InputCollect, OutputModels, decompSpendDistPar, resultHypParamPar, xDecompAggPar, **kwargs) for respN in range(len(decompSpendDistPar["rn"]))]
        )
    else:
        resp_collect = pd.concat(
            [run_dt_resp(respN, InputCollect, OutputModels, decompSpendDistPar, resultHypParamPar, xDecompAggPar, **kwargs) for respN in range(len(decompSpendDistPar["rn"]))]
        )

    decompSpendDist = pd.merge(
        decompSpendDist,
        resp_collect,
        on=["solID", "rn"],
        how='left'
    )
    decompSpendDist["roi_mean"] = decompSpendDist["mean_response"] / decompSpendDist["mean_spend"]
    decompSpendDist["roi_total"] = decompSpendDist["xDecompAgg"] / decompSpendDist["total_spend"]
    decompSpendDist["cpa_mean"] = decompSpendDist["mean_spend"] / decompSpendDist["mean_response"]
    decompSpendDist["cpa_total"] = decompSpendDist["total_spend"] / decompSpendDist["xDecompAgg"]

    xDecompAgg = pd.merge(
        xDecompAgg,
        decompSpendDist[["rn", "solID", "total_spend", "mean_spend", "mean_spend_adstocked", "mean_carryover", "mean_response", "spend_share", "effect_share", "roi_mean", "roi_total", "cpa_total"]],
        on=["solID", "rn"],
        how='left'
    )

    # mediaVecCollect = []
    mediaVecCollect = pd.DataFrame()
    xDecompVecCollect = []
    plotDataCollect = {}
    df_caov_pct_all = pd.DataFrame()
    dt_mod = InputCollect["robyn_inputs"]["dt_mod"]
    dt_modRollWind = InputCollect["robyn_inputs"]["dt_modRollWind"]
    rw_start_loc = InputCollect["robyn_inputs"]["rollingWindowStartWhich"]
    rw_end_loc = InputCollect["robyn_inputs"]["rollingWindowEndWhich"]

    for pf in pareto_fronts_vec:
        plotMediaShare = xDecompAgg[
            (xDecompAgg["robynPareto"] == pf) & (xDecompAgg["rn"].isin(InputCollect["robyn_inputs"]["paid_media_spends"]))
        ]
        uniqueSol = plotMediaShare["solID"].unique()
        plotWaterfall = xDecompAgg[xDecompAgg["robynPareto"] == pf]
        if not quiet and len(uniqueSol) > 1:
            print(f">> Pareto-Front: {pf} [{len(uniqueSol)} models]")

        # Calculations for pareto AND pareto plots
        for sid in uniqueSol:
            if not quiet and len(uniqueSol) > 1:
                print(f"Processing solution {sid} of {len(uniqueSol)}", end="\r")

            ## 1. Spend x effect share comparison
            temp = plotMediaShare[plotMediaShare["solID"] == sid].melt(
                id_vars=["rn", "nrmse", "decomp.rssd", "rsq_train"],
                var_name="variable",
                value_name="value",
            )
            plotMediaShareLoopBar = temp[temp["variable"].isin(["spend_share", "effect_share"])]
            plotMediaShareLoopLine = temp[temp["variable"] == "roi_total"]
            line_rm_inf = ~temp["value"].isin([np.inf, -np.inf])
            ySecScale = max(plotMediaShareLoopLine["value"][line_rm_inf]) / max(plotMediaShareLoopBar["value"]) * 1.1
            plot1data = {
                "plotMediaShareLoopBar": plotMediaShareLoopBar,
                "plotMediaShareLoopLine": plotMediaShareLoopLine,
                "ySecScale": ySecScale,
            }

            ## 2. Waterfall
            plotWaterfallLoop = plotWaterfall[plotWaterfall["solID"] == sid].sort_values("xDecompPerc")
            plotWaterfallLoop["end"] = 1 - plotWaterfallLoop["xDecompPerc"].cumsum()
            plotWaterfallLoop["start"] = plotWaterfallLoop["end"].shift(1)
            plotWaterfallLoop.loc[0, "start"] = 1
            plotWaterfallLoop["id"] = range(len(plotWaterfallLoop))
            plotWaterfallLoop["rn"] = plotWaterfallLoop["rn"].astype("category")
            plotWaterfallLoop["sign"] = plotWaterfallLoop["xDecompPerc"].apply(lambda x: "Positive" if x >= 0 else "Negative")
            plot2data = {"plotWaterfallLoop": plotWaterfallLoop}

            ## 3. Adstock rate
            dt_geometric = weibullCollect = wb_type = None
            resultHypParamLoop = resultHypParam[resultHypParam["solID"] == sid]
            get_hp_names = [name for name in InputCollect["robyn_inputs"]["hyperparameters"].keys() if not name.endswith("_penalty")]
            hypParam = resultHypParamLoop[get_hp_names]
            if InputCollect["robyn_inputs"]["adstock"] == "geometric":
                hypParam_thetas = np.array([hypParam[f"{media}_thetas"] for media in InputCollect["robyn_inputs"]["all_media"]])
                dt_geometric = pd.DataFrame({"channels": InputCollect["robyn_inputs"]["all_media"], "thetas": hypParam_thetas.flatten()})

            if InputCollect["robyn_inputs"]["adstock"] in ["weibull_cdf", "weibull_pdf"]:
                shapeVec = np.array([hypParam[f"{media}_shapes"] for media in InputCollect["robyn_inputs"]["all_media"]])
                scaleVec = np.array([hypParam[f"{media}_scales"] for media in InputCollect["robyn_inputs"]["all_media"]])
                wb_type = InputCollect["robyn_inputs"]["adstock"][9:11]
                weibullCollect = []
                n = 1
                for v1 in range(len(InputCollect["robyn_inputs"]["all_media"])):
                    dt_weibull = pd.DataFrame(
                        {
                            "x": list(range(1, InputCollect["robyn_inputs"][rollingWindowLength + 1])),
                            "decay_accumulated": adstock_weibull(
                                list(range(1, InputCollect.rollingWindowLength + 1)),
                                shape=shapeVec[v1],
                                scale=scaleVec[v1],
                                type=wb_type
                            ).thetaVecCum,
                            "type": wb_type,
                            "channel": InputCollect["robyn_inputs"]["all_media"][v1]
                        }
                    )
                    dt_weibull["halflife"] = np.argmin(np.abs(dt_weibull.decay_accumulated - 0.5))
                    max_non0 = np.max(np.where(dt_weibull.decay_accumulated > 0.001, dt_weibull.x, np.nan))
                    dt_weibull["cut_time"] = np.where(max_non0 <= 5, 2 * max_non0, np.floor(max_non0 + max_non0 / 3))
                    weibullCollect.append(dt_weibull)
                    n += 1
                weibullCollect = pd.concat(weibullCollect)
                weibullCollect = weibullCollect.loc[weibullCollect.x <= weibullCollect.cut_time.max()]
                wb_type = wb_type.upper()
            plot3data = {
                "dt_geometric": dt_geometric,
                "weibullCollect": weibullCollect,
                "wb_type": wb_type
            }
            ## 4. Spend response curve
            # Select columns from dt_mod for independent variables
            dt_transformPlot = dt_mod[["ds"] + InputCollect["robyn_inputs"]["all_media"]]
            # Add paid media spends to dt_transformPlot
            dt_transformSpend = pd.concat([dt_transformPlot[["ds"]], InputCollect["robyn_inputs"]["dt_input"][InputCollect["robyn_inputs"]["paid_media_spends"]]], axis=1)
            # Select rows within rolling window
            dt_transformSpendMod = dt_transformPlot.iloc[rw_start_loc:rw_end_loc, :]

            dt_transformAdstock = dt_transformPlot
            dt_transformSaturation = dt_transformPlot.iloc[rw_start_loc:rw_end_loc]

            m_decayRate = []
            for med in range(len(InputCollect["robyn_inputs"]["all_media"])):
                med_select = InputCollect["robyn_inputs"]["all_media"][med]
                m = dt_transformPlot[med_select]
                # Adstocking
                theta, shape, scale = None, None, None
                adstock = InputCollect["robyn_inputs"]["adstock"]
                if adstock == "geometric":
                    theta = hypParam[f"{med_select}_thetas"].iloc[0]
                if adstock.startswith("weibull"):
                    shape = hypParam[f"{med_select}_shapes"].iloc[0]
                    scale = hypParam[f"{med_select}_scales"].iloc[0]
                x_list = transform_adstock(m, adstock, theta=theta, shape=shape, scale=scale)
                m_adstocked = x_list.x_decayed
                dt_transformAdstock.loc[:, med_select] = m_adstocked
                m_adstockedRollWind = m_adstocked.loc[rw_start_loc:rw_end_loc]
                ## Saturation
                alpha = hypParam[f"{med_select}_alphas"].iloc[0]
                gamma = hypParam[f"{med_select}_gammas"].iloc[0]
                dt_transformSaturation.loc[:, med_select] = saturation_hill(
                    x=m_adstockedRollWind, alpha=alpha, gamma=gamma
                )
            dt_transformSaturationDecomp = dt_transformSaturation
            for i in range(InputCollect["robyn_inputs"]["mediaVarCount"]):
                coefs = plotWaterfall.loc[plotWaterfall.rn == InputCollect["robyn_inputs"]["all_media"][i], "coefs"].values[0]
                dt_transformSaturationDecomp.loc[:, InputCollect["robyn_inputs"]["all_media"][i]] = coefs * dt_transformSaturationDecomp.loc[:, InputCollect["robyn_inputs"]["all_media"][i]]
            dt_transformSaturationSpendReverse = dt_transformAdstock.loc[rw_start_loc:rw_end_loc, :]

            dt_scurvePlot = pd.melt(dt_transformSaturationDecomp, id_vars='ds', var_name='channel', value_name='response')

            dt_spend = pd.melt(dt_transformSaturationSpendReverse, id_vars='ds', var_name='channel', value_name='spend')
            dt_scurvePlot['channel'] = dt_scurvePlot['channel'].apply(lambda x: x.replace('_S', ''))
            dt_spend['channel'] = dt_spend['channel'].apply(lambda x: x.replace('_S', ''))
            dt_scurvePlot = pd.merge(dt_scurvePlot, dt_spend, on=['ds', 'channel'], how='left')

            # Remove outlier introduced by MM nls fitting
            dt_scurvePlot = dt_scurvePlot.loc[dt_scurvePlot.spend >= 0, :]
            dt_scurvePlotMean = plotWaterfall.loc[(plotWaterfall.solID == sid) & (~pd.isna(plotWaterfall.mean_spend)),
                ["rn", "mean_spend", "mean_spend_adstocked", "mean_carryover", "mean_response", "solID"]
            ].rename(columns={"rn": "channel"})

            # Exposure response curve
            plot4data = {
                "dt_scurvePlot": dt_scurvePlot,
                "dt_scurvePlotMean": dt_scurvePlotMean
            }

            # 5. Fitted vs actual
            selected_columns = ['ds', 'dep_var'] + InputCollect['robyn_inputs']['prophet_vars'] + InputCollect['robyn_inputs']['context_vars']
            media_columns = InputCollect['robyn_inputs']['all_media']
            dt_transformDecomp = pd.concat([
                dt_modRollWind[selected_columns],
                dt_transformSaturation[media_columns]
            ], axis=1)

            col_order = ["ds", "dep_var"] + InputCollect['robyn_inputs']['all_ind_vars']
            dt_transformDecomp = dt_transformDecomp[col_order]
            #dt_transformDecomp = dt_transformDecomp.loc[:, ~dt_transformDecomp.columns.duplicated()]

            filtered_df = xDecompAgg[xDecompAgg['solID'] == sid][['solID', 'rn', 'coefs']]
            filtered_df_agg = filtered_df.groupby(['solID', 'rn']).agg({'coefs': 'sum'}).reset_index()
            xDecompVec = filtered_df_agg.pivot(index='solID', columns='rn', values='coefs').reset_index()
            xDecompVec['(Intercept)'] = xDecompVec.get('(Intercept)', 0)
            relevant_cols = ['solID', '(Intercept)'] + [col for col in col_order if col not in ['ds', 'dep_var', 'solID', '(Intercept)'] and col in xDecompVec.columns]
            xDecompVec = xDecompVec[relevant_cols]
            intercept = xDecompVec['(Intercept)'].values[0]
            for col in relevant_cols[2:]:  # Skipping 'solID' and '(Intercept)'
                dt_transformDecomp[col] = dt_transformDecomp[col] * xDecompVec[col].values[0]

            dt_transformDecomp['intercept'] = intercept
            numeric_cols = dt_transformDecomp.select_dtypes(include=[np.number])
            dt_transformDecomp['depVarHat'] = numeric_cols.sum(axis=1)
            xDecompVec = pd.concat([dt_transformDecomp[['ds', 'dep_var', 'depVarHat', 'intercept']], dt_transformDecomp.drop(columns=['ds', 'dep_var', 'intercept'])], axis=1)
            xDecompVec['solID'] = sid
            xDecompVec = xDecompVec.loc[:, ~xDecompVec.columns.duplicated()]

            xDecompVecPlot = dt_transformDecomp[['ds', 'dep_var', 'depVarHat']].rename(columns={"dep_var": "actual", "depVarHat": "predicted"})
            xDecompVecPlot = xDecompVecPlot.loc[:, ~xDecompVecPlot.columns.duplicated()]
            xDecompVecPlot = xDecompVecPlot.rename(columns={"dep_var": "actual", "depVarHat": "predicted"})
            xDecompVecPlotMelted = pd.melt(xDecompVecPlot, id_vars=["ds"], value_vars=["actual", "predicted"], var_name="variable", value_name="value")
            rsq = xDecompAgg[xDecompAgg['solID'] == sid]['rsq_train'].iloc[0]
            plot5data = {"xDecompVecPlotMelted": xDecompVecPlotMelted, "rsq": rsq}

            # 6. Diagnostic: fitted vs residual
            plot6data = {"xDecompVecPlot": xDecompVecPlot}

            # 7. Immediate vs carryover response
            hypParamSam = resultHypParam[resultHypParam.solID == sid]
            dt_saturated_dfs = run_transformations(InputCollect, hypParamSam, adstock)
            coefs = xDecompAgg['coefs'][xDecompAgg["solID"] == sid]
            coefs.index = xDecompAgg['rn'][xDecompAgg['solID'] == sid].values
            coefs = coefs.rename('s0').to_frame()

            df_reset = coefs.reset_index()
            df_deduped = df_reset.drop_duplicates(subset=['index'], keep='first')
            coefs = df_deduped.set_index('index')
            coefs.index.name = 'rn'

            from .model import model_decomp

            decompCollect = model_decomp(
                coefs=coefs,
                y_pred=dt_saturated_dfs["dt_modSaturated"]["dep_var"],
                dt_modSaturated=dt_saturated_dfs["dt_modSaturated"],
                dt_saturatedImmediate=dt_saturated_dfs["dt_saturatedImmediate"],
                dt_saturatedCarryover=dt_saturated_dfs["dt_saturatedCarryover"],
                dt_modRollWind=dt_modRollWind,
                refreshAddedStart=InputCollect["robyn_inputs"]["refreshAddedStart"]
            )
            decompCollectCopy = decompCollect.copy()
            mediaDecompImmediate = decompCollect["mediaDecompImmediate"].drop(
                columns=["ds", "y"]
            )
            mediaDecompImmediate.columns = [f"{col}_MDI" for col in mediaDecompImmediate.columns]
            mediaDecompCarryover = decompCollect["mediaDecompCarryover"].drop(
                columns=["ds", "y"]
            )
            mediaDecompCarryover.columns = [f"{col}_MDC" for col in mediaDecompCarryover.columns]
            temp = pd.concat(
                [
                    decompCollect["xDecompVec"],
                    mediaDecompImmediate,
                    mediaDecompCarryover
                ],
                axis=1
            ).assign(solID=sid)
            vec_collect = {
                "xDecompVec": temp.drop(columns=temp.iloc[:, temp.columns.str.endswith('_MDI') | temp.columns.str.endswith('_MDC')], axis=1),
                "xDecompVecImmediate": temp.drop(columns=temp.iloc[:, temp.columns.str.endswith('_MDC')], axis=1)
                                           .drop(columns=(column for column in temp.columns if any(column == name for name in InputCollect["robyn_inputs"]["all_media"])), axis=1),
                "xDecompVecCarryover": temp.drop(columns=temp.iloc[:, temp.columns.str.endswith('_MDI')], axis=1)
                                           .drop(columns=(column for column in temp.columns if any(column == name for name in InputCollect["robyn_inputs"]["all_media"])), axis=1)
            }
            this = vec_collect["xDecompVecImmediate"].columns.str.replace("_MDI", "")
            vec_collect["xDecompVecImmediate"].columns = [col for col in this]
            vec_collect["xDecompVecCarryover"].columns = [col for col in this]
            df_caov = vec_collect["xDecompVecCarryover"][InputCollect["robyn_inputs"]["all_media"] + ["solID"]].groupby("solID").sum().reset_index()
            df_total = vec_collect["xDecompVec"][InputCollect["robyn_inputs"]["all_media"] + ["solID"]].groupby("solID").sum().reset_index()
            df_caov_pct = pd.concat([df_caov[["solID"]], df_caov[InputCollect["robyn_inputs"]["all_media"]].div(df_total[InputCollect["robyn_inputs"]["all_media"]], axis=0)], axis=1)
            df_caov_pct = df_caov_pct.melt(id_vars="solID", var_name="rn", value_name="carryover_pct")
            df_caov_pct.replace(pd.NA, 0, inplace=True)

            df_caov_pct_all = pd.concat([df_caov_pct_all, df_caov_pct], ignore_index=True)
            # Gather everything in an aggregated format
            xDecompVecImmeCaov = pd.concat(
                [
                    vec_collect["xDecompVecImmediate"][InputCollect["robyn_inputs"]["all_media"]+ ["solID"]].assign(type="Immediate"),
                    vec_collect["xDecompVecCarryover"][InputCollect["robyn_inputs"]["all_media"]+ ["solID"]].assign(type="Carryover")
                ],
                ignore_index=True
            ).melt(
                id_vars=["solID", "type"],
                var_name="rn",
                value_name="value"
            ).groupby(["solID", "rn", "type"]).sum().reset_index()
            xDecompVecImmeCaov["percentage"] = xDecompVecImmeCaov.groupby(["solID", "rn"])["value"].transform(
                lambda x: x / x.sum()
            )
            xDecompVecImmeCaov.fillna(0, inplace=True)
            xDecompVecImmeCaov = xDecompVecImmeCaov.merge(
                df_caov_pct_all,
                on=["solID", "rn"]
            )
            if len(xDecompAgg.solID.unique()) == 1:
                xDecompVecImmeCaov["solID"] = OutputModels.trial1.resultCollect.resultHypParam.solID
            plot7data = xDecompVecImmeCaov

            # 8. Bootstrapped ROI/CPA with CIs
            # plot8data = "Empty"  # Filled when running robyn_onepagers() with clustering data

            # Gather all results
            # mediaVecCollect = pd.concat(
            #     [
            #         dt_transformAdstock.assign(type="adstockedMedia", solID=sid),
            #         dt_transformPlot.assign(type="rawMedia", solID=sid),
            #         dt_transformSpend.assign(type="rawSpend", solID=sid),
            #         dt_transformSpendMod.assign(type="predictedExposure", solID=sid),
            #         dt_transformSaturation.assign(type="saturatedMedia", solID=sid),
            #         dt_transformSaturationSpendReverse.assign(type="saturatedSpendReversed", solID=sid),
            #         dt_transformSaturationDecomp.assign(type="decompMedia", solID=sid)
            #     ],
            #     ignore_index=True
            # )
            new_data = pd.concat(
                [
                    dt_transformAdstock.assign(type="adstockedMedia", solID=sid),
                    dt_transformPlot.assign(type="rawMedia", solID=sid),
                    dt_transformSpend.assign(type="rawSpend", solID=sid),
                    dt_transformSpendMod.assign(type="predictedExposure", solID=sid),
                    dt_transformSaturation.assign(type="saturatedMedia", solID=sid),
                    dt_transformSaturationSpendReverse.assign(type="saturatedSpendReversed", solID=sid),
                    dt_transformSaturationDecomp.assign(type="decompMedia", solID=sid)
                ],
                ignore_index=True
            )
            mediaVecCollect = pd.concat([mediaVecCollect, new_data], ignore_index=True)
            xDecompVecCollect.append(xDecompVec)
            plotDataCollect[sid] = {
                "plot1data": plot1data,
                "plot2data": plot2data,
                "plot3data": plot3data,
                "plot4data": plot4data,
                "plot5data": plot5data,
                "plot6data": plot6data,
                "plot7data": plot7data
                # "plot8data": plot8data
            }

    xDecompVecCollect = pd.concat(xDecompVecCollect, ignore_index=True)


    ## Manually added some data to following dict since some of them printed as None.
    pareto_results = {
            "pareto_solutions": list(xDecompVecCollect["solID"].unique()),
            "pareto_fronts": pareto_fronts,
            "resultHypParam": resultHypParam,
            "xDecompAgg": xDecompAgg,
            "resultCalibration": resultCalibration,
            "mediaVecCollect": mediaVecCollect,
            "xDecompVecCollect": xDecompVecCollect,
            "plotDataCollect": plotDataCollect,
            "df_caov_pct_all": df_caov_pct_all
    }

    return pareto_results

def pareto_front(x, y, fronts=1, sort=True):
    if len(x) != len(y):
        raise ValueError("Length of x and y must be equal")

    d = pd.DataFrame({'x': x, 'y': y})

    if sort:
        D = d.sort_values(by=['x', 'y'], ascending=[True, True])
    else:
        D = d.copy()

    Dtemp = D.copy()
    df = pd.DataFrame(columns=['x', 'y', 'pareto_front'])

    i = 1
    while len(Dtemp) >= 1 and i <= max(fronts, 1):
        Dtemp['cummin'] = Dtemp['y'].cummin()
        these = Dtemp[Dtemp['y'] == Dtemp['cummin']].copy()
        these['pareto_front'] = i
        df = pd.concat([df, these], ignore_index=True)
        Dtemp = Dtemp[~Dtemp.index.isin(these.index)]
        i += 1

    ret = pd.merge(d, df[['x', 'y', 'pareto_front']], on=['x', 'y'], how='left', sort=sort)
    return ret

def get_pareto_fronts(pareto_fronts):
    if pareto_fronts == 'auto':
        return 1
    else:
        return pareto_fronts


def run_dt_resp(respN, InputCollect, OutputModels, decompSpendDistPar, resultHypParamPar, xDecompAggPar, **kwargs):
    get_solID = decompSpendDistPar.solID.values[respN]
    get_spendname = decompSpendDistPar.rn.values[respN]
    startRW = InputCollect["robyn_inputs"]["rollingWindowStartWhich"]
    endRW = InputCollect["robyn_inputs"]["rollingWindowEndWhich"]

    get_resp = robyn_response(
        select_model=get_solID,
        metric_name=get_spendname,
        date_range="all",
        dt_hyppar=resultHypParamPar,
        dt_coef=xDecompAggPar,
        InputCollect=InputCollect,
        OutputCollect=OutputModels,
        quiet=True
    )

    mean_spend_adstocked = np.mean(get_resp['input_total'][startRW:endRW])
    mean_carryover = np.mean(get_resp['input_carryover'][startRW:endRW])
    dt_hyppar = resultHypParamPar[resultHypParamPar.solID == get_solID]
    chnAdstocked = pd.DataFrame({get_spendname: get_resp['input_total'][startRW:endRW]})
    dt_coef = xDecompAggPar[(xDecompAggPar.solID == get_solID) & (xDecompAggPar.rn == get_spendname)][["rn", "coefs"]]
    hills = get_hill_params(
        InputCollect, None, dt_hyppar, dt_coef,
        mediaSpendSorted=get_spendname,
        select_model=get_solID,
        chnAdstocked=chnAdstocked
    )
    mean_response = fx_objective(
        x=decompSpendDistPar.mean_spend.values[respN],
        coeff=hills['coefs_sorted'],
        alpha=hills['alphas'],
        inflexion=hills['inflexions'],
        x_hist_carryover=mean_carryover,
        get_sum=False
    )
    dt_resp = pd.DataFrame({
        "mean_response": mean_response,
        "mean_spend_adstocked": mean_spend_adstocked,
        "mean_carryover": mean_carryover,
        "rn": decompSpendDistPar.rn.values[respN],
        "solID": decompSpendDistPar.solID.values[respN]
    })
    return dt_resp

================
File: plots.py
================
# Copyright (c) Meta Platforms, Inc. and its affiliates.

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

####################################################################
import multiprocessing
import os
import re
from itertools import product

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from plotnine import *
import seaborn as sns
from .auxiliary import robyn_palette
from tqdm import tqdm
from scipy.stats.mstats import winsorize
import json

# Plotting using plotnine
from plotnine import (
    aes,
    element_blank,
    facet_grid,
    geom_bar,
    geom_text,
    ggplot,
    labs,
    scale_fill_manual,
    scale_y_continuous,
    theme,
)


def robyn_plots(InputCollect, OutputCollect, export=True, plot_folder=None, **kwargs):
    def check_class(class_name, obj):
        # You should implement the check_class function according to your needs
        pass

    pareto_fronts = OutputCollect["pareto_fronts"]
    hyper_fixed = OutputCollect["hyper_fixed"]
    temp_all = OutputCollect["allPareto"]
    all_plots = {}

    if not hyper_fixed:
        # Prophet
        if (
            "prophet_vars" in InputCollect and len(InputCollect["prophet_vars"]) > 0
        ) or ("factor_vars" in InputCollect and len(InputCollect["factor_vars"]) > 0):
            dt_plotProphet = InputCollect["dt_mod"][
                ["ds", "dep_var"]
                + InputCollect["prophet_vars"]
                + InputCollect["factor_vars"]
            ]
            dt_plotProphet = dt_plotProphet.melt(
                id_vars=["ds"], var_name="variable", value_name="value"
            )
            dt_plotProphet["ds"] = pd.to_datetime(
                dt_plotProphet["ds"], origin="1970-01-01"
            )
            all_plots["pProphet"] = pProphet = sns.lineplot(
                data=dt_plotProphet,
                x="ds",
                y="value",
                hue="variable",
                palette="steelblue",
            )
            pProphet.set(title="Prophet decomposition", xlabel=None, ylabel=None)
            pProphet = pProphet + sns.scale_y_log()

            if export:
                pProphet.figure.savefig(
                    f"{plot_folder}prophet_decomp.png",
                    dpi=600,
                    bbox_inches="tight",
                    pad_inches=0.1,
                )

        # Spend exposure model (Code for this part is commented out in the original R code)

        # Hyperparameter sampling distribution
        if len(temp_all) > 0:
            resultHypParam = temp_all["resultHypParam"]
            hpnames_updated = [
                re.sub("lambda", "lambda_hp", col)
                for col in InputCollect["hyperparameters"]
            ]
            resultHypParam_melted = pd.melt(resultHypParam[hpnames_updated])
            resultHypParam_melted["variable"] = np.where(
                resultHypParam_melted["variable"] == "lambda_hp",
                "lambda",
                resultHypParam_melted["variable"],
            )
            resultHypParam_melted["type"] = [
                col.split("_")[-1] for col in resultHypParam_melted["variable"]
            ]
            resultHypParam_melted["channel"] = [
                col.replace(f'_{resultHypParam_melted["type"].iloc[i]}', "")
                for i, col in enumerate(resultHypParam_melted["variable"])
            ]
            resultHypParam_melted["type"] = pd.Categorical(
                resultHypParam_melted["type"],
                categories=np.unique(resultHypParam_melted["type"]),
            )
            resultHypParam_melted["channel"] = pd.Categorical(
                resultHypParam_melted["channel"],
                categories=np.unique(resultHypParam_melted["channel"]),
            )

            all_plots["pSamp"] = pSamp = sns.violinplot(
                data=resultHypParam_melted,
                x="value",
                y="channel",
                hue="channel",
                palette="viridis",
                inner=None,
                scale="width",
            )
            pSamp.set(
                title="Hyperparameters Optimization Distributions",
                xlabel="Hyperparameter space",
                ylabel=None,
            )

            if export:
                pSamp.figure.savefig(
                    f"{plot_folder}hypersampling.png",
                    dpi=600,
                    bbox_inches="tight",
                    pad_inches=0.1,
                )

        # Pareto front
        if len(temp_all) > 0:
            pareto_fronts_vec = list(range(1, pareto_fronts + 1))
            resultHypParam = temp_all["resultHypParam"]

            if (
                "calibration_input" in InputCollect
                and InputCollect["calibration_input"] is not None
            ):
                resultHypParam["iterations"] = np.where(
                    resultHypParam["robynPareto"].isna(),
                    np.nan,
                    resultHypParam["iterations"],
                )
                resultHypParam = resultHypParam[~resultHypParam["robynPareto"].isna()]
                resultHypParam = resultHypParam.sort_values(
                    by=["robynPareto"], ascending=False
                )

            calibrated = (
                "calibration_input" in InputCollect
                and InputCollect["calibration_input"] is not None
            )

            pParFront = sns.scatterplot(
                data=resultHypParam,
                x="nrmse",
                y="decomp.rssd",
                hue="iterations",
                palette="coolwarm",
                alpha=0.7,
                size="mape",
            )
            pParFront.set(
                title="Multi-objective Evolutionary Performance with Calibration"
                if calibrated
                else "Multi-objective Evolutionary Performance",
                xlabel="NRMSE",
                ylabel="DECOMP.RSSD",
            )

            for pfs in range(1, max(pareto_fronts_vec) + 1):
                pf_color = "coral2" if pfs == 2 else "coral" if pfs == 3 else "coral"
                temp = resultHypParam[resultHypParam["robynPareto"] == pfs]
                if len(temp) > 1:
                    pParFront.plot(
                        temp["nrmse"],
                        temp["decomp.rssd"],
                        color=pf_color,
                        linestyle="-",
                    )

            all_plots["pParFront"] = pParFront

            if export:
                pParFront.figure.savefig(
                    f"{plot_folder}pareto_front.png",
                    dpi=600,
                    bbox_inches="tight",
                    pad_inches=0.1,
                )

        # Ridgeline model convergence (Code for this part is commented out in the original R code)

    get_height = int(np.ceil(12 * OutputCollect["OutputModels"]["trials"] / 3))

    if OutputCollect["OutputModels"]["ts_validation"]:
        # You should implement the ts_validation function according to your needs
        ts_validation_plot = ts_validation(
            OutputCollect["OutputModels"], quiet=True, **kwargs
        )
        ts_validation_plot.figure.savefig(
            f"{plot_folder}ts_validation.png",
            dpi=300,
            bbox_inches="tight",
            pad_inches=0.1,
        )

    return all_plots


def robyn_onepagers(
    InputCollect,
    OutputCollect,
    select_model=None,
    quiet=False,
    export=True,
    plot_folder=None,
    baseline_level=0,
    *args,
    **kwargs,
):
    def isTRUE(val):
        return val

    check_class("robyn_outputs", OutputCollect)

    if True:
        pareto_fronts = OutputCollect["pareto_fronts"]
        hyper_fixed = OutputCollect["hyper_fixed"]
        resultHypParam = pd.DataFrame(OutputCollect["resultHypParam"])
        xDecompAgg = pd.DataFrame(OutputCollect["xDecompAgg"])
        val = isTRUE(OutputCollect["OutputModels"]["ts_validation"])
        sid = None  # for parallel loops

    if select_model is not None:
        if "clusters" in select_model:
            select_model = OutputCollect["clusters"]["models"]["solID"]
        resultHypParam = resultHypParam[resultHypParam["solID"].isin(select_model)]
        xDecompAgg = xDecompAgg[xDecompAgg["solID"].isin(select_model)]
        if not quiet and resultHypParam.shape[0] > 1:
            print(
                ">> Generating only cluster results one-pagers (",
                resultHypParam.shape[0],
                ")...",
            )

    # Baseline variables
    bvars = baseline_vars(InputCollect, baseline_level)

    # Prepare for parallel plotting
    if check_parallel_plot() and OutputCollect["cores"] > 1:
        registerDoParallel(OutputCollect["cores"])
    else:
        registerDoSEQ()

    if not hyper_fixed:
        pareto_fronts_vec = list(range(1, pareto_fronts + 1))
        count_mod_out = resultHypParam[
            resultHypParam["robynPareto"].isin(pareto_fronts_vec)
        ].shape[0]
    else:
        pareto_fronts_vec = [1]
        count_mod_out = resultHypParam.shape[0]

    all_fronts = list(filter(lambda x: not pd.isna(x), xDecompAgg["robynPareto"]))
    all_fronts = sorted(all_fronts)
    if not all(
        pareto_fronts_vec[i] in all_fronts for i in range(len(pareto_fronts_vec))
    ):
        pareto_fronts_vec = all_fronts

    if check_parallel_plot():
        if not quiet and resultHypParam.shape[0] > 1:
            print(
                ">> Plotting",
                count_mod_out,
                "selected models on",
                OutputCollect["cores"],
                "cores...",
            )
    else:
        if not quiet and resultHypParam.shape[0] > 1:
            print(
                ">> Plotting",
                count_mod_out,
                "selected models on 1 core (MacOS fallback)...",
            )

    if not quiet and count_mod_out > 1:
        pbplot = tqdm(total=count_mod_out)
    temp = OutputCollect["allPareto"]["plotDataCollect"]
    all_plots = []
    cnt = 0

    for pf in pareto_fronts_vec:
        plotMediaShare = xDecompAgg[
            (xDecompAgg["robynPareto"] == pf)
            & (xDecompAgg["rn"].isin(InputCollect["paid_media_spends"]))
        ]
        uniqueSol = plotMediaShare["solID"].unique()

        def parallel_result(sid):
            if True:
                plotMediaShareLoop = plotMediaShare[plotMediaShare["solID"] == sid]
                rsq_train_plot = round(plotMediaShareLoop["rsq_train"].iloc[0], 4)
                rsq_val_plot = round(plotMediaShareLoop["rsq_val"].iloc[0], 4)
                rsq_test_plot = round(plotMediaShareLoop["rsq_test"].iloc[0], 4)
                nrmse_train_plot = round(plotMediaShareLoop["nrmse_train"].iloc[0], 4)
                nrmse_val_plot = round(plotMediaShareLoop["nrmse_val"].iloc[0], 4)
                nrmse_test_plot = round(plotMediaShareLoop["nrmse_test"].iloc[0], 4)
                decomp_rssd_plot = round(plotMediaShareLoop["decomp.rssd"].iloc[0], 4)

            if InputCollect["calibration_input"] is not None:
                mape_lift_plot = round(plotMediaShareLoop["mape"].iloc[0], 4)
            else:
                mape_lift_plot = None

            train_size = round(plotMediaShareLoop["train_size"].iloc[0], 4)

            if val:
                errors = [
                    "Adj.R2: train = {}, val = {}, test = {} |".format(
                        rsq_train_plot, rsq_val_plot, rsq_test_plot
                    ),
                    "NRMSE: train = {}, val = {}, test = {} |".format(
                        nrmse_train_plot, nrmse_val_plot, nrmse_test_plot
                    ),
                    "DECOMP.RSSD = {} |".format(decomp_rssd_plot),
                    "MAPE = {}".format(mape_lift_plot)
                    if mape_lift_plot is not None
                    else "",
                ]
            else:
                errors = [
                    "Adj.R2: train = {} |".format(rsq_train_plot),
                    "NRMSE: train = {} |".format(nrmse_train_plot),
                    "DECOMP.RSSD = {} |".format(decomp_rssd_plot),
                    "MAPE = {}".format(mape_lift_plot)
                    if mape_lift_plot is not None
                    else "",
                ]
            errors = " ".join(errors)

            ## 1. Spend x effect share comparison
            plotMediaShareLoopBar = temp[sid]["plot1data"]["plotMediaShareLoopBar"]
            plotMediaShareLoopLine = temp[sid]["plot1data"]["plotMediaShareLoopLine"]
            ySecScale = temp[sid]["plot1data"]["ySecScale"]

            plotMediaShareLoopBar["variable"] = (
                plotMediaShareLoopBar["variable"].str.replace("_", " ").str.title()
            )

            type = "CPA" if InputCollect["dep_var_type"] == "conversion" else "ROI"
            plotMediaShareLoopLine["type_colour"] = type_colour = "#03396C"

            p1 = (
                ggplot(plotMediaShareLoopBar, aes(x="rn", y="value", fill="variable"))
                + geom_bar(stat="identity", width=0.5, position="dodge")
                + geom_text(
                    aes(y=0, label="value"),
                    format_string="{:.1%}",
                    hjust=-0.1,
                    position=position_dodge(width=0.5),
                    fontweight="bold",
                )
                + geom_line(
                    plotMediaShareLoopLine,
                    aes(x="rn", y="value / ySecScale", group=1),
                    color=type_colour,
                    size=1,
                )
                + geom_point(
                    plotMediaShareLoopLine,
                    aes(x="rn", y="value / ySecScale", group=1),
                    color=type_colour,
                    size=3.5,
                )
                + geom_text(
                    plotMediaShareLoopLine,
                    aes(label="value", x="rn", y="value / ySecScale", group=1),
                    format_string="{:.2f}",
                    color=type_colour,
                    fontweight="bold",
                    hjust=-0.4,
                    size=10,
                )
                + scale_y_continuous(labels=percent_format())
                + coord_flip()
                + theme(axis_text_x=element_blank(), legend_position="top")
                + scale_fill_brewer(palette=3)
                + labs(
                    title=f"Total Spend% VS Effect% with total {type}",
                    y="Total Share by Channel",
                )
            )

            # 2. Waterfall
            plotWaterfallLoop = temp[sid]["plot2data"]["plotWaterfallLoop"]
            plotWaterfallLoop = plotWaterfallLoop.assign(
                rn=np.where(
                    plotWaterfallLoop["rn"].isin(bvars),
                    f"Baseline_L{baseline_level}",
                    plotWaterfallLoop["rn"],
                )
            )
            plotWaterfallLoop = (
                plotWaterfallLoop.groupby("rn")
                .agg({"xDecompAgg": "sum", "xDecompPerc": "sum"})
                .reset_index()
            )
            plotWaterfallLoop = plotWaterfallLoop.sort_values("xDecompPerc")
            plotWaterfallLoop["end"] = 1 - plotWaterfallLoop["xDecompPerc"].cumsum()
            plotWaterfallLoop["start"] = plotWaterfallLoop["end"].shift()
            plotWaterfallLoop["start"] = plotWaterfallLoop["start"].fillna(1)
            plotWaterfallLoop["id"] = range(1, len(plotWaterfallLoop) + 1)
            plotWaterfallLoop["rn"] = pd.Categorical(plotWaterfallLoop["rn"])
            plotWaterfallLoop["sign"] = np.where(
                plotWaterfallLoop["xDecompPerc"] >= 0, "Positive", "Negative"
            )

            p2 = (
                ggplot(plotWaterfallLoop, aes(x="id", fill="sign"))
                + geom_rect(
                    aes(
                        x="rn",
                        xmin="id - 0.45",
                        xmax="id + 0.45",
                        ymin="end",
                        ymax="start",
                    ),
                    stat="identity",
                )
                + scale_x_discrete(
                    "",
                    breaks=plotWaterfallLoop["rn"].cat.categories,
                    labels=plotWaterfallLoop["rn"].cat.categories,
                )
                + scale_y_percent()
                + scale_fill_manual(
                    values={"Positive": "#59B3D2", "Negative": "#E5586E"}
                )
                + theme_lares(background="white", legend="top")
                + geom_text(
                    mapping=aes(label="xDecompAggFormatted"),
                    y=plotWaterfallLoop.apply(
                        lambda x: x["end"] + x["xDecompPerc"] / 2, axis=1
                    ),
                    fontface="bold",
                    lineheight=0.7,
                )
                + coord_flip()
                + labs(
                    title="Response Decomposition Waterfall by Predictor",
                    x=None,
                    y=None,
                    fill="Sign",
                )
            )

            # Formatting the xDecompAgg column
            plotWaterfallLoop["xDecompAggFormatted"] = plotWaterfallLoop.apply(
                lambda x: f"{x['xDecompAgg']:.2f}\n{round(x['xDecompPerc'] * 100, 1)}%",
                axis=1,
            )

            # 3. Adstock rate
            if InputCollect["adstock"] == "geometric":
                dt_geometric = temp[sid]["plot3data"]["dt_geometric"]
                p3 = (
                    ggplot(
                        dt_geometric,
                        aes(x=".data$channels", y=".data$thetas", fill="coral"),
                    )
                    + geom_bar(stat="identity", width=0.5)
                    + theme_lares(background="white", legend="none", grid="Xx")
                    + coord_flip()
                    + geom_text(
                        aes(label=formatNum(100 * ".data$thetas", 1, pos="%")),
                        hjust=-0.1,
                        position=position_dodge(width=0.5),
                        fontface="bold",
                    )
                    + scale_y_percent(limit=[0, 1])
                    + labs(
                        title="Geometric Adstock: Fixed Rate Over Time",
                        y=f"Thetas [by {InputCollect['intervalType']}]",
                        x=None,
                    )
                )

            if InputCollect["adstock"] in ["weibull_cdf", "weibull_pdf"]:
                weibullCollect = temp[sid]["plot3data"]["weibullCollect"]
                wb_type = temp[sid]["plot3data"]["wb_type"]
                p3 = (
                    ggplot(
                        weibullCollect,
                        aes(
                            x=".data$x",
                            y=".data$decay_accumulated",
                            color=".data$channel",
                        ),
                    )
                    + geom_line()
                    + facet_wrap("~ .data$channel")
                    + geom_hline(yintercept=0.5, linetype="dashed", color="gray")
                    + geom_text(
                        x=max(".data$x"),
                        y=0.5,
                        vjust=-0.5,
                        hjust=1,
                        label="Halflife",
                        color="gray",
                    )
                    + theme_lares(background="white", legend="none", grid="Xx")
                    + labs(
                        title=f"Weibull {wb_type} Adstock: Flexible Rate Over Time",
                        x=f"Time unit [{InputCollect['intervalType']}s]",
                        y=None,
                    )
                )

            # 4. Response curves
            dt_scurvePlot = temp["sid"]["plot4data"]["dt_scurvePlot"]
            dt_scurvePlotMean = temp["sid"]["plot4data"]["dt_scurvePlotMean"]
            trim_rate = 1.3

            if trim_rate > 0:
                dt_scurvePlot = dt_scurvePlot[
                    (
                        dt_scurvePlot["spend"]
                        < dt_scurvePlotMean["mean_spend_adstocked"].max() * trim_rate
                    )
                    & (
                        dt_scurvePlot["response"]
                        < dt_scurvePlotMean["mean_response"].max() * trim_rate
                    )
                ]
                dt_scurvePlot = dt_scurvePlot.merge(
                    dt_scurvePlotMean[["channel", "mean_carryover"]],
                    on="channel",
                    how="left",
                )

            if "channel" not in dt_scurvePlotMean.columns:
                dt_scurvePlotMean["channel"] = dt_scurvePlotMean["rn"]

            p4 = (
                ggplot(dt_scurvePlot, aes(x="spend", y="response", color="channel"))
                + geom_line()
                + geom_area(
                    dt_scurvePlot[
                        dt_scurvePlot["spend"] <= dt_scurvePlot["mean_carryover"]
                    ],
                    aes(x="spend", y="response", fill="channel"),
                    position="stack",
                    alpha=0.4,
                    show_legend=False,
                )
                + geom_point(
                    dt_scurvePlotMean, aes(x="mean_spend_adstocked", y="mean_response")
                )
                + geom_text(
                    dt_scurvePlotMean,
                    aes(
                        x="mean_spend_adstocked",
                        y="mean_response",
                        label="mean_spend_adstocked",
                    ),
                    format_string="{:.2e}",
                    hjust=-0.2,
                    show_legend=False,
                )
                + theme(
                    legend_position=(0.9, 0.2),
                    legend_background=element_rect(fill="grey98", color="grey90"),
                )
                + labs(
                    title="Response Curves and Mean Spends by Channel",
                    x="Spend (carryover + immediate)",
                    y="Response",
                )
                + scale_y_continuous(labels=scientific_format())
                + scale_x_continuous(labels=scientific_format())
            )

            # 5. Fitted vs actual
            xDecompVecPlotMelted = temp[sid]["plot5data"]["xDecompVecPlotMelted"]
            xDecompVecPlotMelted = xDecompVecPlotMelted.assign(
                linetype=np.where(
                    xDecompVecPlotMelted["variable"] == "predicted", "solid", "dotted"
                ),
                variable=xDecompVecPlotMelted["variable"].str.title(),
                ds=pd.to_datetime(xDecompVecPlotMelted["ds"], origin="1970-01-01"),
            )

            p5 = (
                ggplot(
                    xDecompVecPlotMelted,
                    aes(x=".data$ds", y=".data$value", color=".data$variable"),
                )
                + geom_path(aes(linetype=".data$linetype"), size=0.6)
                + theme_lares(background="white", legend="top", pal=2)
                + scale_y_abbr()
                + guides(linetype="none")
                + labs(
                    title="Actual vs. Predicted Response",
                    x="Date",
                    y="Response",
                    color=None,
                )
            )

            if val:
                days = sorted(xDecompVecPlotMelted["ds"].unique())
                ndays = len(days)
                train_cut = round(ndays * train_size)
                val_cut = train_cut + round(ndays * (1 - train_size) / 2)

                p5 = (
                    p5
                    + geom_vline(
                        xintercept=days[train_cut], colour="#39638b", alpha=0.8
                    )
                    + geom_text(
                        x=days[train_cut],
                        y=np.inf,
                        hjust=0,
                        vjust=1.2,
                        angle=270,
                        colour="#39638b",
                        alpha=0.5,
                        size=3.2,
                        label=f"Train: {formatNum(100 * train_size, 1, pos='%')}",
                    )
                    + geom_vline(xintercept=days[val_cut], colour="#39638b", alpha=0.8)
                    + geom_text(
                        x=days[val_cut],
                        y=np.inf,
                        hjust=0,
                        vjust=1.2,
                        angle=270,
                        colour="#39638b",
                        alpha=0.5,
                        size=3.2,
                        label=f"Validation: {formatNum(100 * (1 - train_size) / 2, 1, pos='%')}",
                    )
                    + geom_vline(
                        xintercept=days[ndays - 1], colour="#39638b", alpha=0.8
                    )
                    + geom_text(
                        x=days[ndays - 1],
                        y=np.inf,
                        hjust=0,
                        vjust=1.2,
                        angle=270,
                        colour="#39638b",
                        alpha=0.5,
                        size=3.2,
                        label=f"Test: {formatNum(100 * (1 - train_size) / 2, 1, pos='%')}",
                    )
                )

            # 6. Diagnostic: fitted vs residual
            xDecompVecPlot = temp[sid]["plot6data"]["xDecompVecPlot"]

            p6 = (
                qplot(
                    x=".data$predicted",
                    y=".data$actual - .data$predicted",
                    data=xDecompVecPlot,
                )
                + geom_hline(yintercept=0)
                + geom_smooth(se=True, method="loess", formula="y ~ x")
                + scale_x_abbr()
                + scale_y_abbr()
                + theme_lares(background="white")
                + labs(x="Fitted", y="Residual", title="Fitted vs. Residual")
            )

            # 7. Immediate vs carryover
            df_imme_caov = temp[sid]["plot7data"]

            p7 = (
                df_imme_caov.assign(
                    type=pd.Categorical(
                        df_imme_caov["type"], categories=["Carryover", "Immediate"]
                    )
                ).pipe(
                    ggplot,
                    aes(
                        x=".data$percentage",
                        y=".data$rn",
                        fill="reorder(.data$type, as.integer(.data$type))",
                        label='paste0(round(.data$percentage * 100), "%")',
                    ),
                )
                + geom_bar(stat="identity", width=0.5)
                + geom_text(position=position_stack(vjust=0.5))
                + scale_fill_manual(
                    values={"Immediate": "#59B3D2", "Carryover": "coral"}
                )
                + scale_x_percent()
                + theme_lares(background="white", legend="top", grid="Xx")
                + labs(
                    x="% Response",
                    y=None,
                    fill=None,
                    title="Immediate vs. Carryover Response Percentage",
                )
            )

            # 8. Bootstrapped ROI/CPA with CIs
            if "ci_low" in xDecompAgg.columns:
                metric = (
                    "CPA" if InputCollect["dep_var_type"] == "conversion" else "ROI"
                )
                p8 = (
                    xDecompAgg.loc[
                        ~xDecompAgg["ci_low"].isna() & (xDecompAgg["solID"] == sid)
                    ]
                    .loc[:, ["rn", "solID", "boot_mean", "ci_low", "ci_up"]]
                    .pipe(ggplot, aes(x=".data$rn", y=".data$boot_mean"))
                    + geom_point(size=3)
                    + geom_text(
                        aes(label="signif(.data$boot_mean, 2)"), vjust=-0.7, size=3.3
                    )
                    + geom_text(
                        aes(y=".data$ci_low", label="signif(.data$ci_low, 2)"),
                        hjust=1.1,
                        size=2.8,
                    )
                    + geom_text(
                        aes(y=".data$ci_up", label="signif(.data$ci_up, 2)"),
                        hjust=-0.1,
                        size=2.8,
                    )
                    + geom_errorbar(
                        aes(ymin=".data$ci_low", ymax=".data$ci_up"), width=0.25
                    )
                    + labs(
                        title=f"In-cluster bootstrapped {metric} with 95% CI & mean",
                        x=None,
                        y=None,
                    )
                    + coord_flip()
                    + theme_lares(background="white")
                )

                if metric == "ROI":
                    p8 += geom_hline(
                        yintercept=1, alpha=0.5, colour="grey50", linetype="dashed"
                    )
            else:
                p8 = lares.noPlot("No bootstrap results")

            # Aggregate one-pager plots and export
            ver = str(utils.packageVersion("Robyn"))
            rver = utils.sessionInfo().R_version
            onepagerTitle = f"One-pager for Model ID: {sid}"
            onepagerCaption = f"Robyn v{ver} [R-{rver.major}.{rver.minor}]"
            get_height = len(plotMediaShareLoopLine["rn"].unique()) / 5

            pg = (
                (p2 + p5) / (p1 + p8) / (p3 + p7) / (p4 + p6)
                + plot_layout(heights=[get_height, get_height, get_height, 1])
                + plot_annotation(
                    title=onepagerTitle,
                    subtitle=errors,
                    theme=theme_lares(background="white"),
                    caption=onepagerCaption,
                )
            )

            all_plots[sid] = pg

            if export:
                filename = f"{plot_folder}{sid}.png"
                ggsave(
                    filename=filename,
                    plot=pg,
                    limitsize=False,
                    dpi=400,
                    width=17,
                    height=19,
                )
                if count_mod_out == 1:
                    print(f"Exporting charts as: {filename}")

            if check_parallel_plot() and not quiet and count_mod_out > 1:
                cnt += 1

            return all_plots

        with multiprocessing.Pool() as pool:
            results = pool.map(parallel_result, uniqueSol)

        if not quiet and count_mod_out > 1:
            cnt += len(uniqueSol)

    if not quiet and count_mod_out > 1:
        pbplot.close()

    return results[0]


def allocation_plots(
    InputCollect,
    OutputCollect,
    dt_optimOut,
    select_model,
    scenario,
    eval_list,
    export=True,
    plot_folder=None,
    quiet=False,
    **kwargs,
):
    outputs = {}

    adstocked = "(adstocked**) " if dt_optimOut["adstocked"][0] else ""
    total_spend_increase = round(mean(dt_optimOut["optmSpendUnitTotalDelta"]) * 100, 1)
    total_response_increase = round(
        mean(dt_optimOut["optmResponseUnitTotalLift"]) * 100, 1
    )

    subtitle = f"Total {adstocked}spend increase: {total_spend_increase}%\nTotal response increase: {total_response_increase}% with optimised spend allocation"

    metric = "ROAS" if InputCollect["dep_var_type"] == "revenue" else "CPA"

    if metric == "ROAS":
        formulax1 = "ROAS = total response / raw spend | mROAS = marginal response / marginal spend"
        formulax2 = "When reallocating budget, mROAS converges across media within respective bounds"
    else:
        formulax1 = "CPA = raw spend / total response | mCPA = marginal spend / marginal response"
        formulax2 = "When reallocating budget, mCPA converges across media within respective bounds"

    plotDT_scurveMeanResponse = OutputCollect["xDecompAgg"][
        (OutputCollect["xDecompAgg"]["solID"] == select_model)
        & (OutputCollect["xDecompAgg"]["rn"].isin(InputCollect["paid_media_spends"]))
    ]

    # Calculate the statistics
    rsq_train_plot = round(plotDT_scurveMeanResponse["rsq_train"].iloc[0], 4)
    rsq_val_plot = round(plotDT_scurveMeanResponse["rsq_val"].iloc[0], 4)
    rsq_test_plot = round(plotDT_scurveMeanResponse["rsq_test"].iloc[0], 4)
    nrmse_train_plot = round(plotDT_scurveMeanResponse["nrmse_train"].iloc[0], 4)
    nrmse_val_plot = round(plotDT_scurveMeanResponse["nrmse_val"].iloc[0], 4)
    nrmse_test_plot = round(plotDT_scurveMeanResponse["nrmse_test"].iloc[0], 4)
    decomp_rssd_plot = round(plotDT_scurveMeanResponse["decomp_rssd"].iloc[0], 4)
    mape_lift_plot = (
        round(plotDT_scurveMeanResponse["mape"].iloc[0], 4)
        if "calibration_input" in InputCollect
        else None
    )

    # Create the error message string
    if OutputCollect["OutputModels"]["ts_validation"]:
        errors = f"Adj.R2: train = {rsq_train_plot}, val = {rsq_val_plot}, test = {rsq_test_plot} | NRMSE: train = {nrmse_train_plot}, val = {nrmse_val_plot}, test = {nrmse_test_plot} | DECOMP.RSSD = {decomp_rssd_plot} | MAPE = {mape_lift_plot}"
    else:
        errors = f"Adj.R2: train = {rsq_train_plot} | NRMSE: train = {nrmse_train_plot} | DECOMP.RSSD = {decomp_rssd_plot} | MAPE = {mape_lift_plot}"

    init_total_spend = dt_optimOut["initSpendTotal"][0]
    init_total_response = dt_optimOut["initResponseTotal"][0]
    init_total_roi = (
        init_total_response / init_total_spend
        if init_total_spend != 0
        else float("inf")
    )
    init_total_cpa = (
        init_total_spend / init_total_response
        if init_total_response != 0
        else float("inf")
    )

    optm_total_spend_bounded = dt_optimOut["optmSpendTotal"][0]
    optm_total_response_bounded = dt_optimOut["optmResponseTotal"][0]
    optm_total_roi_bounded = (
        optm_total_response_bounded / optm_total_spend_bounded
        if optm_total_spend_bounded != 0
        else float("inf")
    )
    optm_total_cpa_bounded = (
        optm_total_spend_bounded / optm_total_response_bounded
        if optm_total_response_bounded != 0
        else float("inf")
    )

    optm_total_spend_unbounded = dt_optimOut["optmSpendTotalUnbound"][0]
    optm_total_response_unbounded = dt_optimOut["optmResponseTotalUnbound"][0]
    optm_total_roi_unbounded = (
        optm_total_response_unbounded / optm_total_spend_unbounded
        if optm_total_spend_unbounded != 0
        else float("inf")
    )
    optm_total_cpa_unbounded = (
        optm_total_spend_unbounded / optm_total_response_unbounded
        if optm_total_response_unbounded != 0
        else float("inf")
    )

    bound_mult = dt_optimOut["unconstr_mult"][0]

    optm_topped_unbounded = optm_topped_bounded = any_topped = False

    if "total_budget" in eval_list and eval_list["total_budget"] is not None:
        optm_topped_bounded = round(optm_total_spend_bounded) < round(
            eval_list["total_budget"]
        )
        optm_topped_unbounded = round(optm_total_spend_unbounded) < round(
            eval_list["total_budget"]
        )
        any_topped = optm_topped_bounded or optm_topped_unbounded

        if optm_topped_bounded and not quiet:
            print(
                "NOTE: Given the upper/lower constrains, the total budget can't be fully allocated (^)"
            )

    levs1 = eval_list["levs1"]

    if scenario == "max_response":
        levs2 = [
            "Initial",
            f"Bounded{'^' if optm_topped_bounded else ''}",
            f"Unbounded{'^' if optm_topped_unbounded else ''} x{bound_mult}",
        ]
    elif scenario == "target_efficiency":
        levs2 = levs1

    resp_metric = pd.DataFrame(
        {
            "type": pd.Categorical(levs1, categories=levs1),
            "type_lab": pd.Categorical(levs2, categories=levs2),
            "total_spend": [
                init_total_spend,
                optm_total_spend_bounded,
                optm_total_spend_unbounded,
            ],
            "total_response": [
                init_total_response,
                optm_total_response_bounded,
                optm_total_response_unbounded,
            ],
            "total_response_lift": [
                0,
                dt_optimOut["optmResponseUnitTotalLift"][0],
                dt_optimOut["optmResponseUnitTotalLiftUnbound"][0],
            ],
            "total_roi": [
                init_total_roi,
                optm_total_roi_bounded,
                optm_total_roi_unbounded,
            ],
            "total_cpa": [
                init_total_cpa,
                optm_total_cpa_bounded,
                optm_total_cpa_unbounded,
            ],
        }
    )

    df_roi = (
        resp_metric.assign(
            spend=resp_metric["total_spend"], response=resp_metric["total_response"]
        )[["type", "spend", "response"]]
        .melt(id_vars="type", var_name="name")
        .merge(resp_metric, on="type")
    )

    df_roi["name"] = pd.Categorical(
        "total " + df_roi["name"], categories=["total spend", "total response"]
    )
    df_roi["name_label"] = pd.Categorical(
        df_roi["type"] + "\n" + df_roi["name"],
        categories=[
            t + "\n" + n for t in df_roi["type"].unique() for n in ["spend", "response"]
        ],
    )

    df_roi["value_norm"] = df_roi.groupby("name")["value"].transform(
        lambda x: x / x.iloc[0]
    )

    # Calculate metric values and labels
    metric_vals = (
        resp_metric["total_roi"] if metric == "ROAS" else resp_metric["total_cpa"]
    )
    labs = [
        f"{lev}\nSpend: {100 * (spend - resp_metric['total_spend'].iloc[0]) / resp_metric['total_spend'].iloc[0]:.3f}%\nResp: {100 * lift:.3f}%\n{metric}: {m_val:.2f}"
        for lev, spend, lift, m_val in zip(
            levs2,
            resp_metric["total_spend"],
            df_roi["total_response_lift"],
            metric_vals,
        )
    ]
    df_roi["labs"] = pd.Categorical(np.repeat(labs, 2), categories=labs)

    p1 = (
        ggplot(df_roi, aes(x="name", y="value_norm", fill="type"))
        + facet_grid(". ~ labs", scales="free")
        + scale_fill_manual(values=["grey", "steelblue", "darkgoldenrod4"])
        + geom_bar(stat="identity", width=0.6, alpha=0.7)
        + geom_text(
            aes(label="value"), format_string="{:.3f}", color="black", vjust=-0.5
        )
        # The theme_lares() function is specific to R and doesn't have an exact equivalent in plotnine, so we use a basic theme modification
        + theme(
            legend_position="none",
            plot_background=element_blank(),
            axis_text_y=element_blank(),
        )
        + labs(title="Total Budget Optimization Result", fill=None, y=None, x=None)
        + scale_y_continuous(limits=(0, df_roi["value_norm"].max() * 1.2))
    )

    outputs["p1"] = p1

    # 2. Response and spend comparison per channel plot
    df_plots = dt_optimOut.copy()
    df_plots["channel"] = df_plots["channels"].astype("category")
    df_plots["Initial"] = df_plots["initResponseUnitShare"]
    df_plots["Bounded"] = df_plots["optmResponseUnitShare"]
    df_plots["Unbounded"] = df_plots["optmResponseUnitShareUnbound"]

    response_share = df_plots.melt(
        id_vars="channel",
        value_vars=["Initial", "Bounded", "Unbounded"],
        var_name="type",
        value_name="response_share",
    )

    df_plots["Initial"] = df_plots["initSpendShare"]
    df_plots["Bounded"] = df_plots["optmSpendShareUnit"]
    df_plots["Unbounded"] = df_plots["optmSpendShareUnitUnbound"]

    spend_share = df_plots.melt(
        id_vars="channel",
        value_vars=["Initial", "Bounded", "Unbounded"],
        var_name="type",
        value_name="spend_share",
    )

    df_plots = response_share.merge(spend_share, on=["channel", "type"])

    # Create channel ROI or CPA based on 'metric'
    if metric == "ROAS":
        value_vars = "roiUnit"
    else:
        value_vars = "cpaUnit"

    df_plots["Initial"] = df_plots[f"init{value_vars}"]
    df_plots["Bounded"] = df_plots[f"optm{value_vars}"]
    df_plots["Unbounded"] = df_plots[f"optm{value_vars}Unbound"]

    channel_metric = df_plots.melt(
        id_vars="channel",
        value_vars=["Initial", "Bounded", "Unbounded"],
        var_name="type",
        value_name=f"channel_{metric.lower()}",
    )

    df_plots = df_plots.merge(channel_metric, on=["channel", "type"])

    # Same logic for marginal ROI or CPA
    df_plots["Initial"] = df_plots[f"initResponseMargUnit"].apply(
        lambda x: x if metric == "ROAS" else 1 / x
    )
    df_plots["Bounded"] = df_plots[f"optmResponseMargUnit"].apply(
        lambda x: x if metric == "ROAS" else 1 / x
    )
    df_plots["Unbounded"] = df_plots[f"optmResponseMargUnitUnbound"].apply(
        lambda x: x if metric == "ROAS" else 1 / x
    )

    marginal_metric = df_plots.melt(
        id_vars="channel",
        value_vars=["Initial", "Bounded", "Unbounded"],
        var_name="type",
        value_name=f"marginal_{metric.lower()}",
    )

    df_plots = df_plots.merge(marginal_metric, on=["channel", "type"])

    # Final join with resp_metric
    df_plots = df_plots.merge(resp_metric, on="type")

    # Combining different dataframes with different metrics
    df_plot_share_spend = df_plots[
        ["channel", "type", "type_lab", "spend_share"]
    ].copy()
    df_plot_share_spend["metric"] = "spend"
    df_plot_share_spend.rename(columns={"spend_share": "values"}, inplace=True)

    df_plot_share_response = df_plots[
        ["channel", "type", "type_lab", "response_share"]
    ].copy()
    df_plot_share_response["metric"] = "response"
    df_plot_share_response.rename(columns={"response_share": "values"}, inplace=True)

    channel_cols = [col for col in df_plots.columns if col.startswith("channel_")]
    df_plot_share_channel = df_plots[
        ["channel", "type", "type_lab"] + channel_cols
    ].copy()
    df_plot_share_channel["metric"] = metric
    df_plot_share_channel.rename(
        columns={col: "values" for col in channel_cols}, inplace=True
    )

    marginal_cols = [col for col in df_plots.columns if col.startswith("marginal_")]
    df_plot_share_marginal = df_plots[
        ["channel", "type", "type_lab"] + marginal_cols
    ].copy()
    df_plot_share_marginal["metric"] = "m" + metric
    df_plot_share_marginal.rename(
        columns={col: "values" for col in marginal_cols}, inplace=True
    )

    df_plot_share = pd.concat(
        [
            df_plot_share_spend,
            df_plot_share_response,
            df_plot_share_channel,
            df_plot_share_marginal,
        ]
    )

    # Additional data manipulation
    df_plot_share["type"] = pd.Categorical(df_plot_share["type"], categories=levs1)
    df_plot_share["name_label"] = pd.Categorical(
        df_plot_share["type"] + "\n" + df_plot_share["metric"],
        categories=[
            t + "\n" + m for t in levs1 for m in df_plot_share["metric"].unique()
        ],
    )

    # Handling extreme values
    df_plot_share["values"] = df_plot_share["values"].replace([np.inf, -np.inf], np.nan)
    df_plot_share["values"] = df_plot_share["values"].fillna(0)
    df_plot_share["values"] = df_plot_share["values"].round(4)

    # Formatting values_label
    def format_values(row):
        if row["metric"] in ["CPA", "mCPA", "ROAS", "mROAS"]:
            return f"{row['values']:.2f}"
        else:
            return f"{100 * row['values']:.1f}%"

    df_plot_share["values_label"] = df_plot_share.apply(format_values, axis=1)
    df_plot_share["values_label"] = df_plot_share["values_label"].replace(
        ["NA", "NaN"], "-"
    )

    # More data manipulation
    df_plot_share["channel"] = pd.Categorical(
        df_plot_share["channel"], categories=reversed(df_plot_share["channel"].unique())
    )
    df_plot_share["metric"] = df_plot_share["metric"].apply(
        lambda x: x + "%" if x in ["spend", "response"] else x
    )
    df_plot_share["metric"] = pd.Categorical(
        df_plot_share["metric"],
        categories=[
            m + p for m in df_plot_share["metric"].unique() for p in ["", "", "%", "%"]
        ],
    )

    # Normalizing values
    def normalize(series):
        min_val = series.min()
        max_val = series.max()
        return (series - min_val) / (max_val - min_val)

    df_plot_share["values_norm"] = df_plot_share.groupby("name_label")[
        "values"
    ].transform(normalize)
    df_plot_share["values_norm"] = df_plot_share["values_norm"].fillna(0)

    p2 = (
        ggplot(df_plot_share, aes(x="metric", y="channel", fill="type"))
        + geom_tile(aes(alpha="values_norm"), color="white")
        + scale_fill_manual(values=["grey50", "steelblue", "darkgoldenrod4"])
        + scale_alpha(range=(0.6, 1))
        + geom_text(aes(label="values_label"), color="black")
        + facet_grid(". ~ type_lab", scales="free")
        # The theme_lares() function is specific to R and doesn't have an exact equivalent in plotnine, so we use a basic theme modification
        + theme(legend_position="none", plot_background=element_blank())
        + labs(
            title="Budget Allocation per Channel*", fill=None, x=None, y="Paid Channels"
        )
    )

    outputs["p2"] = p2

    ## 3. Response curves
    constr_labels = dt_optimOut.copy()
    constr_labels['constr_label'] = constr_labels.apply(lambda x: f"{x['channels']}\n[{x['constr_low']} - {x['constr_up']}] & [{round(x['constr_low_unb'], 1)} - {round(x['constr_up_unb'], 1)}]", axis=1)
    constr_labels = constr_labels.rename(columns={'channels': 'channel'})[['channel', 'constr_label', 'constr_low_abs', 'constr_up_abs', 'constr_low_unb_abs', 'constr_up_unb_abs']]

    # Left join with plotDT_scurve
    plotDT_scurve = eval_list['plotDT_scurve'].merge(constr_labels, on='channel', how='left')

    # Data manipulation for mainPoints
    mainPoints = eval_list['mainPoints'].merge(constr_labels, on='channel', how='left')
    mainPoints = mainPoints.merge(resp_metric, on='type', how='left')

    mainPoints['type'] = mainPoints['type'].fillna('Carryover').astype(str)
    mainPoints['type'] = pd.Categorical(mainPoints['type'], categories=['Carryover'] + levs1)

    mainPoints['type_lab'] = mainPoints['type_lab'].fillna('Carryover').astype(str)
    mainPoints['type_lab'] = pd.Categorical(mainPoints['type_lab'], categories=['Carryover'] + levs2)

    caov_points = mainPoints[mainPoints['type'] == 'Carryover'][['channel', 'spend_point']].rename(columns={'spend_point': 'caov_spend'})

    # Left join and mutate mainPoints with caov_points
    mainPoints = mainPoints.merge(caov_points, on='channel', how='left')

    def calculate_abs(row, type_index, constr_type):
        if row['type'] == levs1[type_index]:
            return row[f'{constr_type}_abs'] + row['caov_spend']
        else:
            return None

    mainPoints['constr_low_abs'] = mainPoints.apply(lambda row: calculate_abs(row, 1, 'constr_low'), axis=1)
    mainPoints['constr_up_abs'] = mainPoints.apply(lambda row: calculate_abs(row, 1, 'constr_up'), axis=1)
    mainPoints['constr_low_unb_abs'] = mainPoints.apply(lambda row: calculate_abs(row, 2, 'constr_low_unb'), axis=1)
    mainPoints['constr_up_unb_abs'] = mainPoints.apply(lambda row: calculate_abs(row, 2, 'constr_up_unb'), axis=1)

    mainPoints['plot_lb'] = mainPoints.apply(lambda row: row['constr_low_unb_abs'] if pd.isna(row['constr_low_abs']) else row['constr_low_abs'], axis=1)
    mainPoints['plot_ub'] = mainPoints.apply(lambda row: row['constr_up_unb_abs'] if pd.isna(row['constr_up_abs']) else row['constr_up_abs'], axis=1)

    # Creating caption
    caption_parts = [
        f" Given the upper/lower constrains, the total budget ({eval_list['total_budget']}) can't be fully allocated \n" if any_topped else "",
        f"* {formulax1}\n",
        f"* {formulax2}\n",
        "** Dotted lines show budget optimization lower-upper ranges per media"
    ]
    caption = "".join(part for part in caption_parts if part.strip())

    p3 = (
        ggplot(plotDT_scurve)
        + scale_x_continuous(labels=scientific_format())  # scale_x_abbr() is not directly available in plotnine
        + scale_y_continuous(labels=scientific_format())  # scale_y_abbr() is not directly available in plotnine
        + geom_line(aes(x='spend', y='total_response'), show_legend=False, size=0.5)
        + facet_wrap('constr_label', scales='free', ncol=3)
        + geom_area(
            data=plotDT_scurve[plotDT_scurve['spend'] <= plotDT_scurve['mean_carryover']],
            mapping=aes('spend', 'total_response', color='constr_label'),
            stat='identity', position='stack', size=0.1,
            fill='grey50', alpha=0.4, show_legend=False
        )
        + geom_errorbar(
            data=mainPoints[~mainPoints['constr_label'].isna()],
            mapping=aes(x='spend_point', y='response_point', xmin='plot_lb', xmax='plot_ub'),
            color='black', linetype='dotted'
        )
        + geom_point(
            data=mainPoints[~mainPoints['plot_lb'].isna() & ~mainPoints['mean_spend'].isna()],
            mapping=aes(x='plot_lb', y='response_point'), shape=18
        )
        + geom_point(
            data=mainPoints[~mainPoints['plot_ub'].isna() & ~mainPoints['mean_spend'].isna()],
            mapping=aes(x='plot_ub', y='response_point'), shape=18
        )
        + geom_point(
            data=mainPoints[~mainPoints['constr_label'].isna()],
            mapping=aes(x='spend_point', y='response_point', fill='type_lab'),
            size=2.5, shape=21
        )
        + scale_fill_manual(values=["white", "grey", "steelblue", "darkgoldenrod4"])
        # The theme_lares() function is specific to R and doesn't have an exact equivalent in plotnine, so we use a basic theme modification
        + theme(legend_position='top', plot_background=element_blank())
        + labs(
            title="Simulated Response Curve for Selected Allocation Period",
            x=f"Spend** per {InputCollect['intervalType']} (Mean Adstock Zone in Grey)",
            y=f"Total Response [{InputCollect['dep_var_type']}]",
            caption=caption
        )
    )

    outputs['p3'] = p3

    min_period_loc = dt_optimOut['periods'].astype(str).apply(lambda x: x.split(' ')[0]).astype(int).idxmin()
    subtitle = f"{errors}\nSimulation date range: {dt_optimOut.loc[0, 'date_min']} to {dt_optimOut.loc[0, 'date_max']} ({dt_optimOut.loc[min_period_loc, 'periods']}) | Scenario: {scenario}"

    # Calculate the heights for the subplots based on the data
    heights = [
        0.8,
        0.2 + len(dt_optimOut['channels']) * 0.2,
        np.ceil(len(dt_optimOut['channels']) / 3)
    ]

    # Create a matplotlib figure with subplots
    fig, axs = plt.subplots(3, 1, gridspec_kw={'height_ratios': heights}, figsize=(10, 15))

    # Draw the plots on the subplots
    draw(p1, ax=axs[0])
    draw(p2, ax=axs[1])
    draw(p3, ax=axs[2])

    # Set the title and subtitle
    axs[0].set_title(f"Budget Allocation Onepager for Model ID {select_model}", fontsize=16)
    axs[0].set_xlabel(subtitle, fontsize=12)

    if export:
        # Determine the file suffix based on conditions
        if scenario == "max_response" and metric == "ROAS":
            suffix = "best_roas"
        elif scenario == "max_response" and metric == "CPA":
            suffix = "best_cpa"
        elif scenario == "target_efficiency" and metric == "ROAS":
            suffix = "target_roas"
        elif scenario == "target_efficiency" and metric == "CPA":
            suffix = "target_cpa"
        else:
            suffix = "none"

        # Constructing the filename
        filename = os.path.join(plot_folder, f"{select_model}_reallocated_{suffix}.png")

        # Saving the figure
        fig.set_size_inches(12, 10 + 2 * np.ceil(len(dt_optimOut['channels']) / 3))
        fig.savefig(filename, dpi=350)

        if not quiet:
            print(f"Exporting to: {filename}")

    return outputs


def refresh_plots(InputCollectRF, OutputCollectRF, ReportCollect, export=True, **kwargs):
    selectID = ReportCollect['selectIDs'][-1] if ReportCollect['selectIDs'] else ReportCollect['resultHypParamReport']['solID'][-1]
    print(f">> Plotting refresh results for model: {selectID}")

    # Assuming ReportCollect contains DataFrames for xDecompVecReport and xDecompAggReport
    xDecompVecReport = ReportCollect['xDecompVecReport'][ReportCollect['xDecompVecReport']['solID'] == selectID]
    xDecompAggReport = ReportCollect['xDecompAggReport'][ReportCollect['xDecompAggReport']['solID'] == selectID]

    plot_folder = OutputCollectRF['plot_folder']
    outputs = {}

    xDecompVecReportPlot = xDecompVecReport.copy()
    xDecompVecReportPlot['refreshStart'] = xDecompVecReportPlot.groupby('refreshStatus')['ds'].transform('min')
    xDecompVecReportPlot['refreshEnd'] = xDecompVecReportPlot.groupby('refreshStatus')['ds'].transform('max')
    xDecompVecReportPlot['duration'] = (
        (xDecompVecReportPlot['refreshEnd'].astype('datetime64') - xDecompVecReportPlot['refreshStart'].astype('datetime64')).dt.days
        + InputCollectRF['dayInterval']
    ) / InputCollectRF['dayInterval']

    dt_refreshDates = xDecompVecReportPlot[['refreshStatus', 'refreshStart', 'refreshEnd', 'duration']].drop_duplicates()
    dt_refreshDates['label'] = dt_refreshDates.apply(
        lambda x: f"Initial: {x['refreshStart']}, {x['duration']} {InputCollectRF['intervalType']}s"
        if x['refreshStatus'] == 0
        else f"Refresh #{x['refreshStatus']}: {x['refreshStart']}, {x['duration']} {InputCollectRF['intervalType']}s",
        axis=1
    )

    xDecompVecReportMelted = xDecompVecReportPlot.melt(
        id_vars=['ds', 'refreshStatus', 'refreshStart', 'refreshEnd'],
        value_vars=['dep_var', 'depVarHat'],
        var_name='variable', value_name='value'
    )
    xDecompVecReportMelted['variable'] = xDecompVecReportMelted['variable'].replace({'dep_var': 'actual', 'depVarHat': 'prediction'})

    # Function to calculate R-squared
    def get_rsq(true, predicted):
        residual_sum_of_squares = ((true - predicted) ** 2).sum()
        total_sum_of_squares = ((true - true.mean()) ** 2).sum()
        return 1 - residual_sum_of_squares / total_sum_of_squares

    # Creating the plot
    pFitRF = (
        ggplot(xDecompVecReportMelted, aes(x='ds', y='value', color='variable'))
        + geom_line()
        + geom_rect(
            data=dt_refreshDates,
            mapping=aes(xmin='refreshStart', xmax='refreshEnd', fill='refreshStatus.astype(str)'),
            ymin=float('-inf'), ymax=float('inf'), alpha=0.2
        )
        # Theme customization
        + theme(
            panel_grid_major=element_blank(),
            panel_grid_minor=element_blank(),
            panel_background=element_blank(),
            legend_background=element_rect(fill='white', alpha=0.4)
        )
        # Additional elements
        + scale_fill_brewer(palette="BuGn")
        + geom_text(
            data=dt_refreshDates, mapping=aes(x='refreshStart', y=xDecompVecReportMelted['value'].max(),
                                            label='label'),
            angle=270, hjust=-0.1, vjust=-0.2, color="gray40"
        )
        + labs(
            title="Model Refresh: Actual vs. Predicted Response",
            subtitle=f"Assembled R2: {round(get_rsq(xDecompVecReportPlot['dep_var'], xDecompVecReportPlot['depVarHat']), 2)}",
            x="Date", y="Response", fill="Refresh", color="Type"
        )
        + scale_y_continuous(labels=scientific_format())
    )

    outputs['pFitRF'] = pFitRF

    if export:
        # Construct the filename
        filename = os.path.join(plot_folder, "report_actual_fitted.png")

        # Set the size of the plot
        pFitRF.save(filename, dpi=900, width=12, height=8, limitsize=False)

        print(f"Plot saved to {filename}")

    xDecompAggReportPlotBase = xDecompAggReport[
        xDecompAggReport['rn'].isin(InputCollectRF['prophet_vars'] + ["(Intercept)"])
    ].copy()
    xDecompAggReportPlotBase['perc'] = xDecompAggReportPlotBase.apply(
        lambda x: x['xDecompPerc'] if x['refreshStatus'] == 0 else x['xDecompPercRF'], axis=1
    )
    xDecompAggReportPlotBase = xDecompAggReportPlotBase.groupby('refreshStatus').agg(
        variable=('rn', lambda x: "baseline"),
        percentage=('perc', 'sum'),
        roi_total=('perc', lambda _: float('nan'))  # NA equivalent in Python
    )

    xDecompAggReportPlot = xDecompAggReport[
        ~xDecompAggReport['rn'].isin(InputCollectRF['prophet_vars'] + ["(Intercept)"])
    ].copy()
    xDecompAggReportPlot['percentage'] = xDecompAggReportPlot.apply(
        lambda x: x['xDecompPerc'] if x['refreshStatus'] == 0 else x['xDecompPercRF'], axis=1
    )
    xDecompAggReportPlot = xDecompAggReportPlot[['refreshStatus', 'rn', 'percentage', 'roi_total']]
    xDecompAggReportPlot = xDecompAggReportPlot.rename(columns={'rn': 'variable'})
    xDecompAggReportPlot = pd.concat([xDecompAggReportPlot, xDecompAggReportPlotBase])
    xDecompAggReportPlot = xDecompAggReportPlot.sort_values(by=['refreshStatus', 'variable'], ascending=[True, False])
    xDecompAggReportPlot['refreshStatus'] = xDecompAggReportPlot['refreshStatus'].apply(
        lambda x: "Init.mod" if x == 0 else f"Refresh{x}"
    )

    # Calculating scaling factor and max y-axis limit for the plot
    ySecScale = 0.75 * max(xDecompAggReportPlot['roi_total'].dropna() / xDecompAggReportPlot['percentage'].max())
    ymax = 1.1 * max(max(xDecompAggReportPlot['roi_total'].dropna() / ySecScale), xDecompAggReportPlot['percentage'].max())

    pBarRF = (
        ggplot(xDecompAggReportPlot, aes(x='variable', y='percentage', fill='variable'))
        + geom_bar(alpha=0.8, position='dodge', stat='identity', na_rm=True)
        + facet_wrap('~refreshStatus', scales='free')
        # The theme_lares() function is specific to R and doesn't have an exact equivalent in plotnine, so we use a basic theme modification
        + theme(legend_position='none', axis_text_x=element_blank(), axis_ticks_x=element_blank())
        + scale_fill_manual(values=robyn_palette()['fill'])
        + geom_text(aes(label=f"{round('percentage' * 100, 1)}%"), size=3, na_rm=True, position=position_dodge(width=0.9), hjust=-0.25)
        + geom_point(aes(x='variable', y='roi_total' / ySecScale, color='variable'), size=4, shape=17, na_rm=True)
        + geom_text(aes(label=round('roi_total', 2), x='variable', y='roi_total' / ySecScale), size=3, na_rm=True, hjust=-0.4, fontface='bold', position=position_dodge(width=0.9))
        + scale_color_manual(values=robyn_palette()['fill'])
        + scale_y_continuous(sec_axis=sec_axis(lambda x: x * ySecScale), breaks=range(0, int(ymax) + 1, 2), limits=(0, ymax), name='Total ROI')
        + coord_flip()
        + labs(
            title="Model Refresh: Decomposition & Paid Media ROI",
            subtitle="Baseline includes intercept and prophet vars: " + ', '.join(InputCollectRF['prophet_vars'])
        )
    )

    outputs['pBarRF'] = pBarRF

    # Exporting the plot if required
    if export:
        filename = os.path.join(plot_folder, "report_decomposition.png")
        pBarRF.save(filename, dpi=900, width=12, height=8, limitsize=False)
        print(f"Plot saved to {filename}")

    return outputs


def refresh_plots_json(output_collect_rf, json_file, export=True, **kwargs):
    with open(json_file, 'r') as file:
        chain_data = json.load(file)

    sol_id = list(chain_data.keys())[-1]
    day_interval = chain_data[sol_id]['InputCollect']['dayInterval']
    interval_type = chain_data[sol_id]['InputCollect']['intervalType']
    rsq = chain_data[sol_id]['ExportedModel']['errors']['rsq_train']
    plot_folder = output_collect_rf['plot_folder']

    # 1. Fitted vs actual
    temp = output_collect_rf['allPareto']['plotDataCollect'][sol_id]
    x_decomp_vec_plot_melted = pd.DataFrame(temp['plot5data']['xDecompVecPlotMelted'])
    x_decomp_vec_plot_melted['linetype'] = x_decomp_vec_plot_melted['variable'].apply(
        lambda x: 'solid' if x == 'predicted' else 'dotted')
    x_decomp_vec_plot_melted['variable'] = x_decomp_vec_plot_melted['variable'].str.title()
    x_decomp_vec_plot_melted['ds'] = pd.to_datetime(x_decomp_vec_plot_melted['ds'], origin='1970-01-01', unit='D')

    # Creating dt_refreshDates dataframe
    def extract_dates(data):
        return {
            'window_start': pd.to_datetime(data['InputCollect']['window_start'], origin='1970-01-01', unit='D'),
            'window_end': pd.to_datetime(data['InputCollect']['window_end'], origin='1970-01-01', unit='D'),
            'duration': data['InputCollect']['refresh_steps']
        }

    dt_refresh_dates = pd.DataFrame({key: extract_dates(value) for key, value in chain_data.items()}).T
    dt_refresh_dates['solID'] = dt_refresh_dates.index
    dt_refresh_dates = dt_refresh_dates[dt_refresh_dates['duration'] > 0]
    dt_refresh_dates['refreshStatus'] = dt_refresh_dates.reset_index().index + 1
    dt_refresh_dates['refreshStart'] = dt_refresh_dates['window_end'] - pd.to_timedelta(dt_refresh_dates['duration'] * day_interval, unit='D')
    dt_refresh_dates['refreshEnd'] = dt_refresh_dates['window_end']

    def create_label(row):
        if row['refreshStatus'] == 0:
            return f"Initial: {row['refreshStart'].date()}, {row['duration']} {interval_type}s"
        else:
            return f"Refresh #{row['refreshStatus']}: {row['refreshStart'].date()}, {row['duration']} {interval_type}s"

    dt_refresh_dates['label'] = dt_refresh_dates.apply(create_label, axis=1)
    dt_refresh_dates = dt_refresh_dates.reset_index(drop=True)

    pFitRF = (ggplot(x_decomp_vec_plot_melted, aes(x='ds', y='value', color='variable', linetype='linetype'))
            + geom_path(size=0.6)
            + geom_rect(data=dt_refresh_dates, mapping=aes(xmin='refreshStart', xmax='refreshEnd', fill='refreshStatus'),
                        ymin=float('-inf'), ymax=float('inf'), alpha=0.2)
            + scale_fill_brewer(palette='BuGn')
            + geom_text(data=dt_refresh_dates, mapping=aes(x='refreshStart', y=x_decomp_vec_plot_melted['value'].max(),
                                                            label='label'), angle=270, hjust=0, vjust=-0.2, color='gray40')
            # The theme_lares is specific to ggplot2, you might need to customize this using theme() in plotnine
            + theme(figure_size=(12, 8), subplots_adjust={'right': 0.85})
            + scale_y_continuous(labels='comma')
            + guides(linetype='none', fill='none')
            + labs(title='Actual vs. Predicted Response', x='Date', y='Response'))

    # Save the plot
    if export:
        ggsave(plot=pFitRF, filename=f'{plot_folder}/report_actual_fitted.png', dpi=300, width=12, height=8, limitsize=False)

    # 2. Stacked bar plot preparation
    df_list = [chain_data[key]['ExportedModel']['summary'].assign(solID=key) for key in chain_data.keys()]
    df = pd.concat(df_list)

    # Adjusting the DataFrame according to the R code logic
    df['solID'] = pd.Categorical(df['solID'], categories=list(chain_data.keys()))
    df['label'] = [f"{sid} [{int(sid) - 1}]" for sid in df['solID']]
    label_levels = [f"{name} [{i}]" for i, name in enumerate(chain_data.keys())]
    df['label'] = pd.Categorical(df['label'], categories=label_levels)

    # Assuming 'prophet_vars' is a list of variables
    prophet_vars = chain_data[list(chain_data.keys())[0]]['InputCollect']['prophet_vars']
    df['variable'] = df['variable'].apply(lambda x: 'baseline' if x in prophet_vars or x == '(Intercept)' else x)

    # Summarizing the data
    df_grouped = df.groupby(['solID', 'label', 'variable']).sum().reset_index()

    # Function to format numbers, similar to formatNum in R
    def format_num(value, digits=2, suffix=""):
        formatted = f"{value:.{digits}f}"
        if suffix:
            formatted += suffix
        return formatted

    # Plotting
    pBarRF = (ggplot(df, aes(y='variable'))
            + geom_col(aes(x='decompPer'))
            + geom_text(aes(x='decompPer', label=df['decompPer'].apply(format_num, args=(2, '%'))),
                        na_rm=True, hjust=-0.2, size=8)  # Adjusted text size for visibility
            + geom_point(aes(x='performance'), na_rm=True, size=2, colour="#39638b")
            + geom_text(aes(x='performance', label=df['performance'].apply(format_num, args=(2,))),
                        na_rm=True, hjust=-0.4, size=8, colour="#39638b")  # Adjusted text size for visibility
            + facet_wrap('~label', scales='free')
            # Uncomment the next line if scale_x_percent is needed (custom implementation may be required)
            # + scale_x_percent(limits=(0, df['performance'].max() * 1.2))
            + labs(title="Model refresh: Decomposition & Paid Media",
                    subtitle="Baseline includes intercept and all prophet vars: " + ', '.join(prophet_vars),
                    x=None, y=None)
            + theme(figure_size=(12, 8), subplots_adjust={'right': 0.85},
                    axis_text_x=element_blank(), axis_ticks_x=element_blank()))

    if export:
        # Constructing the filename
        last_chain_data = chainData[list(chainData.keys())[-1]]['ExportedModel']
        plot_filename = f"{last_chain_data['plot_folder']}report_decomposition.png"

        # Saving the plot
        ggsave(plot=pBarRF, filename=plot_filename, dpi=300, width=12, height=8, limitsize=False)

    # Prepare the outputs dictionary, if needed
    outputs = {'pBarRF': pBarRF}

    # Return the outputs
    return outputs

####################################################################
#' Generate Plots for Time-Series Validation
#'
#' Create a plot to visualize the convergence for each of the datasets
#' when time-series validation is enabled when running \code{robyn_run()}.
#' As a reference, the closer the test and validation convergence points are,
#' the better, given the time-series wasn't overfitted.
#'
#' @rdname robyn_outputs
#' @return Invisible list with \code{ggplot} plots.
#' @export
def ts_validation_fun(output_models, quiet=False, **kwargs):
    filtered_models = output_models["trials"]

    result_hyp_param = pd.concat([model['resultCollect']['resultHypParam'] for model in filtered_models])
    result_hyp_param['i'] = result_hyp_param.groupby('trial').cumcount() + 1
    result_hyp_param.reset_index(drop=True, inplace=True)

    # selected_columns = result_hyp_param[['solID', 'i', 'trial', 'train_size'] + \
    #                                     [col for col in result_hyp_param if col.startswith('rsq_')]]
    # selected_columns['trial'] = selected_columns['trial'].apply(lambda x: f"Trial {x}")

    # # Pivot longer/melt the DataFrame
    # rsq_long = pd.melt(selected_columns, id_vars=['solID', 'i', 'trial', 'train_size'],
    #                    value_vars=[col for col in selected_columns if col.startswith('rsq_')],
    #                    var_name='dataset', value_name='rsq')

    # nrmse_cols = pd.melt(result_hyp_param[['solID'] + [col for col in result_hyp_param if col.startswith('nrmse_')]],
    #                      id_vars=['solID'], value_vars=[col for col in result_hyp_param if col.startswith('nrmse_')],
    #                      var_name='del', value_name='nrmse').drop(columns=['del'])


    # rsq_long_indexed = rsq_long.set_index(['solID', 'i', 'trial', 'dataset'])
    # nrmse_cols_indexed = nrmse_cols.set_index(['solID'])  # Adjust as necessary
    # result_hyp_param_long = pd.concat([rsq_long_indexed, nrmse_cols_indexed], axis=1, join='inner').reset_index()

    rsq_long = pd.melt(
        result_hyp_param[['solID', 'i', 'trial', 'train_size'] + [col for col in result_hyp_param if col.startswith('rsq_')]],
        id_vars=['solID', 'i', 'trial', 'train_size'],
        var_name='dataset',
        value_name='rsq'
    )
    rsq_long['trial'] = rsq_long['trial'].apply(lambda x: f"Trial {x}")
    rsq_long['dataset'] = rsq_long['dataset'].str.replace('rsq_', '')
    print(rsq_long['rsq'])
    rsq_long['rsq'] = winsorize(rsq_long['rsq'], limits=[0.01, 0.99])

    nrmse_long = pd.melt(
        result_hyp_param[['solID'] + [col for col in result_hyp_param if col.startswith('nrmse_')]],
        id_vars=['solID'],
        var_name='dataset',
        value_name='nrmse'
    )
    nrmse_long['dataset'] = nrmse_long['dataset'].str.replace('nrmse_', '')
    nrmse_long['nrmse'] = winsorize(nrmse_long['nrmse'], limits=[0.00, 0.99])

    result_hyp_param_long = pd.merge(rsq_long, nrmse_long[['solID', 'dataset', 'nrmse']], on=['solID', 'dataset'], how='left')
    result_hyp_param_long['dataset'] = result_hyp_param_long['dataset'].str.replace('rsq_', '')

    sns.set_theme(style="whitegrid")

    # Create a figure with a specific size
    fig, axs = plt.subplots(1, 2, figsize=(14, 6))

    # First plot
    sns.scatterplot(x='i', y='train_size', data=result_hyp_param, color='black', alpha=0.5, s=12, ax=axs[0])
    axs[0].set_xlabel('Iteration')
    axs[0].set_ylabel('Train Size')

    # Second plot
    sns.scatterplot(x='i', y='nrmse', hue='dataset', data=result_hyp_param_long, alpha=0.2, s=9, ax=axs[1])
    axs[1].axhline(0, color='gray', linestyle='--')
    axs[1].set_xlabel('Iteration')
    axs[1].set_ylabel('NRMSE [Upper 1% Winsorized]')
    axs[1].legend(title='Dataset', loc='upper right')

    # Adjust the layout
    plt.tight_layout()
    fig.suptitle("Time-series validation & Convergence", fontsize=16)
    plt.subplots_adjust(top=0.88)

    # Instead of displaying the plot, return the figure and axes objects
    return fig, axs

# def ts_validation_fun(output_models, quiet=False, **kwargs):
#     # if not output_models.get('ts_validation', False):
#     #     return None

#     # Extracting the relevant trials
#     trial_names = [f"trial{i}" for i in range(1, output_models['trials'] + 1)]
#     relevant_trials = [output_models[name]['resultCollect']['resultHypParam'] for name in trial_names if name in output_models]

#     # Binding rows and processing
#     result_hyp_param = pd.concat(relevant_trials, ignore_index=True)
#     result_hyp_param['i'] = result_hyp_param.groupby('trial').cumcount() + 1

#     # Data manipulation
#     result_hyp_param_long = result_hyp_param.copy()
#     result_hyp_param_long = result_hyp_param_long.filter(regex='^rsq_|solID|i|trial|train_size$')
#     result_hyp_param_long['trial'] = 'Trial ' + result_hyp_param_long['trial'].astype(str)
#     rsq_cols = result_hyp_param_long.filter(regex='^rsq_').columns
#     nrmse_cols = result_hyp_param.filter(regex='^nrmse_').columns

#     # Melting the DataFrame
#     rsq_melted = result_hyp_param_long.melt(id_vars=['solID', 'i', 'trial', 'train_size'],
#                                         value_vars=rsq_cols,
#                                         var_name='dataset', value_name='rsq')
#     nrmse_melted = result_hyp_param.melt(id_vars=['solID'],
#                                         value_vars=nrmse_cols,
#                                         var_name='del', value_name='nrmse').drop(columns=['del'])

#     # Combining melted dataframes
#     result_hyp_param_long = pd.concat([rsq_melted, nrmse_melted['nrmse']], axis=1)

#     # Winsorizing and final adjustments
#     result_hyp_param_long['rsq'] = winsorize(result_hyp_param_long['rsq'], limits=[0.01, 0.99])
#     result_hyp_param_long['nrmse'] = winsorize(result_hyp_param_long['nrmse'], limits=[0.00, 0.99])
#     result_hyp_param_long['dataset'] = result_hyp_param_long['dataset'].str.replace('rsq_', '')

#     pIters = (ggplot(result_hyp_param, aes(x='i', y='train_size'))
#             + geom_point(fill='black', alpha=0.5, size=1.2, shape=23)
#             # Uncomment the next line if geom_smooth is required
#             # + geom_smooth()
#             + labs(y='Train Size', x='Iteration')
#             # Uncomment and adjust the next line if scale_y_percent and scale_x_abbr are required
#             # + scale_y_continuous(labels='percent') + scale_x_continuous(labels='abbr')
#             + theme(figure_size=(10, 6), plot_background=element_rect(fill='white')))

#     pNRMSE = (ggplot(result_hyp_param_long, aes(x='i', y='nrmse', color='dataset'))
#             + geom_point(alpha=0.2, size=0.9)
#             + geom_smooth(method='gamm', method_args={'formula': 'y ~ s(x, bs="cs")'})
#             + facet_grid('trial ~ .')
#             + geom_hline(yintercept=0, linetype='dashed')
#             + labs(y='NRMSE [Upper 1% Winsorized]', x='Iteration', color='Dataset')
#             # The theme_lares is specific to ggplot2, you might need to customize this using theme() in plotnine
#             + theme(figure_size=(10, 6), plot_background=element_rect(fill='white'))
#             # Uncomment and adjust the next line if scale_x_abbr is required
#             # + scale_x_continuous(labels='abbr')
#             )

#     if export:
#         ggsave(plot=pNRMSE, filename=f'{plot_folder}/pNRMSE.png', dpi=300, width=12, height=8, limitsize=False)
#         ggsave(plot=pIters, filename=f'{plot_folder}/pIters.png', dpi=300, width=12, height=8, limitsize=False)

#     # Return the plots separately
#     return {'pNRMSE': pNRMSE, 'pIters': pIters}

#' @rdname robyn_outputs
#' @param solID Character vector. Model IDs to plot.
#' @param exclude Character vector. Manually exclude variables from plot.
#' @export
def decomp_plot(input_collect, output_collect, sol_id=None, exclude=None):
    # Check options - Implement this as needed based on your application
    # check_opts(sol_id, output_collect['allSolutions'])

    # String manipulation for interval type and variable type
    int_type = input_collect['intervalType'].title()
    if int_type in ['Month', 'Week']:
        int_type += 'ly'
    elif int_type == 'Day':
        int_type = 'Daily'

    var_type = input_collect['dep_var_type'].title()

    pal = plt.cm.get_cmap('viridis').colors

    # Data manipulation
    df = output_collect['xDecompVecCollect']
    df = df[df['solID'].isin(sol_id)]
    df = pd.melt(df, id_vars=['solID', 'ds', 'dep_var'], var_name='variable', value_name='value')
    if exclude:
        df = df[~df['variable'].isin(exclude)]
    df['variable'] = pd.Categorical(df['variable'], categories=reversed(df['variable'].unique()), ordered=True)

    # Plotting
    p = (ggplot(df, aes(x='ds', y='value', fill='variable'))
         + facet_grid('solID ~ .')
         + labs(title=f'{var_type} Decomposition by Variable', x=None, y=f'{int_type} {var_type}', fill=None)
         + geom_area()
         + theme(figure_size=(10, 6), plot_background=element_rect(fill='white'), legend_position='right')
         + scale_fill_manual(values=pal[:len(df['variable'].unique())])
         # Adjust scale_y_continuous if needed to mimic scale_y_abbr in R
         + scale_y_continuous(labels='comma'))

    return p

================
File: refresh.py
================
# Copyright (c) Meta Platforms, Inc. and its affiliates.

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

####################################################################
import matplotlib as plt

import pandas as pd
import numpy as np

def robyn_refresh(json_file=None, robyn_object=None, dt_input=None, dt_holidays=None, refresh_steps=4, refresh_mode='manual', refresh_iters=1000, refresh_trials=3, plot_folder=None, plot_pareto=True, version_prompt=False, export=True, calibration_input=None, objective_weights=None, **kwargs):
    refresh_control = True
    while refresh_control:
        # Check for NA values
        check_nas(dt_input)
        check_nas(dt_holidays)

        # Load initial model
        if not json_file:
            robyn = list()
            json = robyn_read(json_file, step=2, quiet=True)
            if not plot_folder:
                plot_folder = json['ExportedModel']['plot_folder']
            list_init = robyn_recreate(
                json_file=json_file,
                dt_input=dt_input,
                dt_holidays=dt_holidays,
                plot_folder=plot_folder,
                quiet=False,
                **kwargs
            )
            list_init['InputCollect']['refreshSourceID'] = json['ExportedModel']['select_model']
            chain_data = robyn_chain(json_file)
            list_init['InputCollect']['refreshChain'] = attr(chain_data, 'chain')
            list_init['InputCollect']['refreshDepth'] = refresh_depth = len(attr(chain_data, 'chain'))
            list_init['OutputCollect']['hyper_updated'] = json['ExportedModel']['hyper_updated']
            robyn = [list_init]
            refresh_counter = 1  # Dummy for now (legacy)
        else:
            robyn_imported = robyn_load(robyn_object)
            robyn = robyn_imported['Robyn']
            plot_folder = robyn_imported['objectPath']
            robyn_object = robyn_imported['robyn_object']
            refresh_counter = len(robyn) - sum(names(robyn) == 'refresh')
            refresh_depth = None  # Dummy for now (legacy)

        depth = refresh_counter if refresh_counter == 1 else c('listInit', paste0('listRefresh', 1, refresh_counter - 1))

        # Check rule of thumb: 50% of data shouldn't be new
        check_refresh_data(robyn, dt_input)

        # Get previous data
        if refresh_counter == 1:
            input_collect_rf = robyn['listInit']['InputCollect']
            list_output_prev = robyn['listInit']['OutputCollect']
            input_collect_rf['xDecompAggPrev'] = list_output_prev['xDecompAgg']
            if len(unique(robyn['listInit']['OutputCollect']['resultHypParam']['solID'])) > 1:
                stop("Run robyn_write() first to select and export any Robyn model")
        else:
            list_name = paste0('listRefresh', refresh_counter - 1)
            input_collect_rf = robyn[list_name]['InputCollect']
            list_output_prev = robyn[list_name]['OutputCollect']
            list_report_prev = robyn[list_name]['ReportCollect']
            # Model selection from previous build (new normalization range for error_score)
            if not 'error_score' in names(list_output_prev['resultHypParam']):
                list_output_prev['resultHypParam'] = pd.DataFrame(list_output_prev['resultHypParam'])
                list_output_prev['resultHypParam']['error_score'] = errors_scores(list_output_prev['resultHypParam'], ts_validation=list_output_prev['OutputModels']['ts_validation'], **kwargs)
            which_best_mod_rf = which.min(list_output_prev['resultHypParam']['error_score'])[1]
            list_output_prev['resultHypParam'] = list_output_prev['resultHypParam'][which_best_mod_rf, ]
            list_output_prev['xDecompAgg'] = list_output_prev['xDecompAgg'][which_best_mod_rf, ]
            list_output_prev['mediaVecCollect'] = list_output_prev['mediaVecCollect'][which_best_mod_rf, ]
            list_output_prev['xDecompVecCollect'] = list_output_prev['xDecompVecCollect'][which_best_mod_rf, ]

        # Update refresh model parameters
        input_collect_rf['refreshCounter'] = refresh_counter
        input_collect_rf['refresh_steps'] = refresh_steps
        if True:
            dt_input = pd.DataFrame(dt_input)
            date_input = check_datevar(dt_input, input_collect_rf['date_var'])
            dt_input = date_input['dt_input']  # sort date by ascending
            input_collect_rf['dt_input'] = dt_input
            dt_holidays = pd.DataFrame(dt_holidays)
            input_collect_rf['dt_holidays'] = dt_holidays

        # Load new data
        if True:
            dt_input = pd.DataFrame(dt_input)
            date_input = check_datevar(dt_input, input_collect_rf['date_var'])
            dt_input = date_input['dt_input']  # sort date by ascending
            input_collect_rf['dt_input'] = dt_input
            dt_holidays = pd.DataFrame(dt_holidays)
            input_collect_rf['dt_holidays'] = dt_holidays

        # Refresh rolling window
        if True:
            input_collect_rf['refreshAddedStart'] = pd.DataFrame(input_collect_rf['window_end']) + input_collect_rf['dayInterval']
            total_dates = pd.DataFrame(dt_input[input_collect_rf['date_var']])
            refresh_start = input_collect_rf['window_start'] = pd.DataFrame(input_collect_rf['window_start']) + input_collect_rf['dayInterval'] * refresh_steps
            refresh_start_which = input_collect_rf['rollingWindowStartWhich'] = which.min(abs(pd.DataFrame(total_dates - refresh_start)))
            refresh_end = input_collect_rf['window_end'] = pd.DataFrame(input_collect_rf['window_end']) + input_collect_rf['dayInterval'] * refresh_steps
            refresh_end_which = input_collect_rf['rollingWindowEndWhich'] = which.min(abs(pd.DataFrame(total_dates - refresh_end)))
            input_collect_rf['rollingWindowLength'] = refresh_end_which - refresh_start_which + 1

        if refresh_end > max(total_dates):
            raise ValueError("Not enough data for this refresh. Input data from date {} or later required".format(refresh_end))

        if json_file is not None and refresh_mode == "auto":
            print("Input 'refresh_mode' = 'auto' has been deprecated. Changed to 'manual'")
            refresh_mode = "manual"

        if refresh_mode == "manual":
            refresh_looper = 1
            print("Building refresh model #{} in {} mode".format(depth, refresh_mode))
            refresh_control = False
        else:
            refresh_looper = int(np.floor(np.abs(difftime(max(total_dates), refresh_end, units="days")) / (InputCollectRF.day_interval / refresh_steps)))
            print("Building refresh model #{} in {} mode. {} more to go...".format(depth, refresh_mode, refresh_looper))

        # Update refresh model parameters
        if calibration_input is not None:
            calibration_input = pd.concat([InputCollectRF.calibration_input, calibration_input], ignore_index=True)
            calibration_input = check_calibration(dt_input=InputCollectRF.dt_input, date_var=InputCollectRF.date_var, calibration_input=calibration_input, day_interval=InputCollectRF.day_interval, dep_var=InputCollectRF.dep_var, window_start=InputCollectRF.window_start, window_end=InputCollectRF.window_end, paid_media_spends=InputCollectRF.paid_media_spends, organic_vars=InputCollectRF.organic_vars)
            InputCollectRF.calibration_input = calibration_input

        # Refresh hyperparameter bounds
        InputCollectRF.hyperparameters = refresh_hyps(init_bounds=Robyn.list_init.OutputCollect.hyper_updated, list_output_prev, refresh_steps, rolling_window_length=InputCollectRF.rolling_window_length)

        # Feature engineering for refreshed data
        InputCollectRF = robyn_engineering(InputCollectRF, **kwargs)

        # Refresh model with adjusted decomp.rssd
        OutputModelsRF = robyn_run(InputCollect=InputCollectRF, iterations=refresh_iters, trials=refresh_trials, refresh=True, add_penalty_factor=list_output_prev["add_penalty_factor"], **kwargs)

        OutputCollectRF = robyn_outputs(InputCollectRF, OutputModelsRF, plot_folder=plot_folder, calibration_constraint=rf_cal_constr, export=export, plot_pareto=plot_pareto, objective_weights=objective_weights, **kwargs)

        # Select winner model for current refresh
        OutputCollectRF.result_hyp_param = OutputCollectRF.result_hyp_param.sort_values(by="error_score", ascending=False)

        best_mod = OutputCollectRF.result_hyp_param.iloc[0, 0]
        select_id = None

        while select_id is None or not select_id.isin(OutputCollectRF.all_solutions):
            if version_prompt:
                select_id = input("Input model ID to use for the refresh: ")
                print("Selected model ID: {} for refresh model #{} based on your input".format(select_id, depth))
                if not select_id.isin(OutputCollectRF.all_solutions):
                    print("Selected model ({}) NOT valid. Choose any of: {}".format(select_id, v2t(OutputCollectRF.all_solutions)))
            else:
                select_id = best_mod
                print("Selected model ID: {} for refresh model #{} based on the smallest combined normalized errors".format(select_id, depth))
        OutputCollectRF.select_id = select_id
        # Result collect & save
        these = ["result_hyp_param", "x_decomp_agg", "media_vec_collect", "x_decomp_vec_collect"]
        for tb in these:
            OutputCollectRF[tb] = OutputCollectRF[tb].assign(refresh_status=refresh_counter, best_mod_rf=select_id.isin(best_mod))


        # Create bestModRF and refreshStatus columns in listOutputPrev data.frames
        if refresh_counter == 1:
            for tb in these:
                list_output_prev[tb] = pd.concat([
                    list_output_prev[tb],
                    pd.DataFrame({'bestModRF': True, 'refreshStatus': 0})
                ])
                list_report_prev[tb] = pd.DataFrame({'mediaVecReport': list_output_prev[tb]['mediaVecCollect'], 'xDecompVecReport': list_output_prev[tb]['xDecompVecCollect']})
                names(list_report_prev[tb]) = [f'{name}Report' for name in names(list_report_prev[tb])]

        # Filter and bind rows for listReportPrev and listReportPrev$mediaVecReport
        list_report_prev['resultHypParamReport'] = pd.concat([
            list_report_prev['resultHypParamReport'],
            pd.DataFrame(filter(OutputCollectRF['resultHypParam'], bestModRF==True), columns=['solID', 'refreshStatus']) ##), one more ) added by ai, commented out
        ])
        list_report_prev['xDecompAggReport'] = pd.concat([
            list_report_prev['xDecompAggReport'],
            pd.DataFrame(filter(OutputCollectRF['xDecompAgg'], bestModRF==True), columns=['solID', 'refreshStatus']) ##), one more ) added by ai, commented out
        ])
        list_report_prev['mediaVecReport'] = pd.concat([
            list_report_prev['mediaVecReport'],
            pd.DataFrame(filter(OutputCollectRF['mediaVecCollect'], bestModRF==True), columns=['ds', 'refreshStatus']) ##), one more ) added by ai, commented out
        ])
        list_report_prev['xDecompVecReport'] = pd.concat([
            list_report_prev['xDecompVecReport'],
            pd.DataFrame(filter(OutputCollectRF['xDecompVecCollect'], bestModRF==True), columns=['ds', 'refreshStatus']) ##), one more ) added by ai, commented out
        ])

        # Update listNameUpdate and Robyn with new data
        list_name_update = f'listRefresh{refresh_counter}'
        Robyn[list_name_update] = pd.DataFrame({
            'InputCollect': InputCollectRF,
            'OutputCollect': OutputCollectRF,
            'ReportCollect': list_report_prev
        })

        # Plotting
        if json_file is not None:
            json_temp = robyn_write(InputCollectRF, OutputCollectRF, select_model=selectID, export=True, quiet=True)
            plots = refresh_plots_json(OutputCollectRF, json_file=attr(json_temp, 'json_file'), export=True)
        else:
            plots = refresh_plots(InputCollectRF, OutputCollectRF, ReportCollect, export=True)

        # Export data
        if export:
            message(f'>> Exporting refresh CSVs into directory...')
            pd.write_csv(resultHypParamReport, f'{plot_folder}report_hyperparameters.csv')
            pd.write_csv(xDecompAggReport, f'{plot_folder}report_aggregated.csv')
            pd.write_csv(mediaVecReport, f'{plot_folder}report_media_transform_matrix.csv')
            pd.write_csv(xDecompVecReport, f'{plot_folder}report_alldecomp_matrix.csv')

        if refresh_counter == 0:
            refresh_control = False
            message(f'Reached maximum available date. No further refresh possible')


    ## Indentation was wrong, manually corrected this part.
    # Save some parameters to print
    """
    robyn['refresh'] = list(
        selectIDs=report_collect['selectIDs'],
        refresh_steps=refresh_steps,
        refresh_mode=refresh_mode,
        refresh_trials=refresh_trials,
        refresh_iters=refresh_iters,
        plots=plots
    )
        # Save Robyn object and print parameters
    Robyn['refresh'] = pd.DataFrame({
        'selectIDs': ReportCollect['selectIDs'],
        'refresh_steps': refresh_steps,
        'refresh_mode': refresh_mode,
        'refresh_trials': refresh_trials,
        'refresh_iters': refresh_iters,
        'plots': plots
    })
    """

    ##Partially wrong interpretation since, saving R models is different in Python
    # Save Robyn object locally
    robyn = robyn[robyn.keys()]
    ## class robyn(robyn_refresh, class robyn):
    ##    pass

    ##    if not json_file:
    ##        message('>> Exporting results: ', robyn_object)
    ##        saveRDS(robyn, file=robyn_object)
    ##    else:
    ##        robyn_write(input_collect_rf, output_collect_rf, select_model=selectID, **kwargs)

    return(invisible(robyn))


##MetaMate
def print_robyn_refresh(x, *args):
    top_models = x.refresh.selectIDs
    top_models = [f"{id} ({i})" for i, id in enumerate(top_models)]
    print("Refresh Models: {}".format(len(top_models)))
    print("Mode: {}".format(x.refresh.refresh_mode))
    print("Steps: {}".format(x.refresh.refresh_steps))
    print("Trials: {}".format(x.refresh.refresh_trials))
    print("Iterations: {}".format(x.refresh.refresh_iters))
    print("Models (IDs):\n{}".format(", ".join(top_models)))


##MetaMate
def plot_robyn_refresh(x, *args):
    plt.plot((x.refresh.plots[0] / x.refresh.plots[1]), *args)

##MetaMate
def refresh_hyps(initBounds, listOutputPrev, refresh_steps, rollingWindowLength):
    initBoundsDis = [x[1] - x[0] if len(x) == 2 else 0 for x in initBounds]
    newBoundsFreedom = refresh_steps / rollingWindowLength
    print(">>> New bounds freedom:", round(newBoundsFreedom * 100, 2), "%")
    hyper_updated_prev = listOutputPrev.hyper_updated
    hypNames = listOutputPrev.resultHypParam.columns
    resultHypParam = pd.DataFrame(listOutputPrev.resultHypParam)
    for h in range(len(hypNames)):
        hn = hypNames[h]
        getHyp = resultHypParam[hn].values[0]
        getDis = initBoundsDis[hn]
        if hn == "lambda":
            lambda_max = resultHypParam["lambda_max"].unique()
            lambda_min = lambda_max * 0.0001
            getHyp = getHyp / (lambda_max - lambda_min)
        getRange = initBounds[hn][0]
        if len(getRange) == 2:
            newLowB = getHyp - getDis * newBoundsFreedom
            if newLowB < getRange[0]:
                newLowB = getRange[0]
            newUpB = getHyp + getDis * newBoundsFreedom
            if newUpB > getRange[1]:
                newUpB = getRange[1]
            newBounds = [newLowB, newUpB]
            hyper_updated_prev[hn][0] = newBounds
        else:
            hyper_updated_prev[hn][0] = getRange
    return hyper_updated_prev

================
File: response.py
================
# Copyright (c) Meta Platforms, Inc. and its affiliates.

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

####################################################################
import os
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

from .inputs import robyn_inputs
#from .model import robyn_run
from .transformation import saturation_hill, transform_adstock

from .checks import check_metric_dates, check_metric_type, check_metric_value
import seaborn as sns

plt.ioff()

def robyn_response(InputCollect=None,
                   OutputCollect=None,
                   json_file=None,
                   robyn_object=None,
                   select_build=None,
                   select_model=None,
                   metric_name=None,
                   metric_value=None,
                   date_range=None,
                   dt_hyppar=None,
                   dt_coef=None,
                   quiet=False):
    # Get input
    if json_file:
        # Use previously exported model using json_file
        if InputCollect is None:
            InputCollect = robyn_inputs(json_file=json_file)
        if OutputCollect is None:
            OutputCollect = robyn_run(InputCollect=InputCollect, json_file=json_file, export=False, quiet=quiet)
        if dt_hyppar is None:
            dt_hyppar = OutputCollect.resultHypParam
        if dt_coef is None:
            dt_coef = OutputCollect.xDecompAgg
    else:
        if robyn_object:
            if not os.path.exists(robyn_object):
                raise FileNotFoundError(f"File does not exist or is somewhere else. Check: {robyn_object}")
            else:
                Robyn = readRDS(robyn_object)  # Assume readRDS is a function you have defined to read RDS files in Python
                objectPath = os.path.dirname(robyn_object)
                objectName = re.sub(r'\..*$', '', os.path.basename(robyn_object))

            select_build_all = range(len(Robyn))
            if select_build is None:
                select_build = max(select_build_all)
                if not quiet and len(select_build_all) > 1:
                    print(f"Using latest model: {'initial model' if select_build == 0 else f'refresh model #{select_build}'} for the response function. Use parameter 'select_build' to specify which run to use")

            if select_build not in select_build_all or not isinstance(select_build, int):
                raise ValueError(f"'select_build' must be one value of {', '.join(map(str, select_build_all))}")

            listName = "listInit" if select_build == 0 else f"listRefresh{select_build}"
            InputCollect = Robyn[listName]["InputCollect"]
            OutputCollect = Robyn[listName]["OutputCollect"]
            dt_hyppar = OutputCollect.resultHypParam
            dt_coef = OutputCollect.xDecompAgg
        else:
            # Try to get some pre-filled values
            if dt_hyppar is None:
                dt_hyppar = OutputCollect['resultHypParam']
            if dt_coef is None:
                dt_coef = OutputCollect['xDecompAgg']
            if any(x is None for x in [dt_hyppar, dt_coef, InputCollect, OutputCollect]):
                raise ValueError("When 'robyn_object' is not provided, 'InputCollect' & 'OutputCollect' must be provided")

    # Prep environment
    if True:
        dt_input = InputCollect["robyn_inputs"]["dt_input"]
        startRW = InputCollect["robyn_inputs"]["rollingWindowStartWhich"]
        endRW = InputCollect["robyn_inputs"]["rollingWindowEndWhich"]
        adstock = InputCollect["robyn_inputs"]["adstock"]
        spendExpoMod = InputCollect["robyn_inputs"]["modNLS"]["results"]
        paid_media_vars = InputCollect["robyn_inputs"]["paid_media_vars"]
        paid_media_spends = InputCollect["robyn_inputs"]["paid_media_spends"]
        exposure_vars = InputCollect["robyn_inputs"]["exposure_vars"]
        organic_vars = InputCollect["robyn_inputs"]["organic_vars"]
        allSolutions = dt_hyppar['solID'].unique()
        dayInterval = InputCollect["robyn_inputs"]["dayInterval"]

    # Check select_model
    if not select_model or select_model not in allSolutions:
        raise ValueError(f"Input 'select_model' must be one of these values: {', '.join(allSolutions)}")

    # Get use case based on inputs
    usecase = which_usecase(metric_value, date_range)

    # Check inputs with usecases
    metric_type = check_metric_type(metric_name, paid_media_spends, paid_media_vars, exposure_vars, organic_vars)
    all_dates = dt_input['DATE'].tolist()
    all_values = dt_input[metric_name].tolist()

    if usecase == "all_historical_vec":
        # Calculate dates and values for all historical data
        ds_list = check_metric_dates("all", all_dates[0:endRW], dayInterval, quiet)
        metric_value = None
    elif usecase == "unit_metric_default_last_n":
        # Calculate dates and values for last n days
        ds_list = check_metric_dates("last_{}".format(len(metric_value)), all_dates[0:endRW], dayInterval, quiet)
    else:
        # Calculate dates and values for specified date range
        ds_list = check_metric_dates(date_range, all_dates[0:endRW], dayInterval, quiet)

    val_list = check_metric_value(metric_value, metric_name, all_values, ds_list['metric_loc'])
    date_range_updated = ds_list['date_range_updated']
    metric_value_updated = val_list['metric_value_updated']
    all_values_updated = val_list['all_values_updated']

    # Transform exposure to spend when necessary
    if metric_type == "exposure":
        get_spend_name = paid_media_spends[np.where(paid_media_vars == metric_name)]
        expo_vec = dt_input[metric_name][[1]]
        # Use non-0 mean as marginal level if metric_value not provided
        if metric_value is None:
            metric_value = np.mean(expo_vec[startRW:endRW][expo_vec[startRW:endRW] > 0])
            if not quiet:
                print("Input 'metric_value' not provided. Using mean of ", metric_name, " instead")

        # Fit spend to exposure
        spend_vec = dt_input[get_spend_name][[1]]
        temp = filter(spendExpoMod, dt_input['channel'] == metric_name)
        nls_select = temp['rsq_nls'] > temp['rsq_lm']
        if nls_select:
            Vmax = spendExpoMod['Vmax'][spendExpoMod['channel'] == metric_name]
            Km = spendExpoMod['Km'][spendExpoMod['channel'] == metric_name]
            input_immediate = mic_men(x=metric_value_updated, Vmax=Vmax, Km=Km, reverse=True)
        else:
            coef_lm = spendExpoMod['coef_lm'][spendExpoMod['channel'] == metric_name]
            input_immediate = metric_value_updated / coef_lm

        all_values_updated[ds_list['metric_loc']] = input_immediate
        hpm_name = get_spend_name
    else:
        input_immediate = metric_value_updated
        hpm_name = metric_name

    # Adstocking original
    # media_vec_origin = dt_input[metric_name][[1]]
    media_vec_origin = dt_input[metric_name].tolist()

    dt_hyppar = sanitize_suffixes(dt_hyppar)

    theta = scale = shape = None
    if adstock == "geometric":
        theta_column_name = f"{hpm_name}_thetas"
        theta = dt_hyppar[dt_hyppar['solID'] == select_model][theta_column_name].iloc[0]
        # theta = dt_hyppar[dt_hyppar['solID'] == select_model][["{}{}".format(hpm_name, "_thetas")]][[1]]
    elif re.search("weibull", adstock):
        shape_column_name = f"{hpm_name}_shapes"
        shape = dt_hyppar[dt_hyppar['solID'] == select_model][shape_column_name].iloc[0]

        scale_column_name = f"{hpm_name}_scales"
        scale = dt_hyppar[dt_hyppar['solID'] == select_model][scale_column_name].iloc[0]

    x_list = transform_adstock(media_vec_origin, adstock, theta=theta, shape=shape, scale=scale)
    m_adstocked = x_list['x_decayed']

    # Adstocking simulation
    x_list_sim = transform_adstock(all_values_updated, adstock, theta=theta, shape=shape, scale=scale)
    media_vec_sim = x_list_sim['x_decayed']
    media_vec_sim_imme = True if adstock == "weibull_pdf" else x_list_sim['x']
    input_total = media_vec_sim[ds_list['metric_loc']]
    input_immediate = media_vec_sim_imme[ds_list['metric_loc']]
    input_carryover = input_total - input_immediate

    # Saturation
    m_adstockedRW = m_adstocked[startRW:endRW]
    alpha_column_name = f"{hpm_name}_alphas"
    alpha = dt_hyppar[dt_hyppar['solID'] == select_model][alpha_column_name].iloc[0]

    gamma_column_name = f"{hpm_name}_gammas"
    gamma = dt_hyppar[dt_hyppar['solID'] == select_model][gamma_column_name].iloc[0]
    # alpha = head(dt_hyppar[dt_hyppar['solID'] == select_model, ][["{}{}".format(hpm_name, "_alphas")]], 1)
    # gamma = head(dt_hyppar[dt_hyppar['solID'] == select_model, ][["{}{}".format(hpm_name, "_gammas")]], 1)
    if usecase == "all_historical_vec":
        metric_saturated_total = saturation_hill(x=m_adstockedRW, alpha=alpha, gamma=gamma)
        metric_saturated_carryover = saturation_hill(x=m_adstockedRW, alpha=alpha, gamma=gamma)
    else:
        metric_saturated_total = saturation_hill(x=m_adstockedRW, alpha=alpha, gamma=gamma, x_marginal=input_total)
        metric_saturated_carryover = saturation_hill(x=m_adstockedRW, alpha=alpha, gamma=gamma, x_marginal=input_carryover)

    metric_saturated_immediate = metric_saturated_total - metric_saturated_carryover

    # Decomp
    coeff = dt_coef[(dt_coef['solID'] == select_model) & (dt_coef['rn'] == hpm_name)][['coefs']]

    # metric_saturated_total = metric_saturated_total.reset_index(drop=True)


    coeff_value = coeff.iloc[0]['coefs']
    m_saturated = saturation_hill(x=m_adstockedRW, alpha=alpha, gamma=gamma)
    m_resposne = m_saturated * coeff_value

    response_total = metric_saturated_total * coeff_value
    response_carryover = metric_saturated_carryover * coeff_value
    response_immediate = response_total - response_carryover

    dt_line = pd.DataFrame({'metric': m_adstockedRW, 'response': m_resposne, 'channel': metric_name})

    if usecase == "all_historical_vec":
        dt_point = pd.DataFrame({'input': input_total[startRW:endRW], 'output': response_total, 'ds': date_range_updated[startRW:endRW]})
        dt_point_caov = pd.DataFrame({'input': input_carryover[startRW:endRW], 'output': response_carryover})
        dt_point_imme = pd.DataFrame({'input': input_immediate[startRW:endRW], 'output': response_immediate})
    else:
        dt_point = pd.DataFrame({'input': input_total, 'output': response_total, 'ds': date_range_updated})
        dt_point_caov = pd.DataFrame({'input': input_carryover, 'output': response_carryover})
        dt_point_imme = pd.DataFrame({'input': input_immediate, 'output': response_immediate})

    # Plot optimal response
    # p_res = plt.figure(figsize=(12, 6))
    # sns.lineplot(x='metric', y='response', data=dt_line, color="steelblue")
    # sns.scatterplot(x='input', y='output', data=dt_point, size=3)
    # sns.scatterplot(x='input', y='output', data=dt_point_caov, size=3, marker=8)
    # sns.scatterplot(x='input', y='output', data=dt_point_imme, size=3)
    # plt.title(f"Saturation curve of {metric_name}")
    # plt.text(0.5, 0.95, f"Carryover* Response: {response_carryover} @ Input {input_carryover} \nImmediate Response: {response_immediate} @ Input {input_immediate} \n Total (C+I) Response: {response_total} @ Input {input_total}")
    # plt.xlabel('Input')
    # plt.ylabel('Response')
    # plt.text(0.5, 0.05, f"Response period: {date_range_updated[0]} to {date_range_updated[-1]} [{len(date_range_updated)} periods]")
    # plt.show()

    ret = {
        'metric_name': metric_name,
        'date': date_range_updated,
        'input_total': input_total,
        'input_carryover': input_carryover,
        'input_immediate': input_immediate,
        'response_total': response_total,
        'response_carryover': response_carryover,
        'response_immediate': response_immediate,
        'usecase': usecase,
        # 'plot': p_res
        'plot': None
    }
    return ret

def sanitize_suffixes(df):
    columns = df.columns
    to_drop = []
    rename_map = {}

    for col in columns:
        if col.endswith('_x'):
            base_name = col[:-2]  # Remove '_x'
            y_col = base_name + '_y'
            if y_col in columns:
                to_drop.append(y_col)
            rename_map[col] = base_name

    df = df.drop(columns=to_drop)
    df = df.rename(columns=rename_map)
    return df

def which_usecase(metric_value, date_range):
    usecase = None

    if pd.isnull(metric_value) and pd.isnull(date_range):
        usecase = "all_historical_vec"
    elif pd.isnull(metric_value) and not pd.isnull(date_range):
        usecase = "selected_historical_vec"
    elif (isinstance(metric_value, str) and pd.isnull(date_range)) or (isinstance(metric_value, list) and len(metric_value) == 1 and pd.isnull(date_range)):
        usecase = "total_metric_default_range"
    elif (isinstance(metric_value, str) and not pd.isnull(date_range)) or (isinstance(metric_value, list) and len(metric_value) == 1 and not pd.isnull(date_range)):
        usecase = "total_metric_selected_range"
    elif isinstance(metric_value, list) and len(metric_value) > 1 and pd.isnull(date_range):
        usecase = "unit_metric_default_last_n"
    elif isinstance(metric_value, list) and len(metric_value) > 1 and not pd.isnull(date_range):
        usecase = "unit_metric_selected_dates"

    if date_range is not None and date_range == "all":
        usecase = "all_historical_vec"

    return usecase

================
File: transformation.py
================
# Copyright (c) Meta Platforms, Inc. and its affiliates.

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

####################################################################
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy
from matplotlib.ticker import FuncFormatter
## from scipy.special import weibull
## Manually added for ggplot
from plotnine import ggplot, aes, labs, geom_point, geom_line, theme_gray, geom_hline, geom_text, facet_grid
import sklearn

from .checks import check_adstock

def mic_men(x, Vmax, Km, reverse=False):
    """
    Calculate the Michaelis-Menten transformation.

    Args:
        x (float): The input value.
        Vmax (float): The maximum rate of the transformation.
        Km (float): The Michaelis constant.
        reverse (bool, optional): Whether to perform the reverse transformation. Defaults to False.

    Returns:
        float: The transformed value.
    """
    if not reverse:
        mm_out = Vmax * x / (Km + x)
    else:
        mm_out = x * Km / (Vmax - x)
    return mm_out

def adstock_geometric(x, theta):
    """
    Adstock geometric function

    Parameters:
    x (list): The input values.
    theta (float): The decay factor.

    Returns:
    pandas.DataFrame: A DataFrame containing the original input values, the decayed values, the cumulative decay factors, and the inflation total.
    """
    ##if len(theta) != 1:
    if theta is None:
        ##raise ValueError("Length of theta should be 1")
        raise ValueError("Theta can not be Null")

    if len(x) > 1:
        x_decayed = [x[0]]
        for i in range(1, len(x)):
            decayed_value = x[i] + theta * x_decayed[i - 1]
            if not np.isscalar(decayed_value):
                decayed_value = decayed_value.iloc[0]
            x_decayed.append(decayed_value)
        x_decayed = np.array(x_decayed)

        if not np.isscalar(theta):
            theta = theta.iloc[0]
        thetaVecCum = [theta]
        for i in range(1, len(x)):
            cum_value = thetaVecCum[i - 1] * theta
            if not np.isscalar(cum_value):
                cum_value = cum_value.iloc[0]
            thetaVecCum.append(cum_value)
        thetaVecCum = np.array(thetaVecCum)

    else:
        # x_decayed = [val[0] for val in x]
        x_decayed = [val[0] if isinstance(val, (list, tuple)) else val for val in x]
        ##thetaVecCum = np.array([theta])
        thetaVecCum = list()
        thetaVecCum.append(theta)

    inflation_total = np.sum(x_decayed) / np.sum(x)

    return pd.DataFrame(
        {
            "x": x,
            "x_decayed": x_decayed,
            "thetaVecCum": thetaVecCum,
            "inflation_total": inflation_total,
        }
    )

## def adstock_weibull(x, shape, scale, windlen=len(x), type="cdf"):
## using stype
def adstock_weibull(x, shape, scale, windlen=None, stype="cdf"):
    """
    Adstock Weibull function

    Calculates the adstock transformation using the Weibull function.

    Parameters:
    - x: array-like
        The input time series data.
    - shape: float
        The shape parameter of the Weibull distribution.
    - scale: float
        The scale parameter of the Weibull distribution.
    - windlen: int, optional
        The length of the adstock window. If not provided, it defaults to the length of x.
    - stype: str, optional
        The type of adstock transformation to perform. Valid options are "cdf" (default) and "pdf".

    Returns:
    - dict:
        A dictionary containing the following keys:
        - "x": array-like
            The input time series data.
        - "x_decayed": array-like
            The adstock transformed data.
        - "thetaVecCum": array-like
            The cumulative adstock weights.
        - "inflation_total": float
            The total inflation factor.
        - "x_imme": array-like
            The immediate adstock transformed data.
    """
    ## Added manually since Python function signature fails getting len of x
    if windlen is None:
        windlen = len(x)
    ## if len(shape) != 1:
    if shape is None:
        ##raise ValueError("Length of shape should be 1")
        raise ValueError("Shape should be a number")
    ## if len(scale) != 1:
    if scale is None:
        ##raise ValueError("Length of scale should be 1")
        raise ValueError("Scale should be a number")
    if len(x) > 1:
        ## check_opts(stype.lower(), ["cdf", "pdf"])
        if stype.lower() not in ["cdf", "pdf"]:
            warnings.warn("Not valid type")

        ## x_bin = np.arange(1, windlen + 1)
        x_bin = np.arange(1, windlen + 1)
        scaleTrans = np.round(np.quantile(x_bin, scale), 0)
        if shape == 0 or scale == 0:
            x_decayed = x
            thetaVecCum = np.zeros(windlen)
            x_imme = None
        else:
            ## https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.exponweib.html
            if stype.lower() == "cdf":
                ##[1]
                ##1 - scipy.stats.weibull.cdf(x_bin, shape, scaleTrans)
                ##for _ in range(1, windlen) ##]
                thetaVec = np.append([1], 1 - scipy.stats.exponweib.cdf(x_bin[:len(x_bin)-1], shape, scaleTrans))
                thetaVecCum = np.cumprod(thetaVec)
            elif stype.lower() == "pdf":
                thetaVecCum = scipy.stats.exponweib.pdf(x_bin, shape, scaleTrans)
                ##np.array(
                    ##[1]
                    ##+ [
                        ## scipy.stats.weibull.pdf(x_bin, shape, scaleTrans)
                        ##for _ in range(1, windlen)
                    ##]
                ##)
                thetaVecCum = sklearn.preprocessing.normalize(thetaVecCum[:,np.newaxis], axis=0).ravel()
            ##else:
            ##    raise ValueError("Invalid type")
            ## x_pos = np.arange(1, 100)
            x_pos = np.arange(1, len(x) + 1)
            x_val = x
            x_decayed = list()
            ## Manually implemented mapply of R but can be better , may need to optimize
            for i, val in enumerate(x_val):
                x_pos_repeated = np.repeat(0, x_pos[i] - 1)
                x_pos_repeated_reverse = np.repeat(x_val[i], windlen - x_pos[i] + 1)
                x_vec = np.append(x_pos_repeated, x_pos_repeated_reverse)
                thetaVecCumLag = np.roll(thetaVecCum, x_pos[i]- 1)
                x_decayed_row = np.multiply(x_vec, thetaVecCumLag)
                x_decayed.append(x_decayed_row)

            x_decayed = np.asmatrix(x_decayed)
            x_imme = np.diag(x_decayed)
            x_decayed = np.sum(x_decayed, axis=1)
    else:
        x_imme = x
        x_decayed = x_imme
        ## manually added
        thetaVecCum = 1 ##np.array([1])

    inflation_total = np.sum(x_decayed) / np.sum(x)
    ## return pd.DataFrame
    return {
        "x": x,
        "x_decayed": x_decayed,
        "thetaVecCum": thetaVecCum,
        "inflation_total": inflation_total,
        "x_imme": x_imme
        }

def transform_adstock(x, adstock, theta=None, shape=None, scale=None, windlen=None):
    """
    Transforms the input data using the adstock model.

    Parameters:
    - x: The input data to be transformed.
    - adstock: The type of adstock model to be applied. Possible values are "geometric", "weibull_cdf", and "weibull_pdf".
    - theta: The decay factor for the geometric adstock model. Only applicable if adstock is "geometric".
    - shape: The shape parameter for the Weibull adstock model. Only applicable if adstock is "weibull_cdf" or "weibull_pdf".
    - scale: The scale parameter for the Weibull adstock model. Only applicable if adstock is "weibull_cdf" or "weibull_pdf".
    - windlen: The length of the adstock window. If not provided, it defaults to the length of the input data.

    Returns:
    - x_list_sim: The transformed data based on the adstock model.

    """
    ## Added manually since Python function signature fails getting len of x
    if windlen != None:
        windlen = len(x)

    ## Added manually, LLaMa didn't get this one
    check_adstock(adstock)

    x_list_sim = None

    if adstock == "geometric":
        x_list_sim = adstock_geometric(x = x, theta = theta)
    elif adstock == "weibull_cdf":
        x_list_sim = adstock_weibull(x, shape, scale, windlen, "cdf")
    elif adstock == "weibull_pdf":
        x_list_sim = adstock_weibull(x, shape, scale, windlen, "pdf")
    return x_list_sim

## TODO: diff and range?
def normalize(x):
    """
    Normalizes the input data.

    Parameters:
    x (array-like): The input data to be normalized.

    Returns:
    array-like: The normalized data.
    """
    if np.diff(np.range(x)) == 0:
        return np.array([1, np.zeros(len(x) - 1)])
    else:
        return (x - np.min(x)) / (np.max(x) - np.min(x))


def saturation_hill(x, alpha, gamma, x_marginal=None):
    """
    Implements the saturation hill function.

    Parameters:
    - x: Input values.
    - alpha: Exponent parameter.
    - gamma: Weighting parameter.
    - x_marginal: Optional marginal values.

    Returns:
    - x_scurve: Output values computed using the saturation hill function.
    """
    ## No need to length check for alpha and gamma since they are numbers not like lists in R
    ## linear interpolation by dot product
    ##inflexion <- c(range(x) %*% c(1 - gamma, gamma)) # linear interpolation by dot product
    ##np.repeat(x)
    if alpha is None or gamma is None:
        raise ValueError("Alpha and Gamma cannot be None")

    if not np.isscalar(gamma):
        gamma = gamma.iloc[0]
    if not np.isscalar(alpha):
        alpha = alpha.iloc[0]
    inflexion = np.dot(np.array([1 - gamma, gamma]), np.array([np.min(x), np.max(x)]))
    if x_marginal is None:
        x_scurve = x**alpha / (x**alpha + np.power(inflexion, alpha))
    else:
        x_scurve = x_marginal**alpha / (x_marginal**alpha + np.power(inflexion, alpha))

    return x_scurve

def plot_adstock(plot=True):
    """
    Plots the adstock models.

    Parameters:
        plot (bool): If True, plots the adstock models. Default is True.

    Returns:
        p1 (ggplot): The plot of the geometric adstock model.
        p2 (ggplot): The plot of the Weibull adstock model.
    """
    if plot:
        # Plot geometric
        geomCollect = []
        thetaVec = np.array([0.01, 0.05, 0.1, 0.2, 0.5, 0.6, 0.7, 0.8, 0.9])
        for v in range(len(thetaVec)):
            ## thetaVecCum = np.power(np.array([1, np.inf]), thetaVec[v])
            thetaVecCum = [0] * 100
            thetaVecCum[0] = 1
            ## Manually added
            for t in range(1, 100):
                thetaVecCum[t] = thetaVecCum[t-1] * thetaVec[v]

            dt_geom = pd.DataFrame(
                {
                    "x": np.arange(0, 100),
                    "decay_accumulated": thetaVecCum,
                    "theta": thetaVec[v],
                }
            )
            ## Changed
            ## dt_geom["halflife"] = np.where(dt_geom["decay_accumulated"] == 0.5, 1, 0)
            dt_geom["halflife"] = np.argmin(abs(dt_geom["decay_accumulated"] - 0.5))
            geomCollect.append(dt_geom)

        geomCollect = pd.concat(geomCollect)
        ## Added astype for correction
        geomCollect["theta_halflife"] = (
            geomCollect["theta"].astype(str) + "_" + geomCollect["halflife"].astype(str)
        )

        ## Used plotline to use ggplot almost as is from R
        p1 = (
            ggplot(geomCollect, aes(x="x", y="decay_accumulated"))
            + geom_line(aes(color="theta_halflife"))
            + geom_hline(yintercept=0.5, linetype="dashed", color="gray")
            ##+ geom_text(aes(x = max("x"), y = 0.5, vjust = -0.5, hjust = 1, label = "halflife"), colour = "gray")
            + geom_text(aes(x = max("x"), y = 0.5, label = "halflife"), colour = "gray")
            + labs(
                    title = "Geometric Adstock\n(Fixed decay rate)",
                    subtitle = "Halflife = time until effect reduces to 50%",
                    x = "Time unit",
                    y = "Media decay accumulated"
                )
            + theme_gray()
        )

        # Plot weibull
        weibullCollect = []
        shapeVec = np.array([0.5, 1, 2, 9])
        scaleVec = np.array([0.01, 0.05, 0.1, 0.15, 0.2, 0.5])
        types = ["CDF", "PDF"]
        for t in range(len(types)):
            for v1 in range(len(shapeVec)):
                for v2 in range(len(scaleVec)):
                    dt_weibull = pd.DataFrame(
                        {
                            "x": np.arange(1, 101),
                            "decay_accumulated": adstock_weibull(
                                np.arange(1, 101), shapeVec[v1], scaleVec[v2], stype=types[t].lower()
                            )["thetaVecCum"],
                            "shape": f"shape={shapeVec[v1]}",
                            "scale": scaleVec[v2],
                            "type": types[t],
                        }
                    )
                    ## Manually changed
                    ## dt_weibull["halflife"] = np.where(dt_weibull["decay_accumulated"] == 0.5, 1, 0)
                    dt_weibull["halflife"] = np.argmin(abs(dt_weibull["decay_accumulated"] - 0.5))
                    weibullCollect.append(dt_weibull)
        weibullCollect = pd.concat(weibullCollect)

        ## Using plotline to use ggplot almost as is from R
        p2 = (
            ggplot(weibullCollect, aes(x="x", y="decay_accumulated"))
            + geom_line(aes(color="scale"))
            + facet_grid("shape ~ type")
            + geom_hline(yintercept=0.5, linetype="dashed", color="gray")
            ##+ geom_text(aes(x = max("x"), y = 0.5, vjust = -0.5, hjust = "center", label = "halflife"), colour = "gray")
            + geom_text(aes(x = max("x"), y = 0.5, label = "halflife"), colour = "gray")
            + labs(
                    title = "Weibull Adstock CDF vs PDF\n(Flexible decay rate)",
                    subtitle = "Halflife = time until effect reduces to 50%",
                    x = "Time unit",
                    y = "Media decay accumulated"
                )
            + theme_gray()
        )

        # Create plots
        """ Manually commented out to use ggplot as in R code
        p1 = plt.figure(figsize=(10, 6))
        p1.plot(geomCollect["x"], geomCollect["decay_accumulated"], label="Geometric")
        p1.set_xlabel("Time unit")
        p1.set_ylabel("Media decay accumulated")
        p1.legend()
        p1.title("Geometric Adstock (Fixed decay rate)")

        p2 = plt.figure(figsize=(10, 6))
        p2.plot(
            weibullCollect["x"], weibullCollect["decay_accumulated"], label="Weibull"
        )
        p2.set_xlabel("Time unit")
        p2.set_ylabel("Media decay accumulated")
        p2.legend()
        p2.title("Weibull Adstock (Flexible decay rate)")
        """

        return p1, p2


def plot_saturation(plot=True):
    """
    Plots the saturation response using the hill function.

    Parameters:
    - plot (bool): If True, the plot will be displayed. Default is True.

    Returns:
    - p1 (ggplot object): The plot of the saturation response with varying alpha values.
    - p2 (ggplot object): The plot of the saturation response with varying gamma values.
    """
    ## Too wrong porting
    # Create a sample dataset
    ## Manually corrected the for loops
    if plot:
        x_sample = np.arange(1, 100, 1)
        alpha_sample = np.array([0.1, 0.5, 1, 2, 3])
        gamma_sample = np.array([0.1, 0.3, 0.5, 0.7, 0.9])
        hillAlphaCollect = list()
        for i in range(len(alpha_sample)):

            # Create a dataframe with the sample data
            df = pd.DataFrame({
                "x": x_sample,
                "y": x_sample**alpha_sample[i] / (x_sample**alpha_sample[i] + (0.5 * 100)**alpha_sample[i]),
                "alpha": alpha_sample[i]
                })
            hillAlphaCollect.append(df)

        hillAlphaCollect = pd.concat(hillAlphaCollect)

        """ Manually commented out to use ggplot as in R code
        plt.plot(hillAlphaCollect["x"], hillAlphaCollect["y"])
        plt.xlabel("X")
        plt.ylabel("y")
        plt.title("Saturation Response")
        plt.show()
        """

        p1 = (
            ggplot(hillAlphaCollect, aes(x = "x", y = "y", color = "alpha"))
            + geom_line()
            + labs(title = "Cost response with hill function", subtitle = "Alpha changes while gamma = 0.5")
            + theme_gray(background = "white", pal = 2)
        )

        hillAlphaCollect = list()
        for i in range(len(gamma_sample)):
            # Create a dataframe with the sample data
            df = pd.DataFrame({
                "x": x_sample,
                "y": x_sample**2 / (x_sample**2 + (gamma_sample[i] * 100)**2),
                "gamma": gamma_sample[i]
                })
            hillAlphaCollect.append(df)

        hillAlphaCollect = pd.concat(hillAlphaCollect)

        p2 = (
            ggplot(hillGammaCollect, aes(x = "x", y = "y", color = "gamma"))
            + geom_line()
            + labs(
                title = "Cost response with hill function",
                subtitle = "Gamma changes while alpha = 2"
            )
            + theme_gray(background = "white", pal = 2)
        )

        """ Manually commented out to use ggplot as in R code
        plt.plot(hillAlphaCollect["x"], hillAlphaCollect["y"])
        plt.xlabel("X")
        plt.ylabel("y")
        plt.title("Cost response with hill function")
        plt.show()
        """

        return p1, p2


def run_transformations(input_collect, hyp_param_sam, adstock):
    """
    Run transformations on the input data.

    Args:
        input_collect (pd.DataFrame): The input data frame.
        hyp_param_sam (dict): The dictionary containing the hyperparameters.
        adstock (str): The type of adstocking to be applied.

    Returns:
        dict: A dictionary containing the transformed data frames.
            - dt_modSaturated (pd.DataFrame): The saturated data frame.
            - dt_saturatedImmediate (pd.DataFrame): The saturated immediate data frame.
            - dt_saturatedCarryover (pd.DataFrame): The saturated carryover data frame.
    """

    if "robyn_inputs" in input_collect:
        input_collect = input_collect["robyn_inputs"]

    # Extract the media names from the input collect dataframe
    all_media = input_collect["all_media"]

    # Extract the rolling window start and end indices
    rolling_window_start_which = input_collect["rollingWindowStartWhich"]
    rolling_window_end_which = input_collect["rollingWindowEndWhich"]

    select_columns = [column for column in input_collect['dt_mod'].columns if column != 'ds']
    dt_modAdstocked = input_collect['dt_mod'][select_columns]

    # Create a list to store the media adstocked data
    media_adstocked = dict()

    # Create a list to store the media immediate data
    media_immediate = dict()

    # Create a list to store the media carryover data
    media_carryover = dict()

    # Create a list to store the media cumulative data
    media_vec_cum = dict()

    # Create a list to store the media saturated data
    media_saturated = dict()

    # Create a list to store the media saturated immediate data
    media_saturated_immediate = dict()

    # Create a list to store the media saturated carryover data
    media_saturated_carryover = dict()

    # Iterate over each media name
    for v in range(len(all_media)):
        # Extract the media name
        media = all_media[v]

        m = list(dt_modAdstocked[[media]].values)
        m = np.array(m).reshape(len(m),)
        theta = shape = scale = None
        # Extract the adstocking parameters for this media
        if adstock == "geometric":
            theta = hyp_param_sam[f"{media}_thetas"]##[0]

        if adstock.startswith('weibull'):
            shape = hyp_param_sam[f"{media}_shapes"]##[0]
            scale = hyp_param_sam[f"{media}_scales"]##[0]

        # Calculate the adstocked response
        x_list = transform_adstock(
            m, adstock, theta=theta, shape=shape, scale=scale
        )

        m_adstocked = x_list["x_decayed"]
        m_adstocked = np.array(m_adstocked).reshape(len(m_adstocked),)
        # Store the adstocked data for this media
        ##media_adstocked.append(m_adstocked)
        media_adstocked[media] = m_adstocked

        # Calculate the immediate response
        if adstock == "weibull_pdf":
            m_imme = x_list["x_imme"]
            m_imme = np.array(m_imme).reshape(len(m_imme),)
        else:
            m_imme = m

        # Calculate the carryover response
        ## m_carryover = x_list["x_decayed"] - m_imme
        m_carryover = m_adstocked - m_imme

        # Store the immediate and carryover data for this media
        ##media_immediate.append(m_imme)
        media_immediate[media] = m_imme
        ##media_carryover.append(m_carryover)
        media_carryover[media] = m_carryover
        ##media_vec_cum.append(x_list["thetaVecCum"])
        media_vec_cum[media] = x_list["thetaVecCum"]

        m_adstockedRollWind = m_adstocked[(rolling_window_start_which-1):(rolling_window_end_which)]
        m_carryoverRollWind = m_carryover[(rolling_window_start_which-1):(rolling_window_end_which)]

        # Calculate the saturated response
        alpha = hyp_param_sam[f"{media}_alphas"] ##[0]
        gamma = hyp_param_sam[f"{media}_gammas"] ##[0]
        ##media_saturated.append(saturation_hill(m_adstockedRollWind, alpha = alpha, gamma = gamma))
        media_saturated[media] = saturation_hill(m_adstockedRollWind, alpha = alpha, gamma = gamma)

        # Calculate the saturated carryover response
        media_saturated_carryover[media] = saturation_hill(m_adstockedRollWind, alpha = alpha, gamma = gamma, x_marginal = m_carryoverRollWind)

        # Calculate the saturated immediate response
        ##media_saturated_immediate.append(media_saturated[v] - media_saturated_carryover[v])
        media_saturated_immediate[media] = media_saturated[media] - media_saturated_carryover[media]

    select_columns = [column for column in dt_modAdstocked.columns if column not in all_media]
    media_adstocked = pd.DataFrame(media_adstocked)
    dt_modAdstockedTemp = dt_modAdstocked[select_columns]
    dt_modAdstocked = pd.concat([dt_modAdstockedTemp.reset_index(), media_adstocked], axis=1)

    dt_mediaImmediate = pd.DataFrame(media_immediate)
    dt_mediaCarryover = pd.DataFrame(media_carryover)
    mediaVecCum = pd.DataFrame(media_vec_cum)

    mediaSaturated = pd.DataFrame(media_saturated)
    dt_modSaturatedTemp = dt_modAdstocked.loc[(rolling_window_start_which-1):(rolling_window_end_which-1)][select_columns]
    dt_modSaturated = pd.concat([dt_modSaturatedTemp.reset_index(), mediaSaturated], axis=1)

    dt_saturatedImmediate = pd.DataFrame(media_saturated_immediate)
    dt_saturatedImmediate.fillna(0, inplace=True)
    dt_saturatedCarryover = pd.DataFrame(media_saturated_carryover)
    dt_saturatedCarryover.fillna(0, inplace=True)

    # Create a dataframe with the media data
    ## media_df = pd.DataFrame(
    ##    {
    ##        "media": all_media,
    ##        "adstocked": media_adstocked,
    ##        "immediate": media_immediate,
    ##        "carryover": media_carryover,
    ##        "saturated": media_saturated,
    ##        "saturated_immediate": media_saturated_immediate,
    ##        "saturated_carryover": media_saturated_carryover,
    ##    }
    ##)

    return {
        "dt_modSaturated": dt_modSaturated,
        "dt_saturatedImmediate": dt_saturatedImmediate,
        "dt_saturatedCarryover": dt_saturatedCarryover
    }
